{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "\n# Glue Studio Notebook\nYou are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n\n## Available Magics\n|          Magic              |   Type       |                                                                        Description                                                                        |\n|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n| %stop_session               |              |  Stops the current session.                                                                                                                               |\n| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |",
			"metadata": {
				"editable": false,
				"deletable": false,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "# Step 1: Define your configurations",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "%stop_session",
			"metadata": {
				"trusted": true
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "Stopping session: 77731768-e4db-416c-b63f-4dbf12e4b1df\nStopped session.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%connections hudi-connection\n%glue_version 3.0\n%region us-west-2\n%worker_type G.1X\n%number_of_workers 2\n%spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n%additional_python_modules Faker",
			"metadata": {
				"tags": [],
				"trusted": true
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.37.0 \nConnections to be included:\nhudi-connection\nSetting Glue version to: 3.0\nPrevious region: us-west-2\nSetting new region to: us-west-2\nReauthenticating Glue client with new region: us-west-2\nIAM role has been set to arn:aws:iam::043916019468:role/Lab3. Reauthenticating.\nAuthenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::043916019468:role/Lab3\nAuthentication done.\nRegion is set to: us-west-2\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 2\nPrevious Spark configuration: None\nSetting new Spark configuration to: spark.serializer=org.apache.spark.serializer.KryoSerializer\nAdditional python modules to be included:\nFaker\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 2: Define your Imports",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "try:\n    import sys\n    from awsglue.transforms import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.context import SparkContext\n    from awsglue.context import GlueContext\n    from awsglue.job import Job\n    from pyspark.sql.session import SparkSession\n    from awsglue.dynamicframe import DynamicFrame\n    from pyspark.sql.functions import col, to_timestamp, monotonically_increasing_id, to_date, when\n    from pyspark.sql.functions import *\n    from awsglue.utils import getResolvedOptions\n    from pyspark.sql.types import *\n    from datetime import datetime\n    import boto3\n    from functools import reduce\n    from datetime import datetime, timezone\n    from random import randint\n    import datetime\n    from faker import Faker\n    import random\n\nexcept Exception as e:\n    pass",
			"metadata": {
				"editable": true,
				"trusted": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 2\nSession ID: e92cb3e1-2d31-44f9-9b7b-e5e1868311a6\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.37.0\n--enable-glue-datacatalog true\n--conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n--additional-python-modules Faker\nWaiting for session e92cb3e1-2d31-44f9-9b7b-e5e1868311a6 to get into ready status...\nSession e92cb3e1-2d31-44f9-9b7b-e5e1868311a6 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 3: Create Spark Session",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark = SparkSession.builder.config('spark.serializer','org.apache.spark.serializer.KryoSerializer').config('spark.sql.hive.convertMetastoreParquet','false').config('spark.sql.legacy.pathOptionBehavior.enabled', 'true').getOrCreate()\nsc = spark.sparkContext\nglueContext = GlueContext(sc)\njob = Job(glueContext)\nlogger = glueContext.get_logger()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 27,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 4: Read Data from S3 and Performing Upsert on Datalake \n### To practise Concepts we shall generate fake data and work off that ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "def generate_data():\n    states = (\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\",\n              \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\n              \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\",\n              \"WA\", \"WV\", \"WI\", \"WY\")\n    shipping_types = (\"Free\", \"3-Day\", \"2-Day\")\n\n    product_categories = (\"Garden\", \"Kitchen\", \"Office\", \"Household\")\n    referrals = (\"Other\", \"Friend/Colleague\", \"Repeat Customer\", \"Online Ad\")\n\n    data_array = []\n\n    for i in range(0, 100):\n        item_id = random.randint(1, 100)\n        state = states[random.randint(0, len(states) - 1)]\n        shipping_type = shipping_types[random.randint(0, len(shipping_types) - 1)]\n        product_category = product_categories[random.randint(0, len(product_categories) - 1)]\n        quantity = random.randint(1, 4)\n        referral = referrals[random.randint(0, len(referrals) - 1)]\n        price = random.randint(1, 100)\n        order_date = datetime.date(2016, random.randint(1, 12), random.randint(1, 28)).isoformat()\n        invoiceid = random.randint(1, 20000)\n        data_order = (invoiceid, item_id, product_category, price, quantity, order_date, state, shipping_type, referral)\n\n        data_array.append(\n            data_order\n        )\n    columns = [\"invoiceid\", \"itemid\", \"category\", \"price\", \"quantity\", \"orderdate\", \"destinationstate\",\n                   \"shippingtype\", \"referral\"]\n\n    return data_array, columns",
			"metadata": {
				"trusted": true
			},
			"execution_count": 48,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "data_array, columns = generate_data()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 29,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df = spark.createDataFrame(data=data_array, schema=columns)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 30,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.show(3)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 31,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+------+---------+-----+--------+----------+----------------+------------+----------------+\n|invoiceid|itemid| category|price|quantity| orderdate|destinationstate|shippingtype|        referral|\n+---------+------+---------+-----+--------+----------+----------------+------------+----------------+\n|     6142|    36|Household|   12|       1|2016-12-02|              HI|       3-Day|           Other|\n|    18321|    53|   Office|    7|       2|2016-12-07|              AL|       3-Day|Friend/Colleague|\n|     4684|     1|   Office|   58|       3|2016-07-24|              DE|        Free|Friend/Colleague|\n+---------+------+---------+-----+--------+----------+----------------+------------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 32,
			"outputs": [
				{
					"name": "stdout",
					"text": "3\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.printSchema()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 33,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- invoiceid: long (nullable = true)\n |-- itemid: long (nullable = true)\n |-- category: string (nullable = true)\n |-- price: long (nullable = true)\n |-- quantity: long (nullable = true)\n |-- orderdate: string (nullable = true)\n |-- destinationstate: string (nullable = true)\n |-- shippingtype: string (nullable = true)\n |-- referral: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 5: Define Hudi Settings ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "db_name = \"hudidb\"\ntable_name=\"order\"\n\nrecordkey = 'invoiceid'\nprecombine = 'itemid'\n\npath = f\"s3://soumil-dms-learn/hudi/{db_name}/{table_name}/\"\nmethod = 'upsert'                                 \ntable_type = \"COPY_ON_WRITE\"\n\ncurr_session = boto3.session.Session()\ncurr_region = curr_session.region_name\n",
			"metadata": {
				"trusted": true
			},
			"execution_count": 39,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "connection_options={\n    \"path\": path,\n    \"connectionName\": \"hudi-connection\",\n\n    \"hoodie.datasource.write.storage.type\": table_type,\n    'className': 'org.apache.hudi',\n    'hoodie.table.name': table_name,\n    'hoodie.datasource.write.recordkey.field': recordkey,\n    'hoodie.datasource.write.table.name': table_name,\n    'hoodie.datasource.write.operation': method,\n    'hoodie.datasource.write.precombine.field': precombine,\n\n\n    'hoodie.datasource.hive_sync.enable': 'true',\n    \"hoodie.datasource.hive_sync.mode\":\"hms\",\n    'hoodie.datasource.hive_sync.sync_as_datasource': 'false',\n    'hoodie.datasource.hive_sync.database': db_name,\n    'hoodie.datasource.hive_sync.table': table_name,\n    'hoodie.datasource.hive_sync.use_jdbc': 'false',\n    'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor',\n    'hoodie.datasource.write.hive_style_partitioning': 'true',\n    \n    \n    \n    'hoodie.write.concurrency.mode' : 'optimistic_concurrency_control'\n    ,'hoodie.cleaner.policy.failed.writes' : 'LAZY'\n    ,'hoodie.write.lock.provider' : 'org.apache.hudi.aws.transaction.lock.DynamoDBBasedLockProvider'\n    ,'hoodie.write.lock.dynamodb.table' : 'hudi-lock-table'\n    ,'hoodie.write.lock.dynamodb.partition_key' : 'tablename'\n    ,'hoodie.write.lock.dynamodb.region' : '{0}'.format(curr_region)\n    ,'hoodie.write.lock.dynamodb.endpoint_url' : 'dynamodb.{0}.amazonaws.com'.format(curr_region)\n    ,'hoodie.write.lock.dynamodb.billing_mode' : 'PAY_PER_REQUEST'\n    ,'hoodie.bulkinsert.shuffle.parallelism': 2000\n    \n    \n    \n}",
			"metadata": {
				"trusted": true
			},
			"execution_count": 40,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Step 5: Write data into HUDI Table",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "spark_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 41,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+------+---------+-----+--------+----------+----------------+------------+----------------+\n|invoiceid|itemid| category|price|quantity| orderdate|destinationstate|shippingtype|        referral|\n+---------+------+---------+-----+--------+----------+----------------+------------+----------------+\n|     6142|    36|Household|   12|       1|2016-12-02|              HI|       3-Day|           Other|\n|    18321|    53|   Office|    7|       2|2016-12-07|              AL|       3-Day|Friend/Colleague|\n|     4684|     1|   Office|   58|       3|2016-07-24|              DE|        Free|Friend/Colleague|\n+---------+------+---------+-----+--------+----------+----------------+------------+----------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import datetime \nstart = datetime.datetime.now()\nApacheHudiConnector0101forAWSGlue30_node1671045598524 = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(spark_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"glue_df\",\n    )\n)\nend = datetime.datetime.now()\nprint(\"Execution Time: {} \".format(end-start))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 42,
			"outputs": [
				{
					"name": "stdout",
					"text": "Execution Time: 0:00:09.373432\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "print(\"ok\")",
			"metadata": {
				"trusted": true
			},
			"execution_count": 43,
			"outputs": [
				{
					"name": "stdout",
					"text": "ok\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "# Append Operations",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "data_array, columns = generate_data()\nspark_df = spark.createDataFrame(data=data_array, schema=columns)",
			"metadata": {
				"trusted": true
			},
			"execution_count": 49,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.show()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 50,
			"outputs": [
				{
					"name": "stdout",
					"text": "+---------+------+---------+-----+--------+----------+----------------+------------+----------------+\n|invoiceid|itemid| category|price|quantity| orderdate|destinationstate|shippingtype|        referral|\n+---------+------+---------+-----+--------+----------+----------------+------------+----------------+\n|    15386|    54|   Office|   74|       4|2016-09-24|              AK|        Free|           Other|\n|    17111|    42|  Kitchen|   56|       4|2016-04-28|              WI|       3-Day|           Other|\n|    13383|    55|   Garden|   84|       1|2016-07-10|              CO|       3-Day|Friend/Colleague|\n|     8424|    25|   Garden|   60|       2|2016-11-18|              MN|       3-Day|Friend/Colleague|\n|     5213|    58|Household|   56|       3|2016-11-13|              WY|       3-Day| Repeat Customer|\n|     5240|    80|   Office|   17|       1|2016-08-20|              KY|        Free|           Other|\n|     6075|    20|   Office|   22|       3|2016-03-01|              OK|        Free|           Other|\n|    11427|    27|  Kitchen|   29|       2|2016-03-05|              LA|       2-Day|           Other|\n|    16931|    47|   Office|   68|       4|2016-01-20|              CO|        Free|       Online Ad|\n|    17631|    70|  Kitchen|   75|       1|2016-10-08|              UT|       3-Day|           Other|\n|    14024|     8|   Office|   63|       2|2016-01-18|              TX|        Free|Friend/Colleague|\n|     9715|    85|   Office|   36|       4|2016-03-25|              NH|       3-Day|           Other|\n|     1273|     3|   Office|   38|       2|2016-02-23|              UT|       2-Day|           Other|\n|    17016|    41|   Office|   32|       3|2016-02-09|              KS|       2-Day|           Other|\n|     5897|    77|  Kitchen|   82|       4|2016-05-23|              OR|       2-Day|           Other|\n|     6829|    77|   Office|   90|       4|2016-10-22|              CT|       3-Day|Friend/Colleague|\n|     5528|    19|  Kitchen|   78|       1|2016-01-01|              NC|        Free|           Other|\n|     5550|    16|   Office|   19|       2|2016-01-15|              IA|       2-Day|       Online Ad|\n|     9854|    79|   Office|   70|       1|2016-11-06|              MN|        Free|       Online Ad|\n|    10729|     4|Household|   60|       1|2016-06-03|              ND|       2-Day|Friend/Colleague|\n+---------+------+---------+-----+--------+----------+----------------+------------+----------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "spark_df.count()",
			"metadata": {
				"trusted": true
			},
			"execution_count": 51,
			"outputs": [
				{
					"name": "stdout",
					"text": "100\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import datetime \nstart = datetime.datetime.now()\nApacheHudiConnector0101forAWSGlue30_node1671045598524 = (\n    glueContext.write_dynamic_frame.from_options(\n        frame=DynamicFrame.fromDF(spark_df, glueContext,\"glue_df\"),\n        connection_type=\"marketplace.spark\",\n        connection_options=connection_options,\n        transformation_ctx=\"glue_df\",\n    )\n)\nend = datetime.datetime.now()\nprint(\"Execution Time: {} \".format(end-start))",
			"metadata": {
				"trusted": true
			},
			"execution_count": 52,
			"outputs": [
				{
					"name": "stdout",
					"text": "Execution Time: 0:00:09.009001\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}