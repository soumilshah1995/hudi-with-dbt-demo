

============================== 2022-12-22 18:04:45.940072 | 5d3987a8-9ebc-4b26-84b4-7129ea3200f1 ==============================
[0m18:04:45.940072 [info ] [MainThread]: Running with dbt=1.3.1
[0m18:04:45.941080 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m18:04:45.941080 [debug] [MainThread]: Tracking: tracking
[0m18:04:45.965072 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD7A21A020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD7A219420>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD7A219870>]}
[0m18:04:46.116085 [debug] [MainThread]: Executing "git --help"
[0m18:04:46.173070 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m18:04:46.173070 [debug] [MainThread]: STDERR: "b''"
[0m18:04:46.174071 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD7A7D73A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD7A7D5900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FD7A7D5810>]}
[0m18:04:46.175070 [debug] [MainThread]: Flushing usage events


============================== 2022-12-22 18:05:56.446362 | 9e370bb8-ef99-4a2a-81c0-3b26832efdca ==============================
[0m18:05:56.446362 [info ] [MainThread]: Running with dbt=1.3.1
[0m18:05:56.447363 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m18:05:56.447363 [debug] [MainThread]: Tracking: tracking
[0m18:05:56.470362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A702859930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7028592D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A70285A080>]}
[0m18:05:56.616362 [debug] [MainThread]: Executing "git --help"
[0m18:05:56.678424 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m18:05:56.678424 [debug] [MainThread]: STDERR: "b''"
[0m18:05:56.682362 [debug] [MainThread]: Acquiring new glue connection "debug"
[0m18:05:56.682362 [debug] [MainThread]: Using glue connection "debug"
[0m18:05:56.682362 [debug] [MainThread]: On debug: select 1 as id
[0m18:05:56.682362 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:05:56.682362 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m18:05:56.683362 [debug] [MainThread]: Glue adapter: No session present, starting one
[0m18:05:56.683362 [debug] [MainThread]: Glue adapter: GlueConnection _start_session called
[0m18:07:06.305211 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m18:07:06.305211 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-690a9486-d5a1-4c28-94f8-3f636b3b569c
[0m18:07:14.766226 [debug] [MainThread]: Glue adapter: GlueConnection cursor called
[0m18:07:14.766226 [debug] [MainThread]: Glue adapter: GlueCursor execute called
[0m18:07:14.767274 [debug] [MainThread]: Glue adapter: GlueCursor remove_comments_header called
[0m18:07:14.767274 [debug] [MainThread]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m18:07:14.767274 [debug] [MainThread]: Glue adapter: client : SqlWrapper2.execute('''select 1 as id''')
[0m18:07:42.182486 [debug] [MainThread]: Glue adapter: {'Statement': {'Id': 2, 'Code': "SqlWrapper2.execute('''select 1 as id''')", 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"id": 1}}], "description": [{"name": "id", "type": "IntegerType"}]}'}, 'ExecutionCount': 2, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671732436012, 'CompletedOn': 1671732462421}, 'ResponseMetadata': {'RequestId': '008da8ba-bc44-4c54-8f26-dc9453b6947a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 18:07:43 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '398', 'connection': 'keep-alive', 'x-amzn-requestid': '008da8ba-bc44-4c54-8f26-dc9453b6947a'}, 'RetryAttempts': 0}}
[0m18:07:42.183531 [debug] [MainThread]: Glue adapter: status = ok
[0m18:07:42.183531 [debug] [MainThread]: Glue adapter: GlueCursor execute successfully
[0m18:07:42.183531 [debug] [MainThread]: SQL status: OK in 105.5 seconds
[0m18:07:42.184471 [debug] [MainThread]: On debug: Close
[0m18:07:42.184471 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m18:07:42.186469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A702F11D20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7038FACE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A7038FA2F0>]}
[0m18:07:42.187470 [debug] [MainThread]: Flushing usage events
[0m18:07:42.966020 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 18:33:47.066653 | 845ed561-b7d2-4026-a830-6cd36c135eb6 ==============================
[0m18:33:47.066653 [info ] [MainThread]: Running with dbt=1.3.1
[0m18:33:47.067645 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m18:33:47.067645 [debug] [MainThread]: Tracking: tracking
[0m18:33:47.090644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F7914A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F7915DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F7917700>]}
[0m18:33:47.101652 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m18:33:47.101652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '845ed561-b7d2-4026-a830-6cd36c135eb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F7998160>]}
[0m18:33:47.153645 [debug] [MainThread]: Parsing macros\adapters.sql
[0m18:33:47.172651 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m18:33:47.173657 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m18:33:47.174652 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m18:33:47.177651 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m18:33:47.190652 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m18:33:47.190652 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m18:33:47.198652 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m18:33:47.200668 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m18:33:47.203644 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m18:33:47.204651 [debug] [MainThread]: Parsing macros\adapters.sql
[0m18:33:47.234652 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m18:33:47.237655 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m18:33:47.244655 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m18:33:47.262654 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m18:33:47.266652 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m18:33:47.267652 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m18:33:47.270153 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m18:33:47.279619 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m18:33:47.286627 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m18:33:47.290619 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m18:33:47.291627 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m18:33:47.292629 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m18:33:47.292629 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m18:33:47.293621 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m18:33:47.294627 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m18:33:47.294627 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m18:33:47.295627 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m18:33:47.299629 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m18:33:47.309621 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m18:33:47.311627 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m18:33:47.312627 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m18:33:47.313626 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m18:33:47.323627 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m18:33:47.330627 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m18:33:47.332627 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m18:33:47.334627 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m18:33:47.339628 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m18:33:47.343627 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m18:33:47.353626 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m18:33:47.355620 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m18:33:47.358626 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m18:33:47.363630 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m18:33:47.367627 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m18:33:47.368627 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m18:33:47.368627 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m18:33:47.369627 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m18:33:47.370627 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m18:33:47.371631 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m18:33:47.372627 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m18:33:47.374626 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m18:33:47.375627 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m18:33:47.378626 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m18:33:47.383626 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m18:33:47.390626 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m18:33:47.392626 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m18:33:47.401627 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m18:33:47.411619 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m18:33:47.415626 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m18:33:47.417626 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m18:33:47.421626 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m18:33:47.423626 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m18:33:47.425626 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m18:33:47.426626 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m18:33:47.430627 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m18:33:47.443624 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m18:33:47.448630 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m18:33:47.457623 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m18:33:47.466628 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m18:33:47.467620 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m18:33:47.479626 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m18:33:47.480631 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m18:33:47.484628 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m18:33:47.485626 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m18:33:47.490626 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m18:33:47.491626 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m18:33:47.492626 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m18:33:47.493619 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m18:33:47.494626 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m18:33:47.495628 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m18:33:47.496634 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m18:33:47.497620 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m18:33:47.502626 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m18:33:47.503626 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m18:33:47.504629 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m18:33:47.505620 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m18:33:47.506628 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m18:33:47.507626 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m18:33:47.508626 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m18:33:47.509630 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m18:33:47.510621 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m18:33:47.511627 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m18:33:47.513630 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m18:33:47.514627 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m18:33:47.515627 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m18:33:47.516626 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m18:33:47.517619 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m18:33:47.517619 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m18:33:47.519627 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m18:33:47.775675 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m18:33:47.835676 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'gold_category_invoice_count' in the 'models' section of file 'models\metrics\schema.yml'
[0m18:33:47.863740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '845ed561-b7d2-4026-a830-6cd36c135eb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F7A94430>]}
[0m18:33:47.869737 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '845ed561-b7d2-4026-a830-6cd36c135eb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F7999840>]}
[0m18:33:47.869737 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m18:33:47.870709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '845ed561-b7d2-4026-a830-6cd36c135eb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F7B07B80>]}
[0m18:33:47.871736 [info ] [MainThread]: 
[0m18:33:47.871736 [debug] [MainThread]: Acquiring new glue connection "master"
[0m18:33:47.873678 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m18:33:47.874757 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:33:47.874757 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m18:33:47.874757 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m18:33:47.874757 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m18:34:22.296939 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m18:34:22.296939 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-df66d3a6-ef48-42c5-bf56-da4e875e851c
[0m18:34:30.823969 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m18:34:31.110226 [debug] [ThreadPool]: On list_hudidb: Close
[0m18:34:31.110226 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m18:34:31.112234 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m18:34:31.113226 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:34:31.113226 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m18:34:31.289749 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m18:34:31.290749 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m18:34:31.290749 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-df66d3a6-ef48-42c5-bf56-da4e875e851c
[0m18:34:33.404534 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m18:34:33.681088 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m18:34:33.681088 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m18:34:33.682087 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '845ed561-b7d2-4026-a830-6cd36c135eb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F80DD2D0>]}
[0m18:34:33.683086 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m18:34:33.683086 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m18:34:33.683086 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:34:33.684086 [info ] [MainThread]: 
[0m18:34:33.690087 [debug] [Thread-1 (]: Began running node model.dbtglue.popular_by_category
[0m18:34:33.690087 [info ] [Thread-1 (]: 1 of 1 START sql table model hudidb.popular_by_category ........................ [RUN]
[0m18:34:33.691087 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.popular_by_category"
[0m18:34:33.691087 [debug] [Thread-1 (]: Began compiling node model.dbtglue.popular_by_category
[0m18:34:33.692086 [debug] [Thread-1 (]: Compiling model.dbtglue.popular_by_category
[0m18:34:33.694092 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.popular_by_category"
[0m18:34:33.697086 [debug] [Thread-1 (]: finished collecting timing info
[0m18:34:33.697086 [debug] [Thread-1 (]: Began executing node model.dbtglue.popular_by_category
[0m18:34:33.704092 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:34:33.704092 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m18:34:33.884585 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m18:34:33.885514 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m18:34:33.885514 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-df66d3a6-ef48-42c5-bf56-da4e875e851c
[0m18:34:35.515159 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:34:36.018448 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table popular_by_category not found.
[0m18:34:36.081449 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:34:36.092449 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.popular_by_category"
[0m18:34:36.095450 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m18:34:36.095450 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m18:34:36.096447 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    source_data.category,
    source_data.total
FROM source_data
  
[0m18:34:36.096447 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:34:36.096447 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m18:34:36.096447 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m18:34:36.097449 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m18:34:36.098449 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    source_data.category,
    source_data.total
FROM source_data
  ''')
[0m18:34:40.906546 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n  \n    \n    \tcreate table hudidb.popular_by_category\n    \n    using PARQUET\n    \n    \n    \n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\n    \n\tas\n\twith source_data as (\n    SELECT\n        category,\n        count(*) as total\n    FROM hudidb.order\n    GROUP BY category\n    order by category asc\n    )\n\nSELECT\n    source_data.category,\n    source_data.total\nFROM source_data\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 6, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': 'An error occurred while calling o70.sql.\n: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=\n\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)\n\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)\n\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)\n\tat org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)\n\tat org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)\n\tat org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)\n\tat scala.collection.immutable.List.foldLeft(List.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:388)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)\n\tat scala.collection.immutable.List.foldLeft(List.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:388)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)\n\t... 142 more\n', 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', 'py4j.protocol.Py4JJavaError: An error occurred while calling o70.sql.\n: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=\n\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)\n\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)\n\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)\n\tat org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)\n\tat org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)\n\tat org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)\n\tat scala.collection.immutable.List.foldLeft(List.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:388)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)\n\tat scala.collection.immutable.List.foldLeft(List.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:388)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)\n\t... 142 more\n\n']}, 'Progress': 1.0, 'StartedOn': 1671734077361, 'CompletedOn': 1671734080978}, 'ResponseMetadata': {'RequestId': '595f2508-8e68-40a7-b296-cbdab335c294', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 18:34:42 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '36472', 'connection': 'keep-alive', 'x-amzn-requestid': '595f2508-8e68-40a7-b296-cbdab335c294'}, 'RetryAttempts': 0}}
[0m18:34:40.906546 [debug] [Thread-1 (]: Glue adapter: status = error
[0m18:34:40.907532 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    source_data.category,
    source_data.total
FROM source_data
  '''), Py4JJavaError: An error occurred while calling o70.sql.
: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)
	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)
	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)
	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
	at scala.collection.immutable.List.foldLeft(List.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
	at scala.collection.immutable.List.foldLeft(List.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)
	... 142 more

[0m18:34:40.931706 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    source_data.category,
    source_data.total
FROM source_data
  '''), Py4JJavaError: An error occurred while calling o70.sql.
: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)
	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)
	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)
	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
	at scala.collection.immutable.List.foldLeft(List.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
	at scala.collection.immutable.List.foldLeft(List.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)
	... 142 more

[0m18:34:40.931706 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    source_data.category,
    source_data.total
FROM source_data
  
[0m18:34:40.932714 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: ROLLBACK
[0m18:34:40.932714 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m18:34:40.932714 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: Close
[0m18:34:40.933706 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m18:34:40.933706 [debug] [Thread-1 (]: finished collecting timing info
[0m18:34:40.934706 [debug] [Thread-1 (]: Database Error in model popular_by_category (models\metrics\popular_by_category.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
  
    
      
      	create table hudidb.popular_by_category
      
      using PARQUET
      
      
      
      LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
      
  	as
  	with source_data as (
      SELECT
          category,
          count(*) as total
      FROM hudidb.order
      GROUP BY category
      order by category asc
      )
  
  SELECT
      source_data.category,
      source_data.total
  FROM source_data
    '''), Py4JJavaError: An error occurred while calling o70.sql.
  : java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=
  	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)
  	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)
  	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)
  	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)
  	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)
  	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)
  	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)
  	at scala.Option.getOrElse(Option.scala:121)
  	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)
  	at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)
  	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)
  	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)
  	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)
  	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
  	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)
  	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
  	at scala.collection.immutable.List.foldLeft(List.scala:85)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  	at scala.collection.immutable.List.foreach(List.scala:388)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
  	at scala.collection.immutable.List.foldLeft(List.scala:85)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  	at scala.collection.immutable.List.foreach(List.scala:388)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)
  	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)
  	... 142 more
  
  compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m18:34:40.934706 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '845ed561-b7d2-4026-a830-6cd36c135eb6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F867A140>]}
[0m18:34:40.935707 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model hudidb.popular_by_category ............... [[31mERROR[0m in 7.24s]
[0m18:34:40.936712 [debug] [Thread-1 (]: Finished running node model.dbtglue.popular_by_category
[0m18:34:40.937707 [debug] [MainThread]: Acquiring new glue connection "master"
[0m18:34:40.938708 [debug] [MainThread]: On master: ROLLBACK
[0m18:34:40.938708 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:34:40.938708 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m18:34:41.130887 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m18:34:41.131909 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m18:34:41.131909 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-df66d3a6-ef48-42c5-bf56-da4e875e851c
[0m18:34:42.742979 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m18:34:42.742979 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m18:34:42.742979 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m18:34:42.743987 [debug] [MainThread]: On master: ROLLBACK
[0m18:34:42.743987 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m18:34:42.743987 [debug] [MainThread]: On master: Close
[0m18:34:42.743987 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m18:34:42.744986 [info ] [MainThread]: 
[0m18:34:42.744986 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 54.87 seconds (54.87s).
[0m18:34:42.745979 [debug] [MainThread]: Glue adapter: cleanup called
[0m18:34:42.989671 [info ] [MainThread]: 
[0m18:34:42.990672 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:34:42.991590 [info ] [MainThread]: 
[0m18:34:42.992633 [error] [MainThread]: [33mDatabase Error in model popular_by_category (models\metrics\popular_by_category.sql)[0m
[0m18:34:42.992633 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
[0m18:34:42.993596 [error] [MainThread]:   
[0m18:34:42.993596 [error] [MainThread]:     
[0m18:34:42.994601 [error] [MainThread]:       
[0m18:34:42.994601 [error] [MainThread]:       	create table hudidb.popular_by_category
[0m18:34:42.995594 [error] [MainThread]:       
[0m18:34:42.995594 [error] [MainThread]:       using PARQUET
[0m18:34:42.995594 [error] [MainThread]:       
[0m18:34:42.996595 [error] [MainThread]:       
[0m18:34:42.996595 [error] [MainThread]:       
[0m18:34:42.996595 [error] [MainThread]:       LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
[0m18:34:42.997594 [error] [MainThread]:       
[0m18:34:42.997594 [error] [MainThread]:   	as
[0m18:34:42.998596 [error] [MainThread]:   	with source_data as (
[0m18:34:42.998596 [error] [MainThread]:       SELECT
[0m18:34:42.999594 [error] [MainThread]:           category,
[0m18:34:42.999594 [error] [MainThread]:           count(*) as total
[0m18:34:42.999594 [error] [MainThread]:       FROM hudidb.order
[0m18:34:43.000595 [error] [MainThread]:       GROUP BY category
[0m18:34:43.000595 [error] [MainThread]:       order by category asc
[0m18:34:43.000595 [error] [MainThread]:       )
[0m18:34:43.000595 [error] [MainThread]:   
[0m18:34:43.001594 [error] [MainThread]:   SELECT
[0m18:34:43.001594 [error] [MainThread]:       source_data.category,
[0m18:34:43.002596 [error] [MainThread]:       source_data.total
[0m18:34:43.002596 [error] [MainThread]:   FROM source_data
[0m18:34:43.002596 [error] [MainThread]:     '''), Py4JJavaError: An error occurred while calling o70.sql.
[0m18:34:43.003606 [error] [MainThread]:   : java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=
[0m18:34:43.003606 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)
[0m18:34:43.004594 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)
[0m18:34:43.004594 [error] [MainThread]:   	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)
[0m18:34:43.004594 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)
[0m18:34:43.005616 [error] [MainThread]:   	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)
[0m18:34:43.005616 [error] [MainThread]:   	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)
[0m18:34:43.005616 [error] [MainThread]:   	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)
[0m18:34:43.006595 [error] [MainThread]:   	at scala.Option.getOrElse(Option.scala:121)
[0m18:34:43.006595 [error] [MainThread]:   	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)
[0m18:34:43.006595 [error] [MainThread]:   	at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)
[0m18:34:43.007595 [error] [MainThread]:   	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)
[0m18:34:43.007595 [error] [MainThread]:   	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)
[0m18:34:43.007595 [error] [MainThread]:   	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)
[0m18:34:43.008595 [error] [MainThread]:   	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)
[0m18:34:43.008595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)
[0m18:34:43.009595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
[0m18:34:43.009595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)
[0m18:34:43.010595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:34:43.010595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
[0m18:34:43.010595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
[0m18:34:43.011594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
[0m18:34:43.011594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
[0m18:34:43.012595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
[0m18:34:43.012595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
[0m18:34:43.013620 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
[0m18:34:43.013620 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
[0m18:34:43.013620 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
[0m18:34:43.014595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
[0m18:34:43.014595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:34:43.015595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
[0m18:34:43.015595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
[0m18:34:43.015595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
[0m18:34:43.016595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
[0m18:34:43.016595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
[0m18:34:43.016595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
[0m18:34:43.017595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
[0m18:34:43.017595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
[0m18:34:43.017595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
[0m18:34:43.018594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
[0m18:34:43.018594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:34:43.018594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
[0m18:34:43.019594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
[0m18:34:43.019594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
[0m18:34:43.020601 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)
[0m18:34:43.020601 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)
[0m18:34:43.020601 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
[0m18:34:43.021595 [error] [MainThread]:   	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)
[0m18:34:43.021595 [error] [MainThread]:   	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)
[0m18:34:43.021595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
[0m18:34:43.022595 [error] [MainThread]:   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
[0m18:34:43.022595 [error] [MainThread]:   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
[0m18:34:43.022595 [error] [MainThread]:   	at scala.collection.immutable.List.foldLeft(List.scala:85)
[0m18:34:43.023594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
[0m18:34:43.023594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
[0m18:34:43.024594 [error] [MainThread]:   	at scala.collection.immutable.List.foreach(List.scala:388)
[0m18:34:43.024594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
[0m18:34:43.024594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
[0m18:34:43.025595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)
[0m18:34:43.025595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)
[0m18:34:43.026595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
[0m18:34:43.026595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
[0m18:34:43.026595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
[0m18:34:43.027595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:34:43.027595 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
[0m18:34:43.028594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
[0m18:34:43.028594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
[0m18:34:43.028594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
[0m18:34:43.029594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
[0m18:34:43.029594 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
[0m18:34:43.030606 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
[0m18:34:43.030606 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
[0m18:34:43.030606 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
[0m18:34:43.043860 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
[0m18:34:43.044777 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:34:43.044777 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
[0m18:34:43.045775 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
[0m18:34:43.045775 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
[0m18:34:43.046774 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
[0m18:34:43.046774 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
[0m18:34:43.046774 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
[0m18:34:43.047774 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
[0m18:34:43.047774 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
[0m18:34:43.047774 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
[0m18:34:43.048774 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
[0m18:34:43.048774 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:34:43.049776 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
[0m18:34:43.049776 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
[0m18:34:43.049776 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
[0m18:34:43.050775 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
[0m18:34:43.050775 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
[0m18:34:43.051775 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)
[0m18:34:43.051775 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
[0m18:34:43.051775 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
[0m18:34:43.052774 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
[0m18:34:43.052774 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
[0m18:34:43.053845 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:34:43.053845 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
[0m18:34:43.053845 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
[0m18:34:43.054880 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
[0m18:34:43.054880 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)
[0m18:34:43.054880 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)
[0m18:34:43.055854 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
[0m18:34:43.055854 [error] [MainThread]:   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
[0m18:34:43.056849 [error] [MainThread]:   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
[0m18:34:43.056849 [error] [MainThread]:   	at scala.collection.immutable.List.foldLeft(List.scala:85)
[0m18:34:43.057883 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
[0m18:34:43.058863 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
[0m18:34:43.058863 [error] [MainThread]:   	at scala.collection.immutable.List.foreach(List.scala:388)
[0m18:34:43.059779 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
[0m18:34:43.060463 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
[0m18:34:43.060463 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
[0m18:34:43.061476 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
[0m18:34:43.061476 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
[0m18:34:43.062543 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
[0m18:34:43.062543 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
[0m18:34:43.062543 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
[0m18:34:43.063546 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
[0m18:34:43.063546 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
[0m18:34:43.063546 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
[0m18:34:43.064548 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
[0m18:34:43.064548 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
[0m18:34:43.065541 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m18:34:43.065541 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
[0m18:34:43.066560 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
[0m18:34:43.066560 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
[0m18:34:43.066560 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
[0m18:34:43.067536 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
[0m18:34:43.067536 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m18:34:43.067536 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
[0m18:34:43.068539 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
[0m18:34:43.068539 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m18:34:43.068539 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
[0m18:34:43.069538 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[0m18:34:43.069538 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[0m18:34:43.069538 [error] [MainThread]:   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[0m18:34:43.070550 [error] [MainThread]:   	at java.lang.reflect.Method.invoke(Method.java:498)
[0m18:34:43.070550 [error] [MainThread]:   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[0m18:34:43.070550 [error] [MainThread]:   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[0m18:34:43.071543 [error] [MainThread]:   	at py4j.Gateway.invoke(Gateway.java:282)
[0m18:34:43.071543 [error] [MainThread]:   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[0m18:34:43.071543 [error] [MainThread]:   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[0m18:34:43.072564 [error] [MainThread]:   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[0m18:34:43.072564 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:750)
[0m18:34:43.072564 [error] [MainThread]:   Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: KBQ02JN1EPQJNSBV; S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=; Proxy: null), S3 Extended Request ID: DdeqO1CsfGzhVH74IYcF+RykQ3Sl83sU3Y3B4641K6HgkkFsH7W1upAidUM7v8/tcJjGE/HluPw=
[0m18:34:43.073547 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
[0m18:34:43.073547 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
[0m18:34:43.073547 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
[0m18:34:43.074548 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
[0m18:34:43.074548 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
[0m18:34:43.074548 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[0m18:34:43.075562 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[0m18:34:43.076616 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[0m18:34:43.077206 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[0m18:34:43.077778 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[0m18:34:43.078321 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[0m18:34:43.078858 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)
[0m18:34:43.078858 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)
[0m18:34:43.079396 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)
[0m18:34:43.079913 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)
[0m18:34:43.079913 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)
[0m18:34:43.080474 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)
[0m18:34:43.081006 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)
[0m18:34:43.081648 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)
[0m18:34:43.081648 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)
[0m18:34:43.082186 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)
[0m18:34:43.082722 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)
[0m18:34:43.083255 [error] [MainThread]:   	... 142 more
[0m18:34:43.083255 [error] [MainThread]:   
[0m18:34:43.083787 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m18:34:43.084314 [info ] [MainThread]: 
[0m18:34:43.084825 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:34:43.084851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F7A966E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F86AB670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000270F870AF80>]}
[0m18:34:43.085384 [debug] [MainThread]: Flushing usage events
[0m18:34:43.301274 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 18:35:33.088088 | 3914c75b-f1d0-450d-a842-322dea35cb44 ==============================
[0m18:35:33.088088 [info ] [MainThread]: Running with dbt=1.3.1
[0m18:35:33.088088 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m18:35:33.089088 [debug] [MainThread]: Tracking: tracking
[0m18:35:33.110091 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5D5984A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5D5985DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5D5987700>]}
[0m18:35:33.201087 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:35:33.201087 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\popular_by_category.sql
[0m18:35:33.213089 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m18:35:33.241740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3914c75b-f1d0-450d-a842-322dea35cb44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5D5BE4DC0>]}
[0m18:35:33.248735 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3914c75b-f1d0-450d-a842-322dea35cb44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5D5B7BFA0>]}
[0m18:35:33.249727 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m18:35:33.249727 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3914c75b-f1d0-450d-a842-322dea35cb44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5D5B31510>]}
[0m18:35:33.250728 [info ] [MainThread]: 
[0m18:35:33.251734 [debug] [MainThread]: Acquiring new glue connection "master"
[0m18:35:33.253735 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m18:35:33.253735 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:35:33.253735 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m18:35:33.253735 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m18:35:33.254727 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m18:35:50.168152 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m18:35:50.168152 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-697b2ed6-ef87-44cc-a13f-bbbb421bdb6e
[0m18:35:57.560421 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m18:35:57.812772 [debug] [ThreadPool]: On list_hudidb: Close
[0m18:35:57.813776 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m18:35:57.814705 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m18:35:57.815704 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:35:57.815704 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m18:35:58.068601 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m18:35:58.069605 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m18:35:58.069605 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-697b2ed6-ef87-44cc-a13f-bbbb421bdb6e
[0m18:35:59.722128 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m18:35:59.970805 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m18:35:59.970805 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m18:35:59.971791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3914c75b-f1d0-450d-a842-322dea35cb44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5D5B63640>]}
[0m18:35:59.972816 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m18:35:59.972816 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m18:35:59.973794 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:35:59.974795 [info ] [MainThread]: 
[0m18:35:59.980664 [debug] [Thread-1 (]: Began running node model.dbtglue.popular_by_category
[0m18:35:59.980664 [info ] [Thread-1 (]: 1 of 1 START sql table model hudidb.popular_by_category ........................ [RUN]
[0m18:35:59.981689 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.popular_by_category"
[0m18:35:59.981689 [debug] [Thread-1 (]: Began compiling node model.dbtglue.popular_by_category
[0m18:35:59.981689 [debug] [Thread-1 (]: Compiling model.dbtglue.popular_by_category
[0m18:35:59.983695 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.popular_by_category"
[0m18:35:59.984694 [debug] [Thread-1 (]: finished collecting timing info
[0m18:35:59.984694 [debug] [Thread-1 (]: Began executing node model.dbtglue.popular_by_category
[0m18:35:59.990694 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:35:59.991694 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m18:36:01.159332 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m18:36:01.159332 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m18:36:01.159332 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-697b2ed6-ef87-44cc-a13f-bbbb421bdb6e
[0m18:36:02.821139 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:36:03.318141 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table popular_by_category not found.
[0m18:36:03.348792 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:36:03.352789 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.popular_by_category"
[0m18:36:03.353802 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m18:36:03.353802 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m18:36:03.353802 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  
[0m18:36:03.353802 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:36:03.354797 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m18:36:03.354797 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m18:36:03.354797 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m18:36:03.354797 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  ''')
[0m18:36:07.015689 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n  \n    \n    \tcreate table hudidb.popular_by_category\n    \n    using PARQUET\n    \n    \n    \n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\n    \n\tas\n\twith source_data as (\n    SELECT\n        category,\n        count(*) as total\n    FROM hudidb.order\n    GROUP BY category\n    order by category asc\n    )\n\nSELECT\n    *\nFROM source_data\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 6, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': 'An error occurred while calling o70.sql.\n: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=\n\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)\n\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)\n\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)\n\tat org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)\n\tat org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)\n\tat org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)\n\tat scala.collection.immutable.List.foldLeft(List.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:388)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)\n\tat scala.collection.immutable.List.foldLeft(List.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:388)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)\n\t... 142 more\n', 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', 'py4j.protocol.Py4JJavaError: An error occurred while calling o70.sql.\n: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=\n\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)\n\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)\n\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)\n\tat org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)\n\tat org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)\n\tat org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)\n\tat org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)\n\tat scala.collection.immutable.List.foldLeft(List.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:388)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)\n\tat scala.collection.immutable.List.foldLeft(List.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:388)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)\n\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)\n\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)\n\t... 142 more\n\n']}, 'Progress': 1.0, 'StartedOn': 1671734164612, 'CompletedOn': 1671734167138}, 'ResponseMetadata': {'RequestId': '52ad2548-02e1-4338-b6a4-8db2a0ff2797', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 18:36:08 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '36429', 'connection': 'keep-alive', 'x-amzn-requestid': '52ad2548-02e1-4338-b6a4-8db2a0ff2797'}, 'RetryAttempts': 0}}
[0m18:36:07.015689 [debug] [Thread-1 (]: Glue adapter: status = error
[0m18:36:07.016690 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  '''), Py4JJavaError: An error occurred while calling o70.sql.
: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)
	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)
	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)
	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
	at scala.collection.immutable.List.foldLeft(List.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
	at scala.collection.immutable.List.foldLeft(List.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)
	... 142 more

[0m18:36:07.025686 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  '''), Py4JJavaError: An error occurred while calling o70.sql.
: java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)
	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)
	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)
	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)
	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)
	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)
	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)
	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
	at scala.collection.immutable.List.foldLeft(List.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
	at scala.collection.immutable.List.foldLeft(List.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)
	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)
	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)
	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)
	... 142 more

[0m18:36:07.026687 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  
[0m18:36:07.026687 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: ROLLBACK
[0m18:36:07.026687 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m18:36:07.026687 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: Close
[0m18:36:07.026687 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m18:36:07.027686 [debug] [Thread-1 (]: finished collecting timing info
[0m18:36:07.027686 [debug] [Thread-1 (]: Database Error in model popular_by_category (models\metrics\popular_by_category.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
  
    
      
      	create table hudidb.popular_by_category
      
      using PARQUET
      
      
      
      LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
      
  	as
  	with source_data as (
      SELECT
          category,
          count(*) as total
      FROM hudidb.order
      GROUP BY category
      order by category asc
      )
  
  SELECT
      *
  FROM source_data
    '''), Py4JJavaError: An error occurred while calling o70.sql.
  : java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=
  	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)
  	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)
  	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)
  	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)
  	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)
  	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)
  	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)
  	at scala.Option.getOrElse(Option.scala:121)
  	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)
  	at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)
  	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)
  	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)
  	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)
  	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
  	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)
  	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
  	at scala.collection.immutable.List.foldLeft(List.scala:85)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  	at scala.collection.immutable.List.foreach(List.scala:388)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
  	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
  	at scala.collection.immutable.List.foldLeft(List.scala:85)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  	at scala.collection.immutable.List.foreach(List.scala:388)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)
  	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)
  	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)
  	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)
  	... 142 more
  
  compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m18:36:07.028686 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3914c75b-f1d0-450d-a842-322dea35cb44', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5D670DF60>]}
[0m18:36:07.028686 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model hudidb.popular_by_category ............... [[31mERROR[0m in 7.05s]
[0m18:36:07.029686 [debug] [Thread-1 (]: Finished running node model.dbtglue.popular_by_category
[0m18:36:07.031701 [debug] [MainThread]: Acquiring new glue connection "master"
[0m18:36:07.031701 [debug] [MainThread]: On master: ROLLBACK
[0m18:36:07.031701 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:36:07.031701 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m18:36:07.225861 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m18:36:07.226927 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m18:36:07.226927 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-697b2ed6-ef87-44cc-a13f-bbbb421bdb6e
[0m18:36:08.878390 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m18:36:08.879390 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m18:36:08.879390 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m18:36:08.879390 [debug] [MainThread]: On master: ROLLBACK
[0m18:36:08.880391 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m18:36:08.880391 [debug] [MainThread]: On master: Close
[0m18:36:08.880391 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m18:36:08.881391 [info ] [MainThread]: 
[0m18:36:08.881391 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 35.63 seconds (35.63s).
[0m18:36:08.882391 [debug] [MainThread]: Glue adapter: cleanup called
[0m18:36:09.563292 [info ] [MainThread]: 
[0m18:36:09.564292 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m18:36:09.565318 [info ] [MainThread]: 
[0m18:36:09.566284 [error] [MainThread]: [33mDatabase Error in model popular_by_category (models\metrics\popular_by_category.sql)[0m
[0m18:36:09.567306 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
[0m18:36:09.567306 [error] [MainThread]:   
[0m18:36:09.568325 [error] [MainThread]:     
[0m18:36:09.568325 [error] [MainThread]:       
[0m18:36:09.568325 [error] [MainThread]:       	create table hudidb.popular_by_category
[0m18:36:09.569311 [error] [MainThread]:       
[0m18:36:09.569311 [error] [MainThread]:       using PARQUET
[0m18:36:09.569311 [error] [MainThread]:       
[0m18:36:09.570311 [error] [MainThread]:       
[0m18:36:09.570311 [error] [MainThread]:       
[0m18:36:09.570311 [error] [MainThread]:       LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
[0m18:36:09.571311 [error] [MainThread]:       
[0m18:36:09.571311 [error] [MainThread]:   	as
[0m18:36:09.571311 [error] [MainThread]:   	with source_data as (
[0m18:36:09.572311 [error] [MainThread]:       SELECT
[0m18:36:09.573311 [error] [MainThread]:           category,
[0m18:36:09.573311 [error] [MainThread]:           count(*) as total
[0m18:36:09.573311 [error] [MainThread]:       FROM hudidb.order
[0m18:36:09.574311 [error] [MainThread]:       GROUP BY category
[0m18:36:09.574311 [error] [MainThread]:       order by category asc
[0m18:36:09.574311 [error] [MainThread]:       )
[0m18:36:09.575311 [error] [MainThread]:   
[0m18:36:09.575311 [error] [MainThread]:   SELECT
[0m18:36:09.575311 [error] [MainThread]:       *
[0m18:36:09.576311 [error] [MainThread]:   FROM source_data
[0m18:36:09.576311 [error] [MainThread]:     '''), Py4JJavaError: An error occurred while calling o70.sql.
[0m18:36:09.576311 [error] [MainThread]:   : java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=
[0m18:36:09.577311 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:303)
[0m18:36:09.577311 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:510)
[0m18:36:09.577311 [error] [MainThread]:   	at org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)
[0m18:36:09.578311 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.EmrFileSystem.isDirectory(EmrFileSystem.java:442)
[0m18:36:09.578311 [error] [MainThread]:   	at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)
[0m18:36:09.578311 [error] [MainThread]:   	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)
[0m18:36:09.579311 [error] [MainThread]:   	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$5(HiveMetastoreCatalog.scala:249)
[0m18:36:09.579311 [error] [MainThread]:   	at scala.Option.getOrElse(Option.scala:121)
[0m18:36:09.579311 [error] [MainThread]:   	at org.apache.spark.sql.hive.HiveMetastoreCatalog.$anonfun$convertToLogicalRelation$4(HiveMetastoreCatalog.scala:239)
[0m18:36:09.580311 [error] [MainThread]:   	at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:58)
[0m18:36:09.580311 [error] [MainThread]:   	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:232)
[0m18:36:09.580311 [error] [MainThread]:   	at org.apache.spark.sql.hive.HiveMetastoreCatalog.convert(HiveMetastoreCatalog.scala:137)
[0m18:36:09.581311 [error] [MainThread]:   	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:233)
[0m18:36:09.581311 [error] [MainThread]:   	at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:213)
[0m18:36:09.581311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$2(AnalysisHelper.scala:108)
[0m18:36:09.582311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
[0m18:36:09.582311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:108)
[0m18:36:09.583312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:36:09.583312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
[0m18:36:09.583312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
[0m18:36:09.584311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
[0m18:36:09.584311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
[0m18:36:09.584311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
[0m18:36:09.585323 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
[0m18:36:09.585323 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
[0m18:36:09.585323 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
[0m18:36:09.586311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
[0m18:36:09.586311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
[0m18:36:09.586311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:36:09.587311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
[0m18:36:09.587311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
[0m18:36:09.588311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
[0m18:36:09.588311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$4(AnalysisHelper.scala:113)
[0m18:36:09.588311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
[0m18:36:09.589312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
[0m18:36:09.589312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
[0m18:36:09.589312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
[0m18:36:09.590311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
[0m18:36:09.590311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDown$1(AnalysisHelper.scala:113)
[0m18:36:09.590311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:36:09.591311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown(AnalysisHelper.scala:106)
[0m18:36:09.591311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDown$(AnalysisHelper.scala:104)
[0m18:36:09.592311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsDown(LogicalPlan.scala:29)
[0m18:36:09.592311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators(AnalysisHelper.scala:73)
[0m18:36:09.592311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperators$(AnalysisHelper.scala:72)
[0m18:36:09.593311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:29)
[0m18:36:09.593311 [error] [MainThread]:   	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:213)
[0m18:36:09.593311 [error] [MainThread]:   	at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:198)
[0m18:36:09.594312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
[0m18:36:09.594312 [error] [MainThread]:   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
[0m18:36:09.594312 [error] [MainThread]:   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
[0m18:36:09.595312 [error] [MainThread]:   	at scala.collection.immutable.List.foldLeft(List.scala:85)
[0m18:36:09.595312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
[0m18:36:09.596311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
[0m18:36:09.596311 [error] [MainThread]:   	at scala.collection.immutable.List.foreach(List.scala:388)
[0m18:36:09.596311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
[0m18:36:09.597315 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
[0m18:36:09.597315 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2354)
[0m18:36:09.598311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$$anonfun$apply$21.applyOrElse(Analyzer.scala:2329)
[0m18:36:09.598311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
[0m18:36:09.598311 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
[0m18:36:09.599312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
[0m18:36:09.599312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:36:09.599312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
[0m18:36:09.600312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
[0m18:36:09.600312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
[0m18:36:09.600312 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
[0m18:36:09.601515 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
[0m18:36:09.601515 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
[0m18:36:09.601515 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
[0m18:36:09.602575 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
[0m18:36:09.602575 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
[0m18:36:09.602575 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
[0m18:36:09.603577 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:36:09.603577 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
[0m18:36:09.604527 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
[0m18:36:09.604527 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
[0m18:36:09.605536 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
[0m18:36:09.605536 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
[0m18:36:09.606526 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:423)
[0m18:36:09.606526 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
[0m18:36:09.606526 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
[0m18:36:09.607535 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
[0m18:36:09.607535 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
[0m18:36:09.608532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:36:09.608532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
[0m18:36:09.608532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
[0m18:36:09.609532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
[0m18:36:09.609532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$2(AnalysisHelper.scala:87)
[0m18:36:09.609532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:387)
[0m18:36:09.610532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:425)
[0m18:36:09.610532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:255)
[0m18:36:09.610532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:421)
[0m18:36:09.611524 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:369)
[0m18:36:09.611524 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:87)
[0m18:36:09.611524 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m18:36:09.612532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
[0m18:36:09.612532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
[0m18:36:09.613532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
[0m18:36:09.613532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2329)
[0m18:36:09.613532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAggregateFunctions$.apply(Analyzer.scala:2328)
[0m18:36:09.614532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
[0m18:36:09.614532 [error] [MainThread]:   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
[0m18:36:09.614532 [error] [MainThread]:   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
[0m18:36:09.615533 [error] [MainThread]:   	at scala.collection.immutable.List.foldLeft(List.scala:85)
[0m18:36:09.615533 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
[0m18:36:09.616526 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
[0m18:36:09.616526 [error] [MainThread]:   	at scala.collection.immutable.List.foreach(List.scala:388)
[0m18:36:09.616526 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
[0m18:36:09.617535 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
[0m18:36:09.617535 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
[0m18:36:09.617535 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
[0m18:36:09.618532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
[0m18:36:09.618532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
[0m18:36:09.618532 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
[0m18:36:09.619524 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
[0m18:36:09.619524 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
[0m18:36:09.620547 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
[0m18:36:09.621535 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
[0m18:36:09.621535 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
[0m18:36:09.622532 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
[0m18:36:09.622532 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m18:36:09.622532 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
[0m18:36:09.623532 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
[0m18:36:09.623532 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
[0m18:36:09.623532 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
[0m18:36:09.624532 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
[0m18:36:09.624532 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m18:36:09.625541 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
[0m18:36:09.625541 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
[0m18:36:09.625541 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m18:36:09.626533 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
[0m18:36:09.626533 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[0m18:36:09.627532 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[0m18:36:09.627532 [error] [MainThread]:   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[0m18:36:09.627532 [error] [MainThread]:   	at java.lang.reflect.Method.invoke(Method.java:498)
[0m18:36:09.628528 [error] [MainThread]:   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[0m18:36:09.628528 [error] [MainThread]:   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[0m18:36:09.628528 [error] [MainThread]:   	at py4j.Gateway.invoke(Gateway.java:282)
[0m18:36:09.629524 [error] [MainThread]:   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[0m18:36:09.629524 [error] [MainThread]:   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[0m18:36:09.629524 [error] [MainThread]:   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[0m18:36:09.630524 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:750)
[0m18:36:09.630524 [error] [MainThread]:   Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: X3R1TP606E2YFWDN; S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=; Proxy: null), S3 Extended Request ID: YXq0JZAADRUcxlDqJJyEDH7q7oGbE+tCWBHfwusvb7Jh/rs7Ho/eCIfe+4+8ng/kXMpqk5oxg9Y=
[0m18:36:09.631535 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1819)
[0m18:36:09.631535 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1403)
[0m18:36:09.631535 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1372)
[0m18:36:09.632525 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)
[0m18:36:09.632525 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:802)
[0m18:36:09.632525 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:770)
[0m18:36:09.633525 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:744)
[0m18:36:09.633525 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:704)
[0m18:36:09.634530 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:686)
[0m18:36:09.634530 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:550)
[0m18:36:09.634530 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:530)
[0m18:36:09.635532 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5140)
[0m18:36:09.635532 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5086)
[0m18:36:09.636526 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5080)
[0m18:36:09.636526 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:941)
[0m18:36:09.636526 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)
[0m18:36:09.637524 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)
[0m18:36:09.637524 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:114)
[0m18:36:09.638526 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)
[0m18:36:09.638526 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:186)
[0m18:36:09.638526 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)
[0m18:36:09.639532 [error] [MainThread]:   	at com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:294)
[0m18:36:09.639532 [error] [MainThread]:   	... 142 more
[0m18:36:09.639532 [error] [MainThread]:   
[0m18:36:09.640532 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m18:36:09.640532 [info ] [MainThread]: 
[0m18:36:09.640532 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m18:36:09.641524 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5D66C8A00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5D66CABF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A5D6737A30>]}
[0m18:36:09.641524 [debug] [MainThread]: Flushing usage events
[0m18:36:09.881597 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 18:40:58.604918 | beb8155a-db95-467b-bbaa-470bce822517 ==============================
[0m18:40:58.604918 [info ] [MainThread]: Running with dbt=1.3.1
[0m18:40:58.605920 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m18:40:58.606921 [debug] [MainThread]: Tracking: tracking
[0m18:40:58.630962 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA1FF4A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA1FF5DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA1FF7700>]}
[0m18:40:58.708924 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:40:58.709924 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:40:58.713927 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'beb8155a-db95-467b-bbaa-470bce822517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA21F5E40>]}
[0m18:40:58.719925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'beb8155a-db95-467b-bbaa-470bce822517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA21BDAE0>]}
[0m18:40:58.719925 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m18:40:58.720925 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'beb8155a-db95-467b-bbaa-470bce822517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA21BDD20>]}
[0m18:40:58.721920 [info ] [MainThread]: 
[0m18:40:58.722919 [debug] [MainThread]: Acquiring new glue connection "master"
[0m18:40:58.723956 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m18:40:58.724487 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:40:58.724487 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m18:40:58.724487 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m18:40:58.724487 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m18:41:49.711699 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m18:41:49.711699 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-2bce5fc8-7894-49e6-9b4f-26554918b66a
[0m18:41:58.844407 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m18:41:59.157643 [debug] [ThreadPool]: On list_hudidb: Close
[0m18:41:59.158634 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m18:41:59.159504 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m18:41:59.160567 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:41:59.160567 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m18:41:59.397835 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m18:41:59.398833 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m18:41:59.398833 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-2bce5fc8-7894-49e6-9b4f-26554918b66a
[0m18:42:01.102574 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m18:42:02.014642 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m18:42:02.014642 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m18:42:02.015634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'beb8155a-db95-467b-bbaa-470bce822517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA2689750>]}
[0m18:42:02.016643 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m18:42:02.016643 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m18:42:02.016643 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:42:02.017635 [info ] [MainThread]: 
[0m18:42:02.022634 [debug] [Thread-1 (]: Began running node model.dbtglue.popular_by_category
[0m18:42:02.022634 [info ] [Thread-1 (]: 1 of 1 START sql table model hudidb.popular_by_category ........................ [RUN]
[0m18:42:02.023634 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.popular_by_category"
[0m18:42:02.023634 [debug] [Thread-1 (]: Began compiling node model.dbtglue.popular_by_category
[0m18:42:02.024635 [debug] [Thread-1 (]: Compiling model.dbtglue.popular_by_category
[0m18:42:02.026635 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.popular_by_category"
[0m18:42:02.027635 [debug] [Thread-1 (]: finished collecting timing info
[0m18:42:02.027635 [debug] [Thread-1 (]: Began executing node model.dbtglue.popular_by_category
[0m18:42:02.034642 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:42:02.035642 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m18:42:02.187854 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m18:42:02.188854 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m18:42:02.188854 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-2bce5fc8-7894-49e6-9b4f-26554918b66a
[0m18:42:03.856332 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:42:04.107586 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table popular_by_category not found.
[0m18:42:04.134587 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:42:04.138587 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.popular_by_category"
[0m18:42:04.139587 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m18:42:04.139587 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m18:42:04.139587 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  
[0m18:42:04.139587 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:42:04.139587 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m18:42:04.139587 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m18:42:04.140587 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m18:42:04.140587 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  ''')
[0m18:42:35.357187 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n  \n    \n    \tcreate table hudidb.popular_by_category\n    \n    using PARQUET\n    \n    \n    \n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\n    \n\tas\n\twith source_data as (\n    SELECT\n        category,\n        count(*) as total\n    FROM hudidb.order\n    GROUP BY category\n    order by category asc\n    )\n\nSELECT\n    *\nFROM source_data\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n  \\n    \\n    \\tcreate table hudidb.popular_by_category\\n    \\n    using PARQUET\\n    \\n    \\n    \\n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\\n    \\n\\tas\\n\\twith source_data as (\\n    SELECT\\n        category,\\n        count(*) as total\\n    FROM hudidb.order\\n    GROUP BY category\\n    order by category asc\\n    )\\n\\nSELECT\\n    *\\nFROM source_data\\n  ", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671734525967, 'CompletedOn': 1671734556238}, 'ResponseMetadata': {'RequestId': '8e850786-df88-4ac7-90ad-f4db30a80c5d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 18:42:36 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1547', 'connection': 'keep-alive', 'x-amzn-requestid': '8e850786-df88-4ac7-90ad-f4db30a80c5d'}, 'RetryAttempts': 0}}
[0m18:42:35.357187 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m18:42:35.357187 [debug] [Thread-1 (]: Glue adapter: chunks: ['{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n  \\n    \\n    \\tcreate table hudidb.popular_by_category\\n    \\n    using PARQUET\\n    \\n    \\n    \\n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\\n    \\n\\tas\\n\\twith source_data as (\\n    SELECT\\n        category,\\n        count(*) as total\\n    FROM hudidb.order\\n    GROUP BY category\\n    order by category asc\\n    )\\n\\nSELECT\\n    *\\nFROM source_data\\n  ", "schema": null, "results": null}', '{"type": "results", "rowcount": 0, "results": [], "description": []}']
[0m18:42:35.358181 [debug] [Thread-1 (]: Glue adapter: response: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n  \n    \n    \tcreate table hudidb.popular_by_category\n    \n    using PARQUET\n    \n    \n    \n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\n    \n\tas\n\twith source_data as (\n    SELECT\n        category,\n        count(*) as total\n    FROM hudidb.order\n    GROUP BY category\n    order by category asc\n    )\n\nSELECT\n    *\nFROM source_data\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n  \\n    \\n    \\tcreate table hudidb.popular_by_category\\n    \\n    using PARQUET\\n    \\n    \\n    \\n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\\n    \\n\\tas\\n\\twith source_data as (\\n    SELECT\\n        category,\\n        count(*) as total\\n    FROM hudidb.order\\n    GROUP BY category\\n    order by category asc\\n    )\\n\\nSELECT\\n    *\\nFROM source_data\\n  ", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671734525967, 'CompletedOn': 1671734556238}, 'ResponseMetadata': {'RequestId': '8e850786-df88-4ac7-90ad-f4db30a80c5d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 18:42:36 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1547', 'connection': 'keep-alive', 'x-amzn-requestid': '8e850786-df88-4ac7-90ad-f4db30a80c5d'}, 'RetryAttempts': 0}}
[0m18:42:35.358181 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m18:42:35.358181 [debug] [Thread-1 (]: SQL status: OK in 31.22 seconds
[0m18:42:35.369187 [debug] [Thread-1 (]: finished collecting timing info
[0m18:42:35.369187 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: ROLLBACK
[0m18:42:35.369187 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m18:42:35.370187 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: Close
[0m18:42:35.370187 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m18:42:35.370187 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'beb8155a-db95-467b-bbaa-470bce822517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA2BB77C0>]}
[0m18:42:35.370187 [info ] [Thread-1 (]: 1 of 1 OK created sql table model hudidb.popular_by_category ................... [[32mOK[0m in 33.35s]
[0m18:42:35.372180 [debug] [Thread-1 (]: Finished running node model.dbtglue.popular_by_category
[0m18:42:35.373191 [debug] [MainThread]: Acquiring new glue connection "master"
[0m18:42:35.373191 [debug] [MainThread]: On master: ROLLBACK
[0m18:42:35.374190 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:42:35.374190 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m18:42:35.561550 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m18:42:35.561550 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m18:42:35.561550 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-2bce5fc8-7894-49e6-9b4f-26554918b66a
[0m18:42:37.151462 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m18:42:37.152486 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m18:42:37.153482 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m18:42:37.153482 [debug] [MainThread]: On master: ROLLBACK
[0m18:42:37.153482 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m18:42:37.153482 [debug] [MainThread]: On master: Close
[0m18:42:37.154480 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m18:42:37.154480 [info ] [MainThread]: 
[0m18:42:37.155472 [info ] [MainThread]: Finished running 1 table model in 0 hours 1 minutes and 38.43 seconds (98.43s).
[0m18:42:37.155472 [debug] [MainThread]: Glue adapter: cleanup called
[0m18:42:37.354287 [info ] [MainThread]: 
[0m18:42:37.354287 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:42:37.355288 [info ] [MainThread]: 
[0m18:42:37.356295 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m18:42:37.357296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA2B7B190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA2CDE980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FBA2CB7E80>]}
[0m18:42:37.357296 [debug] [MainThread]: Flushing usage events
[0m18:42:37.568610 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 18:43:12.592643 | d7e68369-6025-4693-ac78-def581406bf4 ==============================
[0m18:43:12.592643 [info ] [MainThread]: Running with dbt=1.3.1
[0m18:43:12.593646 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m18:43:12.593646 [debug] [MainThread]: Tracking: tracking
[0m18:43:12.615173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611DA64A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611DA65DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611DA67700>]}
[0m18:43:12.692165 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m18:43:12.692165 [debug] [MainThread]: Partial parsing: update schema file: dbtglue://models\source_tables.yml
[0m18:43:12.709224 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd7e68369-6025-4693-ac78-def581406bf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611DC855D0>]}
[0m18:43:12.714245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd7e68369-6025-4693-ac78-def581406bf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611DC1FD60>]}
[0m18:43:12.715168 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m18:43:12.715168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd7e68369-6025-4693-ac78-def581406bf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611DC1FDF0>]}
[0m18:43:12.716165 [info ] [MainThread]: 
[0m18:43:12.717165 [debug] [MainThread]: Acquiring new glue connection "master"
[0m18:43:12.718164 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m18:43:12.719164 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:43:12.719164 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m18:43:12.719164 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m18:43:12.719164 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m18:43:28.807202 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m18:43:28.807202 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-8c21f45f-8562-4852-9053-7391ca9c2307
[0m18:43:36.224338 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m18:43:36.717698 [debug] [ThreadPool]: On list_hudidb: Close
[0m18:43:36.717698 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m18:43:36.718700 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m18:43:36.719697 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:43:36.719697 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m18:43:37.158634 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m18:43:37.159641 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m18:43:37.159641 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-8c21f45f-8562-4852-9053-7391ca9c2307
[0m18:43:37.902007 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m18:43:38.142083 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m18:43:38.142083 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m18:43:38.143083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd7e68369-6025-4693-ac78-def581406bf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611DA7D240>]}
[0m18:43:38.143083 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m18:43:38.144084 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m18:43:38.144084 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:43:38.144084 [info ] [MainThread]: 
[0m18:43:38.150173 [debug] [Thread-1 (]: Began running node model.dbtglue.popular_by_category
[0m18:43:38.150173 [info ] [Thread-1 (]: 1 of 1 START sql table model hudidb.popular_by_category ........................ [RUN]
[0m18:43:38.151087 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.popular_by_category"
[0m18:43:38.152163 [debug] [Thread-1 (]: Began compiling node model.dbtglue.popular_by_category
[0m18:43:38.152163 [debug] [Thread-1 (]: Compiling model.dbtglue.popular_by_category
[0m18:43:38.154098 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.popular_by_category"
[0m18:43:38.155084 [debug] [Thread-1 (]: finished collecting timing info
[0m18:43:38.155084 [debug] [Thread-1 (]: Began executing node model.dbtglue.popular_by_category
[0m18:43:38.162094 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m18:43:38.162094 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m18:43:38.339964 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m18:43:38.339964 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m18:43:38.339964 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-8c21f45f-8562-4852-9053-7391ca9c2307
[0m18:43:39.994175 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:43:40.287509 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : popular_by_category
                             type : table
                        
[0m18:43:40.303515 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:43:40.873668 [debug] [Thread-1 (]: Glue adapter: table_name : popular_by_category
[0m18:43:40.873668 [debug] [Thread-1 (]: Glue adapter: table type : table
[0m18:43:40.874640 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m18:43:40.874640 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

        drop table if exists hudidb.popular_by_category
[0m18:43:40.874640 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:43:40.874640 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m18:43:40.874640 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m18:43:40.875607 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m18:43:40.875607 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

        drop table if exists hudidb.popular_by_category''')
[0m18:43:53.398971 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n        drop table if exists hudidb.popular_by_category\'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n        drop table if exists hudidb.popular_by_category", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671734622137, 'CompletedOn': 1671734633546}, 'ResponseMetadata': {'RequestId': '3a87462f-0e1a-4c52-87e5-783aba3fb23a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 18:43:54 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '853', 'connection': 'keep-alive', 'x-amzn-requestid': '3a87462f-0e1a-4c52-87e5-783aba3fb23a'}, 'RetryAttempts': 0}}
[0m18:43:53.399970 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m18:43:53.399970 [debug] [Thread-1 (]: Glue adapter: chunks: ['{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n        drop table if exists hudidb.popular_by_category", "schema": null, "results": null}', '{"type": "results", "rowcount": 0, "results": [], "description": []}']
[0m18:43:53.399970 [debug] [Thread-1 (]: Glue adapter: response: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n        drop table if exists hudidb.popular_by_category\'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n        drop table if exists hudidb.popular_by_category", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671734622137, 'CompletedOn': 1671734633546}, 'ResponseMetadata': {'RequestId': '3a87462f-0e1a-4c52-87e5-783aba3fb23a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 18:43:54 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '853', 'connection': 'keep-alive', 'x-amzn-requestid': '3a87462f-0e1a-4c52-87e5-783aba3fb23a'}, 'RetryAttempts': 0}}
[0m18:43:53.399970 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m18:43:53.400963 [debug] [Thread-1 (]: SQL status: OK in 12.53 seconds
[0m18:43:53.422970 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:43:53.425970 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.popular_by_category"
[0m18:43:53.426970 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m18:43:53.426970 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m18:43:53.427962 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  
[0m18:43:53.427962 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m18:43:53.427962 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m18:43:53.427962 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m18:43:53.427962 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m18:43:53.428962 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    using PARQUET
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  ''')
[0m18:44:02.673685 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n  \n    \n    \tcreate table hudidb.popular_by_category\n    \n    using PARQUET\n    \n    \n    \n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\n    \n\tas\n\twith source_data as (\n    SELECT\n        category,\n        count(*) as total\n    FROM hudidb.order\n    GROUP BY category\n    order by category asc\n    )\n\nSELECT\n    *\nFROM source_data\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n  \\n    \\n    \\tcreate table hudidb.popular_by_category\\n    \\n    using PARQUET\\n    \\n    \\n    \\n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\\n    \\n\\tas\\n\\twith source_data as (\\n    SELECT\\n        category,\\n        count(*) as total\\n    FROM hudidb.order\\n    GROUP BY category\\n    order by category asc\\n    )\\n\\nSELECT\\n    *\\nFROM source_data\\n  ", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671734634686, 'CompletedOn': 1671734642726}, 'ResponseMetadata': {'RequestId': 'cb8a7051-a5e6-4e70-8c2f-ee6007ca7733', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 18:44:03 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1547', 'connection': 'keep-alive', 'x-amzn-requestid': 'cb8a7051-a5e6-4e70-8c2f-ee6007ca7733'}, 'RetryAttempts': 0}}
[0m18:44:02.674683 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m18:44:02.675720 [debug] [Thread-1 (]: Glue adapter: chunks: ['{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n  \\n    \\n    \\tcreate table hudidb.popular_by_category\\n    \\n    using PARQUET\\n    \\n    \\n    \\n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\\n    \\n\\tas\\n\\twith source_data as (\\n    SELECT\\n        category,\\n        count(*) as total\\n    FROM hudidb.order\\n    GROUP BY category\\n    order by category asc\\n    )\\n\\nSELECT\\n    *\\nFROM source_data\\n  ", "schema": null, "results": null}', '{"type": "results", "rowcount": 0, "results": [], "description": []}']
[0m18:44:02.675720 [debug] [Thread-1 (]: Glue adapter: response: {'Statement': {'Id': 7, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n  \n    \n    \tcreate table hudidb.popular_by_category\n    \n    using PARQUET\n    \n    \n    \n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\n    \n\tas\n\twith source_data as (\n    SELECT\n        category,\n        count(*) as total\n    FROM hudidb.order\n    GROUP BY category\n    order by category asc\n    )\n\nSELECT\n    *\nFROM source_data\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n  \\n    \\n    \\tcreate table hudidb.popular_by_category\\n    \\n    using PARQUET\\n    \\n    \\n    \\n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\\n    \\n\\tas\\n\\twith source_data as (\\n    SELECT\\n        category,\\n        count(*) as total\\n    FROM hudidb.order\\n    GROUP BY category\\n    order by category asc\\n    )\\n\\nSELECT\\n    *\\nFROM source_data\\n  ", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671734634686, 'CompletedOn': 1671734642726}, 'ResponseMetadata': {'RequestId': 'cb8a7051-a5e6-4e70-8c2f-ee6007ca7733', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 18:44:03 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1547', 'connection': 'keep-alive', 'x-amzn-requestid': 'cb8a7051-a5e6-4e70-8c2f-ee6007ca7733'}, 'RetryAttempts': 0}}
[0m18:44:02.676685 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m18:44:02.676685 [debug] [Thread-1 (]: SQL status: OK in 9.25 seconds
[0m18:44:02.689662 [debug] [Thread-1 (]: finished collecting timing info
[0m18:44:02.689662 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: ROLLBACK
[0m18:44:02.689662 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m18:44:02.690706 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: Close
[0m18:44:02.690706 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m18:44:02.690706 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd7e68369-6025-4693-ac78-def581406bf4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611E6A3B80>]}
[0m18:44:02.691679 [info ] [Thread-1 (]: 1 of 1 OK created sql table model hudidb.popular_by_category ................... [[32mOK[0m in 24.54s]
[0m18:44:02.692804 [debug] [Thread-1 (]: Finished running node model.dbtglue.popular_by_category
[0m18:44:02.693826 [debug] [MainThread]: Acquiring new glue connection "master"
[0m18:44:02.693826 [debug] [MainThread]: On master: ROLLBACK
[0m18:44:02.693826 [debug] [MainThread]: Opening a new connection, currently in state init
[0m18:44:02.694868 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m18:44:02.869830 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m18:44:02.870838 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m18:44:02.870838 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-8c21f45f-8562-4852-9053-7391ca9c2307
[0m18:44:04.475988 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m18:44:04.476989 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m18:44:04.476989 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m18:44:04.476989 [debug] [MainThread]: On master: ROLLBACK
[0m18:44:04.476989 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m18:44:04.476989 [debug] [MainThread]: On master: Close
[0m18:44:04.476989 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m18:44:04.477991 [info ] [MainThread]: 
[0m18:44:04.478986 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 51.76 seconds (51.76s).
[0m18:44:04.478986 [debug] [MainThread]: Glue adapter: cleanup called
[0m18:44:04.670204 [info ] [MainThread]: 
[0m18:44:04.671133 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:44:04.672123 [info ] [MainThread]: 
[0m18:44:04.673122 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m18:44:04.673122 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611E7418D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611E63B070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002611E648520>]}
[0m18:44:04.674132 [debug] [MainThread]: Flushing usage events
[0m18:44:04.905050 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 19:21:47.717472 | c9530c43-bc39-4b12-b2d5-fe7df1dee850 ==============================
[0m19:21:47.717472 [info ] [MainThread]: Running with dbt=1.3.1
[0m19:21:47.718477 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:21:47.719465 [debug] [MainThread]: Tracking: tracking
[0m19:21:47.739465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AD7955D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AD795C00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AD794910>]}
[0m19:21:47.764472 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m19:21:47.766008 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'c9530c43-bc39-4b12-b2d5-fe7df1dee850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AD219D80>]}
[0m19:21:47.817020 [debug] [MainThread]: Parsing macros\adapters.sql
[0m19:21:47.839053 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m19:21:47.841075 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m19:21:47.842114 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m19:21:47.845114 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m19:21:47.856112 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m19:21:47.856112 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m19:21:47.863111 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m19:21:47.865110 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m19:21:47.868121 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m19:21:47.869123 [debug] [MainThread]: Parsing macros\adapters.sql
[0m19:21:47.897112 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m19:21:47.899118 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m19:21:47.906119 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m19:21:47.922120 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m19:21:47.927050 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m19:21:47.928050 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m19:21:47.930049 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m19:21:47.935057 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m19:21:47.941058 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m19:21:47.944057 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m19:21:47.945057 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m19:21:47.945057 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m19:21:47.946057 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m19:21:47.946057 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m19:21:47.947057 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m19:21:47.948057 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m19:21:47.948057 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m19:21:47.952057 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m19:21:47.960057 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m19:21:47.962057 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m19:21:47.963057 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m19:21:47.963057 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m19:21:47.973058 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m19:21:47.979057 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m19:21:47.981057 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m19:21:47.982060 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m19:21:47.987057 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m19:21:47.990057 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m19:21:48.000057 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m19:21:48.001057 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m19:21:48.004061 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m19:21:48.009057 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m19:21:48.013057 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m19:21:48.014057 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m19:21:48.015057 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m19:21:48.015057 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m19:21:48.016057 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m19:21:48.017057 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m19:21:48.018057 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m19:21:48.020059 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m19:21:48.022057 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m19:21:48.025057 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m19:21:48.030057 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m19:21:48.037058 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m19:21:48.038057 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m19:21:48.048057 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m19:21:48.059058 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m19:21:48.064057 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m19:21:48.066057 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m19:21:48.070288 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m19:21:48.072287 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m19:21:48.074425 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m19:21:48.074936 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m19:21:48.078954 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m19:21:48.090954 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m19:21:48.095957 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m19:21:48.104947 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m19:21:48.112953 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m19:21:48.113955 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m19:21:48.124946 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m19:21:48.126947 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m19:21:48.129946 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m19:21:48.130947 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m19:21:48.134946 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m19:21:48.135947 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m19:21:48.136947 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m19:21:48.136947 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m19:21:48.138948 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m19:21:48.138948 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m19:21:48.140947 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m19:21:48.140947 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m19:21:48.145946 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m19:21:48.147948 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m19:21:48.148948 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m19:21:48.149947 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m19:21:48.150948 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m19:21:48.151953 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m19:21:48.152948 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m19:21:48.152948 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m19:21:48.154948 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m19:21:48.155948 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m19:21:48.157947 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m19:21:48.158947 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m19:21:48.159948 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m19:21:48.160947 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m19:21:48.161947 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m19:21:48.162948 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m19:21:48.163948 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m19:21:48.480776 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m19:21:48.517783 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'gold_category_invoice_count' in the 'models' section of file 'models\metrics\schema.yml'
[0m19:21:48.550783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c9530c43-bc39-4b12-b2d5-fe7df1dee850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AD904160>]}
[0m19:21:48.556783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c9530c43-bc39-4b12-b2d5-fe7df1dee850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AD932AA0>]}
[0m19:21:48.556783 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m19:21:48.557785 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c9530c43-bc39-4b12-b2d5-fe7df1dee850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AD810B20>]}
[0m19:21:48.558784 [info ] [MainThread]: 
[0m19:21:48.558784 [debug] [MainThread]: Acquiring new glue connection "master"
[0m19:21:48.560776 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m19:21:48.560776 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:21:48.560776 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m19:21:48.561776 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m19:21:48.561776 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m19:22:22.052833 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m19:22:22.052833 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-1d1da024-6d91-49e6-b139-cef0c2e01ef8
[0m19:22:33.340717 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m19:22:33.572041 [debug] [ThreadPool]: On list_hudidb: Close
[0m19:22:33.572041 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m19:22:33.574029 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m19:22:33.574029 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:22:33.574029 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m19:22:33.740012 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m19:22:33.740012 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m19:22:33.741013 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-1d1da024-6d91-49e6-b139-cef0c2e01ef8
[0m19:22:35.346518 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m19:22:35.581300 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m19:22:35.582289 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m19:22:35.583297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c9530c43-bc39-4b12-b2d5-fe7df1dee850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AD990850>]}
[0m19:22:35.583297 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m19:22:35.583297 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m19:22:35.584297 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m19:22:35.584297 [info ] [MainThread]: 
[0m19:22:35.589292 [debug] [Thread-1 (]: Began running node model.dbtglue.popular_by_category
[0m19:22:35.590293 [info ] [Thread-1 (]: 1 of 1 START sql table model hudidb.popular_by_category ........................ [RUN]
[0m19:22:35.591292 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.popular_by_category"
[0m19:22:35.591292 [debug] [Thread-1 (]: Began compiling node model.dbtglue.popular_by_category
[0m19:22:35.591292 [debug] [Thread-1 (]: Compiling model.dbtglue.popular_by_category
[0m19:22:35.594296 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.popular_by_category"
[0m19:22:35.595296 [debug] [Thread-1 (]: finished collecting timing info
[0m19:22:35.595296 [debug] [Thread-1 (]: Began executing node model.dbtglue.popular_by_category
[0m19:22:35.606289 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m19:22:35.606289 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m19:22:35.773382 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m19:22:35.773382 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m19:22:35.773382 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-1d1da024-6d91-49e6-b139-cef0c2e01ef8
[0m19:22:37.379462 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m19:22:37.678106 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : popular_by_category
                             type : table
                        
[0m19:22:37.692119 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m19:22:37.979236 [debug] [Thread-1 (]: Glue adapter: table_name : popular_by_category
[0m19:22:37.979236 [debug] [Thread-1 (]: Glue adapter: table type : table
[0m19:22:37.980230 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m19:22:37.980230 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

        drop table if exists hudidb.popular_by_category
[0m19:22:37.981229 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m19:22:37.981229 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m19:22:37.981229 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m19:22:37.981229 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m19:22:37.981229 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

        drop table if exists hudidb.popular_by_category''')
[0m19:22:48.608508 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n        drop table if exists hudidb.popular_by_category\'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n        drop table if exists hudidb.popular_by_category", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671736959260, 'CompletedOn': 1671736969289}, 'ResponseMetadata': {'RequestId': '07622128-10d3-47c0-923c-1e5fa68c073c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 19:22:49 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '853', 'connection': 'keep-alive', 'x-amzn-requestid': '07622128-10d3-47c0-923c-1e5fa68c073c'}, 'RetryAttempts': 0}}
[0m19:22:48.609507 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m19:22:48.609507 [debug] [Thread-1 (]: Glue adapter: chunks: ['{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n        drop table if exists hudidb.popular_by_category", "schema": null, "results": null}', '{"type": "results", "rowcount": 0, "results": [], "description": []}']
[0m19:22:48.609507 [debug] [Thread-1 (]: Glue adapter: response: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n        drop table if exists hudidb.popular_by_category\'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.popular_by_category\\"} */\\n\\n        drop table if exists hudidb.popular_by_category", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671736959260, 'CompletedOn': 1671736969289}, 'ResponseMetadata': {'RequestId': '07622128-10d3-47c0-923c-1e5fa68c073c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 19:22:49 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '853', 'connection': 'keep-alive', 'x-amzn-requestid': '07622128-10d3-47c0-923c-1e5fa68c073c'}, 'RetryAttempts': 0}}
[0m19:22:48.609507 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m19:22:48.610507 [debug] [Thread-1 (]: SQL status: OK in 10.63 seconds
[0m19:22:48.632508 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m19:22:48.635514 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.popular_by_category"
[0m19:22:48.636501 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m19:22:48.636501 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m19:22:48.636501 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    
    using hudi
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  
[0m19:22:48.637507 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m19:22:48.637507 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m19:22:48.637507 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m19:22:48.637507 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m19:22:48.638512 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    
    using hudi
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  ''')
[0m19:22:50.586439 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n  \n    \n    \tcreate table hudidb.popular_by_category\n    \n    \n    using hudi\n    \n    \n    \n    LOCATION \'s3://glue-learn-begineers/views//hudidb/popular_by_category/\'\n    \n\tas\n\twith source_data as (\n    SELECT\n        category,\n        count(*) as total\n    FROM hudidb.order\n    GROUP BY category\n    order by category asc\n    )\n\nSELECT\n    *\nFROM source_data\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 7, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': 'An error occurred while calling o70.sql.\n: java.lang.ClassNotFoundException: Failed to find data source: hudi. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:689)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:743)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:755)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:290)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:39)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)\n\tat scala.collection.immutable.List.foldLeft(List.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:388)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: hudi.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:663)\n\tat scala.util.Try$.apply(Try.scala:209)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:663)\n\tat scala.util.Failure.orElse(Try.scala:220)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:663)\n\t... 55 more\n', 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', 'py4j.protocol.Py4JJavaError: An error occurred while calling o70.sql.\n: java.lang.ClassNotFoundException: Failed to find data source: hudi. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:689)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:743)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:755)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:290)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:39)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)\n\tat scala.collection.immutable.List.foldLeft(List.scala:85)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)\n\tat scala.collection.immutable.List.foreach(List.scala:388)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: hudi.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:663)\n\tat scala.util.Try$.apply(Try.scala:209)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:663)\n\tat scala.util.Failure.orElse(Try.scala:220)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:663)\n\t... 55 more\n\n']}, 'Progress': 1.0, 'StartedOn': 1671736969919, 'CompletedOn': 1671736970654}, 'ResponseMetadata': {'RequestId': '86a752f8-a062-45e7-8ac5-04e5aba1b63e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 19:22:51 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '13608', 'connection': 'keep-alive', 'x-amzn-requestid': '86a752f8-a062-45e7-8ac5-04e5aba1b63e'}, 'RetryAttempts': 0}}
[0m19:22:50.586439 [debug] [Thread-1 (]: Glue adapter: status = error
[0m19:22:50.586439 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    
    using hudi
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  '''), Py4JJavaError: An error occurred while calling o70.sql.
: java.lang.ClassNotFoundException: Failed to find data source: hudi. Please find packages at http://spark.apache.org/third-party-projects.html
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:689)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:743)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:755)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:290)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:48)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:39)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
	at scala.collection.immutable.List.foldLeft(List.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: hudi.DefaultSource
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:663)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:663)
	at scala.util.Failure.orElse(Try.scala:220)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:663)
	... 55 more

[0m19:22:50.593035 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    
    using hudi
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  '''), Py4JJavaError: An error occurred while calling o70.sql.
: java.lang.ClassNotFoundException: Failed to find data source: hudi. Please find packages at http://spark.apache.org/third-party-projects.html
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:689)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:743)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:755)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:290)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:48)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)
	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:39)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
	at scala.collection.immutable.List.foldLeft(List.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:388)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: hudi.DefaultSource
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:663)
	at scala.util.Try$.apply(Try.scala:209)
	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:663)
	at scala.util.Failure.orElse(Try.scala:220)
	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:663)
	... 55 more

[0m19:22:50.593035 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

  
    
    	create table hudidb.popular_by_category
    
    
    using hudi
    
    
    
    LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
    
	as
	with source_data as (
    SELECT
        category,
        count(*) as total
    FROM hudidb.order
    GROUP BY category
    order by category asc
    )

SELECT
    *
FROM source_data
  
[0m19:22:50.593035 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: ROLLBACK
[0m19:22:50.594038 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m19:22:50.594038 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: Close
[0m19:22:50.594038 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m19:22:50.594038 [debug] [Thread-1 (]: finished collecting timing info
[0m19:22:50.595036 [debug] [Thread-1 (]: Database Error in model popular_by_category (models\metrics\popular_by_category.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
  
    
      
      	create table hudidb.popular_by_category
      
      
      using hudi
      
      
      
      LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
      
  	as
  	with source_data as (
      SELECT
          category,
          count(*) as total
      FROM hudidb.order
      GROUP BY category
      order by category asc
      )
  
  SELECT
      *
  FROM source_data
    '''), Py4JJavaError: An error occurred while calling o70.sql.
  : java.lang.ClassNotFoundException: Failed to find data source: hudi. Please find packages at http://spark.apache.org/third-party-projects.html
  	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:689)
  	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:743)
  	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:755)
  	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:290)
  	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:48)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)
  	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:39)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
  	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
  	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
  	at scala.collection.immutable.List.foldLeft(List.scala:85)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
  	at scala.collection.immutable.List.foreach(List.scala:388)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
  	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
  	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
  	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
  	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
  	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  Caused by: java.lang.ClassNotFoundException: hudi.DefaultSource
  	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
  	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
  	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:663)
  	at scala.util.Try$.apply(Try.scala:209)
  	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:663)
  	at scala.util.Failure.orElse(Try.scala:220)
  	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:663)
  	... 55 more
  
  compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m19:22:50.595036 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c9530c43-bc39-4b12-b2d5-fe7df1dee850', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AE52D360>]}
[0m19:22:50.595036 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model hudidb.popular_by_category ............... [[31mERROR[0m in 15.00s]
[0m19:22:50.596036 [debug] [Thread-1 (]: Finished running node model.dbtglue.popular_by_category
[0m19:22:50.598036 [debug] [MainThread]: Acquiring new glue connection "master"
[0m19:22:50.598036 [debug] [MainThread]: On master: ROLLBACK
[0m19:22:50.598036 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:22:50.598036 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m19:22:50.825578 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m19:22:50.826572 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m19:22:50.827574 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-1d1da024-6d91-49e6-b139-cef0c2e01ef8
[0m19:22:52.129453 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m19:22:52.130453 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m19:22:52.130453 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m19:22:52.130453 [debug] [MainThread]: On master: ROLLBACK
[0m19:22:52.130453 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m19:22:52.131453 [debug] [MainThread]: On master: Close
[0m19:22:52.131453 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m19:22:52.131453 [info ] [MainThread]: 
[0m19:22:52.132453 [info ] [MainThread]: Finished running 1 table model in 0 hours 1 minutes and 3.57 seconds (63.57s).
[0m19:22:52.132453 [debug] [MainThread]: Glue adapter: cleanup called
[0m19:22:52.319651 [info ] [MainThread]: 
[0m19:22:52.320650 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m19:22:52.321873 [info ] [MainThread]: 
[0m19:22:52.322654 [error] [MainThread]: [33mDatabase Error in model popular_by_category (models\metrics\popular_by_category.sql)[0m
[0m19:22:52.323651 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
[0m19:22:52.323651 [error] [MainThread]:   
[0m19:22:52.323651 [error] [MainThread]:     
[0m19:22:52.324696 [error] [MainThread]:       
[0m19:22:52.324696 [error] [MainThread]:       	create table hudidb.popular_by_category
[0m19:22:52.325682 [error] [MainThread]:       
[0m19:22:52.325682 [error] [MainThread]:       
[0m19:22:52.326649 [error] [MainThread]:       using hudi
[0m19:22:52.326649 [error] [MainThread]:       
[0m19:22:52.326649 [error] [MainThread]:       
[0m19:22:52.327649 [error] [MainThread]:       
[0m19:22:52.327649 [error] [MainThread]:       LOCATION 's3://glue-learn-begineers/views//hudidb/popular_by_category/'
[0m19:22:52.327649 [error] [MainThread]:       
[0m19:22:52.328649 [error] [MainThread]:   	as
[0m19:22:52.328649 [error] [MainThread]:   	with source_data as (
[0m19:22:52.328649 [error] [MainThread]:       SELECT
[0m19:22:52.329650 [error] [MainThread]:           category,
[0m19:22:52.329650 [error] [MainThread]:           count(*) as total
[0m19:22:52.329650 [error] [MainThread]:       FROM hudidb.order
[0m19:22:52.330650 [error] [MainThread]:       GROUP BY category
[0m19:22:52.330650 [error] [MainThread]:       order by category asc
[0m19:22:52.330650 [error] [MainThread]:       )
[0m19:22:52.331650 [error] [MainThread]:   
[0m19:22:52.331650 [error] [MainThread]:   SELECT
[0m19:22:52.331650 [error] [MainThread]:       *
[0m19:22:52.332650 [error] [MainThread]:   FROM source_data
[0m19:22:52.332650 [error] [MainThread]:     '''), Py4JJavaError: An error occurred while calling o70.sql.
[0m19:22:52.332650 [error] [MainThread]:   : java.lang.ClassNotFoundException: Failed to find data source: hudi. Please find packages at http://spark.apache.org/third-party-projects.html
[0m19:22:52.333649 [error] [MainThread]:   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:689)
[0m19:22:52.333649 [error] [MainThread]:   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:743)
[0m19:22:52.333649 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.org$apache$spark$sql$catalyst$analysis$ResolveSessionCatalog$$isV2Provider(ResolveSessionCatalog.scala:755)
[0m19:22:52.334650 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:290)
[0m19:22:52.334650 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog$$anonfun$apply$1.applyOrElse(ResolveSessionCatalog.scala:48)
[0m19:22:52.334650 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
[0m19:22:52.335650 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
[0m19:22:52.335650 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
[0m19:22:52.336649 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
[0m19:22:52.336649 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
[0m19:22:52.336649 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
[0m19:22:52.337650 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
[0m19:22:52.338405 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:48)
[0m19:22:52.338405 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.ResolveSessionCatalog.apply(ResolveSessionCatalog.scala:39)
[0m19:22:52.338405 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
[0m19:22:52.339484 [error] [MainThread]:   	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122)
[0m19:22:52.339484 [error] [MainThread]:   	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118)
[0m19:22:52.339484 [error] [MainThread]:   	at scala.collection.immutable.List.foldLeft(List.scala:85)
[0m19:22:52.340419 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
[0m19:22:52.340419 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
[0m19:22:52.340419 [error] [MainThread]:   	at scala.collection.immutable.List.foreach(List.scala:388)
[0m19:22:52.341419 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
[0m19:22:52.341419 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
[0m19:22:52.341419 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
[0m19:22:52.342430 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
[0m19:22:52.342430 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
[0m19:22:52.343417 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
[0m19:22:52.343417 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
[0m19:22:52.344429 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
[0m19:22:52.344429 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
[0m19:22:52.345418 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
[0m19:22:52.345418 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
[0m19:22:52.346425 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:192)
[0m19:22:52.346425 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:163)
[0m19:22:52.347425 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m19:22:52.347425 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:163)
[0m19:22:52.347425 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
[0m19:22:52.348426 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
[0m19:22:52.348426 [error] [MainThread]:   	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
[0m19:22:52.348426 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)
[0m19:22:52.349418 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m19:22:52.349418 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
[0m19:22:52.350430 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
[0m19:22:52.350430 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m19:22:52.351426 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
[0m19:22:52.351426 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[0m19:22:52.352425 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[0m19:22:52.352425 [error] [MainThread]:   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[0m19:22:52.353418 [error] [MainThread]:   	at java.lang.reflect.Method.invoke(Method.java:498)
[0m19:22:52.354418 [error] [MainThread]:   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[0m19:22:52.354418 [error] [MainThread]:   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[0m19:22:52.354418 [error] [MainThread]:   	at py4j.Gateway.invoke(Gateway.java:282)
[0m19:22:52.355417 [error] [MainThread]:   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[0m19:22:52.355417 [error] [MainThread]:   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[0m19:22:52.355417 [error] [MainThread]:   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[0m19:22:52.356420 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:750)
[0m19:22:52.356420 [error] [MainThread]:   Caused by: java.lang.ClassNotFoundException: hudi.DefaultSource
[0m19:22:52.356420 [error] [MainThread]:   	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
[0m19:22:52.357417 [error] [MainThread]:   	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
[0m19:22:52.357417 [error] [MainThread]:   	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
[0m19:22:52.357417 [error] [MainThread]:   	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
[0m19:22:52.358417 [error] [MainThread]:   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:663)
[0m19:22:52.358417 [error] [MainThread]:   	at scala.util.Try$.apply(Try.scala:209)
[0m19:22:52.358417 [error] [MainThread]:   	at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:663)
[0m19:22:52.358417 [error] [MainThread]:   	at scala.util.Failure.orElse(Try.scala:220)
[0m19:22:52.359417 [error] [MainThread]:   	at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:663)
[0m19:22:52.359417 [error] [MainThread]:   	... 55 more
[0m19:22:52.360434 [error] [MainThread]:   
[0m19:22:52.360434 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m19:22:52.361428 [info ] [MainThread]: 
[0m19:22:52.361428 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m19:22:52.361428 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AE5208B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AE520A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000153AD970A90>]}
[0m19:22:52.362417 [debug] [MainThread]: Flushing usage events
[0m19:22:52.582534 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 19:33:41.701834 | fc6a2fbc-d65b-4e55-be0d-12dd9ac7db19 ==============================
[0m19:33:41.701834 [info ] [MainThread]: Running with dbt=1.3.1
[0m19:33:41.702834 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:33:41.702834 [debug] [MainThread]: Tracking: tracking
[0m19:33:41.724834 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF11FB5630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF11FB5900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF11FB7670>]}
[0m19:33:41.750841 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m19:33:41.750841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'fc6a2fbc-d65b-4e55-be0d-12dd9ac7db19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF11A39900>]}
[0m19:33:41.797912 [debug] [MainThread]: Parsing macros\adapters.sql
[0m19:33:41.816910 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m19:33:41.817899 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m19:33:41.818838 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m19:33:41.821895 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m19:33:41.833901 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m19:33:41.833901 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m19:33:41.840841 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m19:33:41.843841 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m19:33:41.846841 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m19:33:41.847841 [debug] [MainThread]: Parsing macros\adapters.sql
[0m19:33:41.878840 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m19:33:41.880841 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m19:33:41.887841 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m19:33:41.906841 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m19:33:41.910841 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m19:33:41.911844 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m19:33:41.913840 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m19:33:41.918833 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m19:33:41.924833 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m19:33:41.928833 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m19:33:41.928833 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m19:33:41.929835 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m19:33:41.929835 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m19:33:41.930834 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m19:33:41.931835 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m19:33:41.931835 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m19:33:41.932834 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m19:33:41.936833 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m19:33:41.944833 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m19:33:41.946834 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m19:33:41.947835 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m19:33:41.947835 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m19:33:41.957833 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m19:33:41.965909 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m19:33:41.966896 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m19:33:41.968836 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m19:33:41.973906 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m19:33:41.975897 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m19:33:41.984950 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m19:33:41.986909 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m19:33:41.988895 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m19:33:41.993902 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m19:33:41.996840 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m19:33:41.997840 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m19:33:41.998840 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m19:33:41.999840 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m19:33:41.999840 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m19:33:42.000840 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m19:33:42.001835 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m19:33:42.003840 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m19:33:42.005840 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m19:33:42.007840 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m19:33:42.013842 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m19:33:42.019840 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m19:33:42.020841 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m19:33:42.030834 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m19:33:42.042841 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m19:33:42.047841 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m19:33:42.049841 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m19:33:42.053834 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m19:33:42.056841 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m19:33:42.058842 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m19:33:42.059842 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m19:33:42.062841 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m19:33:42.075841 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m19:33:42.080841 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m19:33:42.089842 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m19:33:42.097841 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m19:33:42.098841 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m19:33:42.108841 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m19:33:42.110841 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m19:33:42.113841 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m19:33:42.114841 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m19:33:42.118835 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m19:33:42.119841 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m19:33:42.120844 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m19:33:42.121842 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m19:33:42.122841 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m19:33:42.123841 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m19:33:42.124841 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m19:33:42.124841 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m19:33:42.128841 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m19:33:42.129841 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m19:33:42.130841 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m19:33:42.131841 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m19:33:42.132841 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m19:33:42.132841 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m19:33:42.133841 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m19:33:42.134841 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m19:33:42.135835 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m19:33:42.136857 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m19:33:42.139840 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m19:33:42.140841 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m19:33:42.141835 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m19:33:42.142834 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m19:33:42.142834 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m19:33:42.143834 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m19:33:42.145834 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m19:33:42.436859 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m19:33:42.470858 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'gold_category_invoice_count' in the 'models' section of file 'models\metrics\schema.yml'
[0m19:33:42.500861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fc6a2fbc-d65b-4e55-be0d-12dd9ac7db19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF119EBEE0>]}
[0m19:33:42.507855 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fc6a2fbc-d65b-4e55-be0d-12dd9ac7db19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF119EBF70>]}
[0m19:33:42.507855 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m19:33:42.508853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc6a2fbc-d65b-4e55-be0d-12dd9ac7db19', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF11FB5900>]}
[0m19:33:42.509854 [info ] [MainThread]: 
[0m19:33:42.510856 [debug] [MainThread]: Acquiring new glue connection "master"
[0m19:33:42.511854 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m19:33:42.512892 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:33:42.512892 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m19:33:42.512892 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m19:33:42.513915 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m19:33:43.151740 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : An error occurred (InvalidInputException) when calling the CreateSession operation: DataCatalog Connection issue : User: arn:aws:sts::043916019468:assumed-role/GlueInteractiveSessionRole/GlueJobRunnerSession is not authorized to perform: glue:GetConnection on resource: arn:aws:glue:us-east-1:043916019468:catalog because no identity-based policy allows the glue:GetConnection action (Service: AWSGlue; Status Code: 400; Error Code: AccessDeniedException; Request ID: a3c7044a-1786-48c4-8de5-3c0ad6f6baba; Proxy: null)
 (Service: AWSGlueJobExecutor; Status Code: 400; Error Code: InvalidInputException; Request ID: 167e287b-e709-4c9a-b706-04eb9f8203b1; Proxy: null)
[0m19:33:43.152741 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : Database Error
  An error occurred (InvalidInputException) when calling the CreateSession operation: DataCatalog Connection issue : User: arn:aws:sts::043916019468:assumed-role/GlueInteractiveSessionRole/GlueJobRunnerSession is not authorized to perform: glue:GetConnection on resource: arn:aws:glue:us-east-1:043916019468:catalog because no identity-based policy allows the glue:GetConnection action (Service: AWSGlue; Status Code: 400; Error Code: AccessDeniedException; Request ID: a3c7044a-1786-48c4-8de5-3c0ad6f6baba; Proxy: null)
   (Service: AWSGlueJobExecutor; Status Code: 400; Error Code: InvalidInputException; Request ID: 167e287b-e709-4c9a-b706-04eb9f8203b1; Proxy: null)
[0m19:33:43.153741 [debug] [ThreadPool]: On list_hudidb: No close available on handle
[0m19:33:43.154740 [debug] [MainThread]: Glue adapter: cleanup called
[0m19:33:43.154740 [error] [MainThread]: Encountered an error:
Database Error
  Got an error when attempting to open a GlueSessions: Database Error
    An error occurred (InvalidInputException) when calling the CreateSession operation: DataCatalog Connection issue : User: arn:aws:sts::043916019468:assumed-role/GlueInteractiveSessionRole/GlueJobRunnerSession is not authorized to perform: glue:GetConnection on resource: arn:aws:glue:us-east-1:043916019468:catalog because no identity-based policy allows the glue:GetConnection action (Service: AWSGlue; Status Code: 400; Error Code: AccessDeniedException; Request ID: a3c7044a-1786-48c4-8de5-3c0ad6f6baba; Proxy: null)
     (Service: AWSGlueJobExecutor; Status Code: 400; Error Code: InvalidInputException; Request ID: 167e287b-e709-4c9a-b706-04eb9f8203b1; Proxy: null)
[0m19:33:43.155741 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF11FC0610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF11FC3100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BF11FC3B50>]}
[0m19:33:43.156746 [debug] [MainThread]: Flushing usage events
[0m19:33:43.382191 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 19:37:32.668142 | da18ba29-4378-4f2b-a874-a46d6ccbdad9 ==============================
[0m19:37:32.668142 [info ] [MainThread]: Running with dbt=1.3.1
[0m19:37:32.669144 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:37:32.669144 [debug] [MainThread]: Tracking: tracking
[0m19:37:32.695078 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D07C055D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D07C05C00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D07C04910>]}
[0m19:37:32.765085 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m19:37:32.766085 [debug] [MainThread]: Partial parsing: update schema file: dbtglue://models\source_tables.yml
[0m19:37:32.782081 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'da18ba29-4378-4f2b-a874-a46d6ccbdad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D07E25570>]}
[0m19:37:32.789080 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'da18ba29-4378-4f2b-a874-a46d6ccbdad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D07DBFD00>]}
[0m19:37:32.790079 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m19:37:32.791079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'da18ba29-4378-4f2b-a874-a46d6ccbdad9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D07DBFD90>]}
[0m19:37:32.791079 [info ] [MainThread]: 
[0m19:37:32.792078 [debug] [MainThread]: Acquiring new glue connection "master"
[0m19:37:32.794077 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m19:37:32.794077 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:37:32.794077 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m19:37:32.794077 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m19:37:32.794077 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m19:37:33.580304 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : An error occurred (InvalidInputException) when calling the CreateSession operation: DataCatalog Connection issue : User: arn:aws:sts::043916019468:assumed-role/GlueInteractiveSessionRole/GlueJobRunnerSession is not authorized to perform: glue:GetConnection on resource: arn:aws:glue:us-east-1:043916019468:catalog because no identity-based policy allows the glue:GetConnection action (Service: AWSGlue; Status Code: 400; Error Code: AccessDeniedException; Request ID: 905532ac-03bd-4651-8a03-f0bd1426fe57; Proxy: null)
 (Service: AWSGlueJobExecutor; Status Code: 400; Error Code: InvalidInputException; Request ID: b4c1259f-43fa-4626-9c93-4439a3e6a89e; Proxy: null)
[0m19:37:33.581309 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : Database Error
  An error occurred (InvalidInputException) when calling the CreateSession operation: DataCatalog Connection issue : User: arn:aws:sts::043916019468:assumed-role/GlueInteractiveSessionRole/GlueJobRunnerSession is not authorized to perform: glue:GetConnection on resource: arn:aws:glue:us-east-1:043916019468:catalog because no identity-based policy allows the glue:GetConnection action (Service: AWSGlue; Status Code: 400; Error Code: AccessDeniedException; Request ID: 905532ac-03bd-4651-8a03-f0bd1426fe57; Proxy: null)
   (Service: AWSGlueJobExecutor; Status Code: 400; Error Code: InvalidInputException; Request ID: b4c1259f-43fa-4626-9c93-4439a3e6a89e; Proxy: null)
[0m19:37:33.582305 [debug] [ThreadPool]: On list_hudidb: No close available on handle
[0m19:37:33.583297 [debug] [MainThread]: Glue adapter: cleanup called
[0m19:37:33.583297 [error] [MainThread]: Encountered an error:
Database Error
  Got an error when attempting to open a GlueSessions: Database Error
    An error occurred (InvalidInputException) when calling the CreateSession operation: DataCatalog Connection issue : User: arn:aws:sts::043916019468:assumed-role/GlueInteractiveSessionRole/GlueJobRunnerSession is not authorized to perform: glue:GetConnection on resource: arn:aws:glue:us-east-1:043916019468:catalog because no identity-based policy allows the glue:GetConnection action (Service: AWSGlue; Status Code: 400; Error Code: AccessDeniedException; Request ID: 905532ac-03bd-4651-8a03-f0bd1426fe57; Proxy: null)
     (Service: AWSGlueJobExecutor; Status Code: 400; Error Code: InvalidInputException; Request ID: b4c1259f-43fa-4626-9c93-4439a3e6a89e; Proxy: null)
[0m19:37:33.584330 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D0835C4C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D0835CE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D0835D090>]}
[0m19:37:33.584330 [debug] [MainThread]: Flushing usage events
[0m19:37:33.828770 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 19:43:56.317702 | f6610cd5-9901-47e4-afaa-35f203407401 ==============================
[0m19:43:56.317702 [info ] [MainThread]: Running with dbt=1.3.1
[0m19:43:56.318713 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:43:56.319701 [debug] [MainThread]: Tracking: tracking
[0m19:43:56.340740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A281D54A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A281D55DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A281D57700>]}
[0m19:43:56.413708 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:43:56.413708 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:43:56.418708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f6610cd5-9901-47e4-afaa-35f203407401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A2FE875EA0>]}
[0m19:43:56.424708 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f6610cd5-9901-47e4-afaa-35f203407401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A2FE841B40>]}
[0m19:43:56.424708 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m19:43:56.425701 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6610cd5-9901-47e4-afaa-35f203407401', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A2FE841D80>]}
[0m19:43:56.426702 [info ] [MainThread]: 
[0m19:43:56.426702 [debug] [MainThread]: Acquiring new glue connection "master"
[0m19:43:56.428701 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m19:43:56.428701 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:43:56.428701 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m19:43:56.428701 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m19:43:56.428701 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m19:43:57.154111 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : An error occurred (InvalidInputException) when calling the CreateSession operation: DataCatalog Connection issue : User: arn:aws:sts::043916019468:assumed-role/GlueInteractiveSessionRole/GlueJobRunnerSession is not authorized to perform: glue:GetConnection on resource: arn:aws:glue:us-east-1:043916019468:connection/hudi-connection because no identity-based policy allows the glue:GetConnection action (Service: AWSGlue; Status Code: 400; Error Code: AccessDeniedException; Request ID: 076c293c-84dd-4fea-8b6b-6ba8ef03593f; Proxy: null)
 (Service: AWSGlueJobExecutor; Status Code: 400; Error Code: InvalidInputException; Request ID: 3e0ff920-d6fb-4373-a22d-377974405457; Proxy: null)
[0m19:43:57.155014 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : Database Error
  An error occurred (InvalidInputException) when calling the CreateSession operation: DataCatalog Connection issue : User: arn:aws:sts::043916019468:assumed-role/GlueInteractiveSessionRole/GlueJobRunnerSession is not authorized to perform: glue:GetConnection on resource: arn:aws:glue:us-east-1:043916019468:connection/hudi-connection because no identity-based policy allows the glue:GetConnection action (Service: AWSGlue; Status Code: 400; Error Code: AccessDeniedException; Request ID: 076c293c-84dd-4fea-8b6b-6ba8ef03593f; Proxy: null)
   (Service: AWSGlueJobExecutor; Status Code: 400; Error Code: InvalidInputException; Request ID: 3e0ff920-d6fb-4373-a22d-377974405457; Proxy: null)
[0m19:43:57.156021 [debug] [ThreadPool]: On list_hudidb: No close available on handle
[0m19:43:57.157023 [debug] [MainThread]: Glue adapter: cleanup called
[0m19:43:57.157023 [error] [MainThread]: Encountered an error:
Database Error
  Got an error when attempting to open a GlueSessions: Database Error
    An error occurred (InvalidInputException) when calling the CreateSession operation: DataCatalog Connection issue : User: arn:aws:sts::043916019468:assumed-role/GlueInteractiveSessionRole/GlueJobRunnerSession is not authorized to perform: glue:GetConnection on resource: arn:aws:glue:us-east-1:043916019468:connection/hudi-connection because no identity-based policy allows the glue:GetConnection action (Service: AWSGlue; Status Code: 400; Error Code: AccessDeniedException; Request ID: 076c293c-84dd-4fea-8b6b-6ba8ef03593f; Proxy: null)
     (Service: AWSGlueJobExecutor; Status Code: 400; Error Code: InvalidInputException; Request ID: 3e0ff920-d6fb-4373-a22d-377974405457; Proxy: null)
[0m19:43:57.157863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A2827D9A80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A2827DA6B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A2827DA8C0>]}
[0m19:43:57.157863 [debug] [MainThread]: Flushing usage events
[0m19:43:57.389402 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 19:47:00.477462 | a1ceca73-d2ac-4c31-bdf1-c770bd13bd90 ==============================
[0m19:47:00.477462 [info ] [MainThread]: Running with dbt=1.3.1
[0m19:47:00.478818 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:47:00.478818 [debug] [MainThread]: Tracking: tracking
[0m19:47:00.502723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014E64F055D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014E64F05C00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014E64F04910>]}
[0m19:47:00.578724 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:47:00.578724 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:47:00.583725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a1ceca73-d2ac-4c31-bdf1-c770bd13bd90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014E65105E40>]}
[0m19:47:00.589725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a1ceca73-d2ac-4c31-bdf1-c770bd13bd90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014E650D1AE0>]}
[0m19:47:00.589725 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m19:47:00.590725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a1ceca73-d2ac-4c31-bdf1-c770bd13bd90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014E650D1D20>]}
[0m19:47:00.591770 [info ] [MainThread]: 
[0m19:47:00.592779 [debug] [MainThread]: Acquiring new glue connection "master"
[0m19:47:00.593789 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m19:47:00.593789 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:47:00.593789 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m19:47:00.594751 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m19:47:00.594751 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m19:49:02.234551 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : GlueSession took more than 120 seconds to start
[0m19:49:02.235546 [debug] [ThreadPool]: On list_hudidb: No close available on handle
[0m19:49:02.236545 [debug] [MainThread]: Glue adapter: cleanup called
[0m19:49:02.236545 [error] [MainThread]: Encountered an error:
Database Error
  Got an error when attempting to open a GlueSessions: GlueSession took more than 120 seconds to start
[0m19:49:02.237544 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014E65A8BF40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014E65A8A7A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014E65A8A890>]}
[0m19:49:02.237544 [debug] [MainThread]: Flushing usage events
[0m19:49:02.466217 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 19:59:58.741430 | 006e7951-5e15-4056-badf-ec3376f1fb54 ==============================
[0m19:59:58.741430 [info ] [MainThread]: Running with dbt=1.3.1
[0m19:59:58.742423 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:59:58.742423 [debug] [MainThread]: Tracking: tracking
[0m19:59:58.762422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014693225630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014693225900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014693227670>]}
[0m19:59:58.843430 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:59:58.843430 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:59:58.849424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '006e7951-5e15-4056-badf-ec3376f1fb54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014693425E70>]}
[0m19:59:58.855424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '006e7951-5e15-4056-badf-ec3376f1fb54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000146933F1B10>]}
[0m19:59:58.855424 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m19:59:58.856423 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '006e7951-5e15-4056-badf-ec3376f1fb54', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000146933F1D50>]}
[0m19:59:58.857423 [info ] [MainThread]: 
[0m19:59:58.858470 [debug] [MainThread]: Acquiring new glue connection "master"
[0m19:59:58.859435 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m19:59:58.860422 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:59:58.860422 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m19:59:58.860422 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m19:59:58.860422 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m20:02:00.097067 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : GlueSession took more than 120 seconds to start
[0m20:02:00.098061 [debug] [ThreadPool]: On list_hudidb: No close available on handle
[0m20:02:00.099065 [debug] [MainThread]: Glue adapter: cleanup called
[0m20:02:00.099065 [error] [MainThread]: Encountered an error:
Database Error
  Got an error when attempting to open a GlueSessions: GlueSession took more than 120 seconds to start
[0m20:02:00.100062 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014693DAB700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014693DAA800>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000014693DAAEF0>]}
[0m20:02:00.100062 [debug] [MainThread]: Flushing usage events
[0m20:02:00.306241 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 20:02:46.390976 | 91fe4c63-ef3e-4177-970f-0c85e9adec77 ==============================
[0m20:02:46.390976 [info ] [MainThread]: Running with dbt=1.3.1
[0m20:02:46.390976 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:02:46.391969 [debug] [MainThread]: Tracking: tracking
[0m20:02:46.411970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002177C414A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002177C415DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002177C417700>]}
[0m20:02:46.437025 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m20:02:46.438033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '91fe4c63-ef3e-4177-970f-0c85e9adec77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002177BE99960>]}
[0m20:02:46.490024 [debug] [MainThread]: Parsing macros\adapters.sql
[0m20:02:46.508033 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m20:02:46.509034 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m20:02:46.510046 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m20:02:46.513029 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m20:02:46.524975 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m20:02:46.525977 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m20:02:46.533976 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m20:02:46.536976 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m20:02:46.538976 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m20:02:46.539976 [debug] [MainThread]: Parsing macros\adapters.sql
[0m20:02:46.571975 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m20:02:46.573975 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m20:02:46.580976 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m20:02:46.599976 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m20:02:46.604975 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m20:02:46.604975 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m20:02:46.606975 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m20:02:46.612976 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m20:02:46.618983 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m20:02:46.622975 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m20:02:46.622975 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m20:02:46.623975 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m20:02:46.624979 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m20:02:46.624979 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m20:02:46.625978 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m20:02:46.626976 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m20:02:46.627976 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m20:02:46.631976 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m20:02:46.640976 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m20:02:46.642976 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m20:02:46.643976 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m20:02:46.643976 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m20:02:46.652976 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m20:02:46.658976 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m20:02:46.659976 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m20:02:46.661976 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m20:02:46.665976 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m20:02:46.668976 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m20:02:46.677975 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m20:02:46.679975 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m20:02:46.681975 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m20:02:46.686976 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m20:02:46.690976 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m20:02:46.691975 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m20:02:46.691975 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m20:02:46.692976 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m20:02:46.693975 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m20:02:46.694976 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m20:02:46.695976 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m20:02:46.697975 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m20:02:46.698976 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m20:02:46.700976 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m20:02:46.706975 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m20:02:46.712975 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m20:02:46.714976 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m20:02:46.724976 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m20:02:46.736970 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m20:02:46.741975 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m20:02:46.744980 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m20:02:46.748976 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m20:02:46.751976 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m20:02:46.753976 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m20:02:46.754976 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m20:02:46.758976 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m20:02:46.771976 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m20:02:46.776977 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m20:02:46.786981 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m20:02:46.795976 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m20:02:46.796976 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m20:02:46.808976 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m20:02:46.810976 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m20:02:46.813976 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m20:02:46.815976 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m20:02:46.820977 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m20:02:46.821976 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m20:02:46.822976 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m20:02:46.823976 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m20:02:46.824980 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m20:02:46.825976 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m20:02:46.826982 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m20:02:46.827976 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m20:02:46.833976 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m20:02:46.835976 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m20:02:46.836976 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m20:02:46.837976 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m20:02:46.838982 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m20:02:46.839982 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m20:02:46.841976 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m20:02:46.842976 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m20:02:46.844976 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m20:02:46.846976 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m20:02:46.848979 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m20:02:46.849977 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m20:02:46.850976 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m20:02:46.851979 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m20:02:46.852981 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m20:02:46.853976 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m20:02:46.855976 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m20:02:47.150584 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m20:02:47.183644 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'gold_category_invoice_count' in the 'models' section of file 'models\metrics\schema.yml'
[0m20:02:47.207653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '91fe4c63-ef3e-4177-970f-0c85e9adec77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000217794D4E20>]}
[0m20:02:47.211642 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '91fe4c63-ef3e-4177-970f-0c85e9adec77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002177BE4BFA0>]}
[0m20:02:47.212646 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m20:02:47.212646 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '91fe4c63-ef3e-4177-970f-0c85e9adec77', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000217793608B0>]}
[0m20:02:47.213586 [info ] [MainThread]: 
[0m20:02:47.214585 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:02:47.215643 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m20:02:47.216587 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:02:47.216587 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:02:47.216587 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m20:02:47.216587 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called


============================== 2022-12-22 20:11:31.774130 | b327985d-778e-4fc7-88c1-f3e0280bfcb5 ==============================
[0m20:11:31.774130 [info ] [MainThread]: Running with dbt=1.3.1
[0m20:11:31.774130 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:11:31.775138 [debug] [MainThread]: Tracking: tracking
[0m20:11:31.800150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E300949D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E30096B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E30097790>]}
[0m20:11:31.889130 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m20:11:31.889130 [debug] [MainThread]: Partial parsing: update schema file: dbtglue://models\metrics\schema.yml
[0m20:11:31.890129 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\popular_by_category.sql
[0m20:11:31.903129 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m20:11:31.920140 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'gold_category_invoice_count' in the 'models' section of file 'models\metrics\schema.yml'
[0m20:11:31.922137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E30262230>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E302C9240>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000029E30277100>]}
[0m20:11:31.922137 [debug] [MainThread]: Flushing usage events
[0m20:11:32.160718 [debug] [MainThread]: Glue adapter: cleanup called
[0m20:11:32.160718 [debug] [MainThread]: Glue adapter: connection not yet initialized
[0m20:11:32.161737 [error] [MainThread]: Encountered an error:
Compilation Error in model popular_by_category (models\metrics\popular_by_category.sql)
  Model 'model.dbtglue.popular_by_category' (models\metrics\popular_by_category.sql) depends on a node named 'hudi_insert_overwrite_table' which was not found


============================== 2022-12-22 20:16:20.686790 | 950f1b8c-81e8-46de-9c84-bdae4d3e90d5 ==============================
[0m20:16:20.686790 [info ] [MainThread]: Running with dbt=1.3.1
[0m20:16:20.687790 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:16:20.687790 [debug] [MainThread]: Tracking: tracking
[0m20:16:20.712787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B282FE4A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B282FE5DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B282FE7700>]}
[0m20:16:20.800787 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m20:16:20.800787 [debug] [MainThread]: Partial parsing: update schema file: dbtglue://models\metrics\schema.yml
[0m20:16:20.801788 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\popular_by_category.sql
[0m20:16:20.814787 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m20:16:20.832787 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'gold_category_invoice_count' in the 'models' section of file 'models\metrics\schema.yml'
[0m20:16:20.851787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '950f1b8c-81e8-46de-9c84-bdae4d3e90d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B2831D4FD0>]}
[0m20:16:20.859786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '950f1b8c-81e8-46de-9c84-bdae4d3e90d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B28319FD90>]}
[0m20:16:20.859786 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m20:16:20.860791 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '950f1b8c-81e8-46de-9c84-bdae4d3e90d5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B283191510>]}
[0m20:16:20.862792 [info ] [MainThread]: 
[0m20:16:20.864792 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:16:20.865787 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m20:16:20.865787 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:16:20.866787 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:16:20.866787 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m20:16:20.866787 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m20:21:22.628543 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : GlueSession took more than 300 seconds to start
[0m20:21:22.629536 [debug] [ThreadPool]: On list_hudidb: No close available on handle
[0m20:21:22.630544 [debug] [MainThread]: Glue adapter: cleanup called
[0m20:21:22.630544 [error] [MainThread]: Encountered an error:
Database Error
  Got an error when attempting to open a GlueSessions: GlueSession took more than 300 seconds to start
[0m20:21:22.631536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B282F7E0B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B282F7EE00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B282F7E2F0>]}
[0m20:21:22.631536 [debug] [MainThread]: Flushing usage events
[0m20:21:23.376768 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 20:22:34.809640 | 97930c29-17fc-4fa9-9c07-7a6ebbf9cd22 ==============================
[0m20:22:34.809640 [info ] [MainThread]: Running with dbt=1.3.1
[0m20:22:34.810640 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:22:34.810640 [debug] [MainThread]: Tracking: tracking
[0m20:22:34.833633 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C1F7B55D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C1F7B5C00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C1F7B4910>]}
[0m20:22:34.862640 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m20:22:34.863640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '97930c29-17fc-4fa9-9c07-7a6ebbf9cd22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C1F239D80>]}
[0m20:22:34.922640 [debug] [MainThread]: Parsing macros\adapters.sql
[0m20:22:34.943640 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m20:22:34.944641 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m20:22:34.945640 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m20:22:34.949642 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m20:22:34.962641 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m20:22:34.962641 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m20:22:34.971634 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m20:22:34.974640 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m20:22:34.976640 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m20:22:34.977640 [debug] [MainThread]: Parsing macros\adapters.sql
[0m20:22:35.010641 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m20:22:35.012640 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m20:22:35.020641 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m20:22:35.039640 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m20:22:35.044641 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m20:22:35.044641 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m20:22:35.046640 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m20:22:35.052648 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m20:22:35.059640 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m20:22:35.063640 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m20:22:35.063640 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m20:22:35.064643 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m20:22:35.065642 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m20:22:35.065642 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m20:22:35.067637 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m20:22:35.067637 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m20:22:35.068637 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m20:22:35.072640 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m20:22:35.080640 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m20:22:35.082647 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m20:22:35.083641 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m20:22:35.083641 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m20:22:35.091640 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m20:22:35.097640 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m20:22:35.098646 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m20:22:35.100641 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m20:22:35.105640 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m20:22:35.108640 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m20:22:35.118640 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m20:22:35.119640 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m20:22:35.122640 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m20:22:35.127640 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m20:22:35.131640 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m20:22:35.133640 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m20:22:35.133640 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m20:22:35.134640 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m20:22:35.135640 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m20:22:35.136640 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m20:22:35.137640 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m20:22:35.139640 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m20:22:35.141640 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m20:22:35.143640 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m20:22:35.149640 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m20:22:35.156640 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m20:22:35.157640 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m20:22:35.167647 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m20:22:35.180640 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m20:22:35.186633 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m20:22:35.188640 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m20:22:35.192640 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m20:22:35.195640 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m20:22:35.196640 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m20:22:35.197640 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m20:22:35.201640 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m20:22:35.214633 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m20:22:35.219640 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m20:22:35.228640 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m20:22:35.236640 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m20:22:35.238633 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m20:22:35.250640 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m20:22:35.251640 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m20:22:35.255641 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m20:22:35.256641 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m20:22:35.261640 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m20:22:35.262633 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m20:22:35.263637 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m20:22:35.264641 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m20:22:35.265640 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m20:22:35.266641 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m20:22:35.267640 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m20:22:35.268643 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m20:22:35.273640 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m20:22:35.275640 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m20:22:35.276640 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m20:22:35.277640 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m20:22:35.278633 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m20:22:35.279642 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m20:22:35.279642 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m20:22:35.280640 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m20:22:35.282640 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m20:22:35.282640 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m20:22:35.284641 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m20:22:35.285640 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m20:22:35.286640 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m20:22:35.287640 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m20:22:35.288640 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m20:22:35.289640 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m20:22:35.291643 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m20:22:35.559632 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m20:22:35.593633 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'gold_category_invoice_count' in the 'models' section of file 'models\metrics\schema.yml'
[0m20:22:35.617640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '97930c29-17fc-4fa9-9c07-7a6ebbf9cd22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C1F9B3FD0>]}
[0m20:22:35.624653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '97930c29-17fc-4fa9-9c07-7a6ebbf9cd22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C1F839660>]}
[0m20:22:35.625640 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m20:22:35.626648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97930c29-17fc-4fa9-9c07-7a6ebbf9cd22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C1F952AA0>]}
[0m20:22:35.627640 [info ] [MainThread]: 
[0m20:22:35.628640 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:22:35.629633 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m20:22:35.629633 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:22:35.630633 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:22:35.630633 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m20:22:35.630633 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m20:23:14.444594 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m20:23:14.445594 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-3be06655-21e6-4fea-b063-de7eb230ccbd
[0m20:23:22.509823 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m20:23:22.718467 [debug] [ThreadPool]: On list_hudidb: Close
[0m20:23:22.719461 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m20:23:22.722454 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m20:23:22.723453 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:23:22.723453 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:23:22.904742 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m20:23:22.905734 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m20:23:22.905734 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-3be06655-21e6-4fea-b063-de7eb230ccbd
[0m20:23:24.586530 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m20:23:24.862454 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m20:23:24.862454 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m20:23:24.863454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '97930c29-17fc-4fa9-9c07-7a6ebbf9cd22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C1F9B45B0>]}
[0m20:23:24.863454 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m20:23:24.863454 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m20:23:24.864454 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:23:24.864454 [info ] [MainThread]: 
[0m20:23:24.868948 [debug] [Thread-1 (]: Began running node model.dbtglue.popular_by_category
[0m20:23:24.869948 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.popular_by_category .................. [RUN]
[0m20:23:24.869948 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.popular_by_category"
[0m20:23:24.870951 [debug] [Thread-1 (]: Began compiling node model.dbtglue.popular_by_category
[0m20:23:24.870951 [debug] [Thread-1 (]: Compiling model.dbtglue.popular_by_category
[0m20:23:24.874951 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.popular_by_category"
[0m20:23:24.875947 [debug] [Thread-1 (]: finished collecting timing info
[0m20:23:24.875947 [debug] [Thread-1 (]: Began executing node model.dbtglue.popular_by_category
[0m20:23:24.894949 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:23:24.895951 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m20:23:25.075919 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m20:23:25.076943 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m20:23:25.076943 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-3be06655-21e6-4fea-b063-de7eb230ccbd
[0m20:23:26.982892 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:23:27.490023 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table popular_by_category not found.
[0m20:23:27.498073 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m20:23:27.499083 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m20:23:27.499083 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m20:23:27.499083 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:23:27.499083 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:23:27.500077 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:23:27.500077 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:23:27.500077 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m20:23:40.210759 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671740608800, 'CompletedOn': 1671740620486}, 'ResponseMetadata': {'RequestId': '9e550f4e-1eb4-4c85-8840-b91d6253ec2d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:23:41 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '708', 'connection': 'keep-alive', 'x-amzn-requestid': '9e550f4e-1eb4-4c85-8840-b91d6253ec2d'}, 'RetryAttempts': 0}}
[0m20:23:40.211925 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m20:23:40.212753 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m20:23:40.212753 [debug] [Thread-1 (]: SQL status: OK in 12.71 seconds
[0m20:23:40.217748 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:23:40.219747 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:23:40.571332 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table popular_by_category not found.
[0m20:23:40.571332 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)




select
    source_data.invoiceid,
    source_data.category,
    source_data.itemid,
    source_data.referral
FROM hudidb.order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
        
spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
        
        
[0m20:23:40.571332 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:23:40.572332 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:23:40.572332 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:23:40.572332 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)




select
    source_data.invoiceid,
    source_data.category,
    source_data.itemid,
    source_data.referral
FROM hudidb.order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")

[0m20:23:44.445252 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""with source_data as (\n    SELECT\n        invoiceid,\n        category,\n        itemid,\n        referral\n    FROM hudidb.order\n\n)\n\n\n\n\nselect\n    source_data.invoiceid,\n    source_data.category,\n    source_data.itemid,\n    source_data.referral\nFROM hudidb.order""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'popular_by_category\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'popular_by_category\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'popular_by_category\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'popular_by_category\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")\n\nspark.sql("""REFRESH TABLE hudidb.popular_by_category""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 7, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "cannot resolve '`source_data.invoiceid`' given input columns: [spark_catalog.hudidb.order._hoodie_commit_seqno, spark_catalog.hudidb.order._hoodie_commit_time, spark_catalog.hudidb.order._hoodie_file_name, spark_catalog.hudidb.order._hoodie_partition_path, spark_catalog.hudidb.order._hoodie_record_key, spark_catalog.hudidb.order.category, spark_catalog.hudidb.order.destinationstate, spark_catalog.hudidb.order.invoiceid, spark_catalog.hudidb.order.itemid, spark_catalog.hudidb.order.orderdate, spark_catalog.hudidb.order.price, spark_catalog.hudidb.order.quantity, spark_catalog.hudidb.order.referral, spark_catalog.hudidb.order.shippingtype]; line 15 pos 4;\n'Project ['source_data.invoiceid, 'source_data.category, 'source_data.itemid, 'source_data.referral]\n+- SubqueryAlias spark_catalog.hudidb.order\n   +- Relation[_hoodie_commit_time#17,_hoodie_commit_seqno#18,_hoodie_record_key#19,_hoodie_partition_path#20,_hoodie_file_name#21,invoiceid#22L,itemid#23L,category#24,price#25L,quantity#26L,orderdate#27,destinationstate#28,shippingtype#29,referral#30] parquet\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: cannot resolve '`source_data.invoiceid`' given input columns: [spark_catalog.hudidb.order._hoodie_commit_seqno, spark_catalog.hudidb.order._hoodie_commit_time, spark_catalog.hudidb.order._hoodie_file_name, spark_catalog.hudidb.order._hoodie_partition_path, spark_catalog.hudidb.order._hoodie_record_key, spark_catalog.hudidb.order.category, spark_catalog.hudidb.order.destinationstate, spark_catalog.hudidb.order.invoiceid, spark_catalog.hudidb.order.itemid, spark_catalog.hudidb.order.orderdate, spark_catalog.hudidb.order.price, spark_catalog.hudidb.order.quantity, spark_catalog.hudidb.order.referral, spark_catalog.hudidb.order.shippingtype]; line 15 pos 4;\n'Project ['source_data.invoiceid, 'source_data.category, 'source_data.itemid, 'source_data.referral]\n+- SubqueryAlias spark_catalog.hudidb.order\n   +- Relation[_hoodie_commit_time#17,_hoodie_commit_seqno#18,_hoodie_record_key#19,_hoodie_partition_path#20,_hoodie_file_name#21,invoiceid#22L,itemid#23L,category#24,price#25L,quantity#26L,orderdate#27,destinationstate#28,shippingtype#29,referral#30] parquet\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671740621905, 'CompletedOn': 1671740625101}, 'ResponseMetadata': {'RequestId': '99c761f0-7a8e-44c8-94a2-c682de575129', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:23:45 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '5507', 'connection': 'keep-alive', 'x-amzn-requestid': '99c761f0-7a8e-44c8-94a2-c682de575129'}, 'RetryAttempts': 0}}
[0m20:23:44.445252 [debug] [Thread-1 (]: Glue adapter: status = error
[0m20:23:44.446253 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)




select
    source_data.invoiceid,
    source_data.category,
    source_data.itemid,
    source_data.referral
FROM hudidb.order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
, AnalysisException: cannot resolve '`source_data.invoiceid`' given input columns: [spark_catalog.hudidb.order._hoodie_commit_seqno, spark_catalog.hudidb.order._hoodie_commit_time, spark_catalog.hudidb.order._hoodie_file_name, spark_catalog.hudidb.order._hoodie_partition_path, spark_catalog.hudidb.order._hoodie_record_key, spark_catalog.hudidb.order.category, spark_catalog.hudidb.order.destinationstate, spark_catalog.hudidb.order.invoiceid, spark_catalog.hudidb.order.itemid, spark_catalog.hudidb.order.orderdate, spark_catalog.hudidb.order.price, spark_catalog.hudidb.order.quantity, spark_catalog.hudidb.order.referral, spark_catalog.hudidb.order.shippingtype]; line 15 pos 4;
'Project ['source_data.invoiceid, 'source_data.category, 'source_data.itemid, 'source_data.referral]
+- SubqueryAlias spark_catalog.hudidb.order
   +- Relation[_hoodie_commit_time#17,_hoodie_commit_seqno#18,_hoodie_record_key#19,_hoodie_partition_path#20,_hoodie_file_name#21,invoiceid#22L,itemid#23L,category#24,price#25L,quantity#26L,orderdate#27,destinationstate#28,shippingtype#29,referral#30] parquet

[0m20:23:44.448253 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)




select
    source_data.invoiceid,
    source_data.category,
    source_data.itemid,
    source_data.referral
FROM hudidb.order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
, AnalysisException: cannot resolve '`source_data.invoiceid`' given input columns: [spark_catalog.hudidb.order._hoodie_commit_seqno, spark_catalog.hudidb.order._hoodie_commit_time, spark_catalog.hudidb.order._hoodie_file_name, spark_catalog.hudidb.order._hoodie_partition_path, spark_catalog.hudidb.order._hoodie_record_key, spark_catalog.hudidb.order.category, spark_catalog.hudidb.order.destinationstate, spark_catalog.hudidb.order.invoiceid, spark_catalog.hudidb.order.itemid, spark_catalog.hudidb.order.orderdate, spark_catalog.hudidb.order.price, spark_catalog.hudidb.order.quantity, spark_catalog.hudidb.order.referral, spark_catalog.hudidb.order.shippingtype]; line 15 pos 4;
'Project ['source_data.invoiceid, 'source_data.category, 'source_data.itemid, 'source_data.referral]
+- SubqueryAlias spark_catalog.hudidb.order
   +- Relation[_hoodie_commit_time#17,_hoodie_commit_seqno#18,_hoodie_record_key#19,_hoodie_partition_path#20,_hoodie_file_name#21,invoiceid#22L,itemid#23L,category#24,price#25L,quantity#26L,orderdate#27,destinationstate#28,shippingtype#29,referral#30] parquet

[0m20:23:44.448253 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""with source_data as (
      SELECT
          invoiceid,
          category,
          itemid,
          referral
      FROM hudidb.order
  
  )
  
  
  
  
  select
      source_data.invoiceid,
      source_data.category,
      source_data.itemid,
      source_data.referral
  FROM hudidb.order""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if None is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
  
  spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
  SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
  , AnalysisException: cannot resolve '`source_data.invoiceid`' given input columns: [spark_catalog.hudidb.order._hoodie_commit_seqno, spark_catalog.hudidb.order._hoodie_commit_time, spark_catalog.hudidb.order._hoodie_file_name, spark_catalog.hudidb.order._hoodie_partition_path, spark_catalog.hudidb.order._hoodie_record_key, spark_catalog.hudidb.order.category, spark_catalog.hudidb.order.destinationstate, spark_catalog.hudidb.order.invoiceid, spark_catalog.hudidb.order.itemid, spark_catalog.hudidb.order.orderdate, spark_catalog.hudidb.order.price, spark_catalog.hudidb.order.quantity, spark_catalog.hudidb.order.referral, spark_catalog.hudidb.order.shippingtype]; line 15 pos 4;
  'Project ['source_data.invoiceid, 'source_data.category, 'source_data.itemid, 'source_data.referral]
  +- SubqueryAlias spark_catalog.hudidb.order
     +- Relation[_hoodie_commit_time#17,_hoodie_commit_seqno#18,_hoodie_record_key#19,_hoodie_partition_path#20,_hoodie_file_name#21,invoiceid#22L,itemid#23L,category#24,price#25L,quantity#26L,orderdate#27,destinationstate#28,shippingtype#29,referral#30] parquet
  
[0m20:23:44.451252 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.popular_by_category"
[0m20:23:44.452253 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m20:23:44.452253 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 
[0m20:23:44.453252 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:23:44.453252 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:23:44.453252 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:23:44.453252 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:23:44.453252 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 ''')
[0m20:23:44.956806 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\nselect * from hudidb.popular_by_category limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 8, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: hudidb.popular_by_category; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671740625754, 'CompletedOn': 1671740626038}, 'ResponseMetadata': {'RequestId': '8f0b77c9-1841-42f6-afcc-3cd0fff41ffa', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:23:46 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1458', 'connection': 'keep-alive', 'x-amzn-requestid': '8f0b77c9-1841-42f6-afcc-3cd0fff41ffa'}, 'RetryAttempts': 0}}
[0m20:23:44.956806 [debug] [Thread-1 (]: Glue adapter: status = error
[0m20:23:44.957889 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false

[0m20:23:44.958893 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false

[0m20:23:44.958893 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 
[0m20:23:44.958893 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: ROLLBACK
[0m20:23:44.958893 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m20:23:44.959881 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: Close
[0m20:23:44.959881 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m20:23:44.959881 [debug] [Thread-1 (]: finished collecting timing info
[0m20:23:44.959881 [debug] [Thread-1 (]: Database Error in model popular_by_category (models\metrics\popular_by_category.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
  select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [hudidb, popular_by_category], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m20:23:44.960878 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '97930c29-17fc-4fa9-9c07-7a6ebbf9cd22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C2059E380>]}
[0m20:23:44.960878 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model hudidb.popular_by_category ......... [[31mERROR[0m in 20.09s]
[0m20:23:44.961818 [debug] [Thread-1 (]: Finished running node model.dbtglue.popular_by_category
[0m20:23:44.962824 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:23:44.962824 [debug] [MainThread]: On master: ROLLBACK
[0m20:23:44.962824 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:23:44.963818 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m20:23:45.151371 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m20:23:45.152401 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m20:23:45.152401 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-3be06655-21e6-4fea-b063-de7eb230ccbd
[0m20:23:46.792273 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m20:23:46.793278 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m20:23:46.794277 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m20:23:46.794277 [debug] [MainThread]: On master: ROLLBACK
[0m20:23:46.795273 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m20:23:46.795273 [debug] [MainThread]: On master: Close
[0m20:23:46.796283 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m20:23:46.797305 [info ] [MainThread]: 
[0m20:23:46.798201 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 11.17 seconds (71.17s).
[0m20:23:46.800198 [debug] [MainThread]: Glue adapter: cleanup called
[0m20:23:47.017171 [info ] [MainThread]: 
[0m20:23:47.017777 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:23:47.018783 [info ] [MainThread]: 
[0m20:23:47.018783 [error] [MainThread]: [33mDatabase Error in model popular_by_category (models\metrics\popular_by_category.sql)[0m
[0m20:23:47.019787 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
[0m20:23:47.020782 [error] [MainThread]:   select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
[0m20:23:47.020782 [error] [MainThread]:   'GlobalLimit 1
[0m20:23:47.020782 [error] [MainThread]:   +- 'LocalLimit 1
[0m20:23:47.021782 [error] [MainThread]:      +- 'Project [*]
[0m20:23:47.021782 [error] [MainThread]:         +- 'UnresolvedRelation [hudidb, popular_by_category], [], false
[0m20:23:47.021782 [error] [MainThread]:   
[0m20:23:47.022782 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m20:23:47.022782 [info ] [MainThread]: 
[0m20:23:47.022782 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:23:47.023781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C1FF71FC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C205F3D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017C205F36A0>]}
[0m20:23:47.023781 [debug] [MainThread]: Flushing usage events
[0m20:23:47.717007 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 20:25:37.527347 | ee2f09ae-4b03-4d6e-85d5-a492f04b7e24 ==============================
[0m20:25:37.527347 [info ] [MainThread]: Running with dbt=1.3.1
[0m20:25:37.527347 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:25:37.527347 [debug] [MainThread]: Tracking: tracking
[0m20:25:37.549294 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021590764A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021590765DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021590767700>]}
[0m20:25:37.623291 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:25:37.623291 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\popular_by_category.sql
[0m20:25:37.635285 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m20:25:37.657292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ee2f09ae-4b03-4d6e-85d5-a492f04b7e24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215909CCDC0>]}
[0m20:25:37.663292 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ee2f09ae-4b03-4d6e-85d5-a492f04b7e24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002159095BFA0>]}
[0m20:25:37.663292 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m20:25:37.664286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ee2f09ae-4b03-4d6e-85d5-a492f04b7e24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021590911510>]}
[0m20:25:37.665284 [info ] [MainThread]: 
[0m20:25:37.666286 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:25:37.667285 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m20:25:37.667285 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:25:37.668291 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:25:37.668291 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m20:25:37.668291 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m20:26:34.140886 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m20:26:34.141884 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-0d8e04b5-82e2-4912-ab2e-341bbc81b57f
[0m20:26:44.221776 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m20:26:44.624060 [debug] [ThreadPool]: On list_hudidb: Close
[0m20:26:44.625053 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m20:26:44.626052 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m20:26:44.626052 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:26:44.626052 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:26:44.803715 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m20:26:44.804706 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m20:26:44.804706 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-0d8e04b5-82e2-4912-ab2e-341bbc81b57f
[0m20:26:46.656986 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m20:26:46.909513 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m20:26:46.910516 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m20:26:46.912145 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ee2f09ae-4b03-4d6e-85d5-a492f04b7e24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021590933FA0>]}
[0m20:26:46.913228 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m20:26:46.913228 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m20:26:46.914244 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:26:46.915770 [info ] [MainThread]: 
[0m20:26:46.922776 [debug] [Thread-1 (]: Began running node model.dbtglue.popular_by_category
[0m20:26:46.922776 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.popular_by_category .................. [RUN]
[0m20:26:46.923807 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.popular_by_category"
[0m20:26:46.923807 [debug] [Thread-1 (]: Began compiling node model.dbtglue.popular_by_category
[0m20:26:46.923807 [debug] [Thread-1 (]: Compiling model.dbtglue.popular_by_category
[0m20:26:46.926873 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.popular_by_category"
[0m20:26:46.927788 [debug] [Thread-1 (]: finished collecting timing info
[0m20:26:46.927788 [debug] [Thread-1 (]: Began executing node model.dbtglue.popular_by_category
[0m20:26:46.946839 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:26:46.946839 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m20:26:47.123395 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m20:26:47.124499 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m20:26:47.124499 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-0d8e04b5-82e2-4912-ab2e-341bbc81b57f
[0m20:26:48.756458 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:26:49.049932 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table popular_by_category not found.
[0m20:26:49.055926 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m20:26:49.056931 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m20:26:49.056931 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m20:26:49.056931 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:26:49.056931 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:26:49.056931 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:26:49.057944 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:26:49.057944 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m20:27:00.634946 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671740810364, 'CompletedOn': 1671740820940}, 'ResponseMetadata': {'RequestId': '65d194f6-2258-4183-a7c1-b960bf001e6c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:27:01 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '708', 'connection': 'keep-alive', 'x-amzn-requestid': '65d194f6-2258-4183-a7c1-b960bf001e6c'}, 'RetryAttempts': 0}}
[0m20:27:00.635936 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m20:27:00.636951 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m20:27:00.636951 [debug] [Thread-1 (]: SQL status: OK in 11.58 seconds
[0m20:27:00.642919 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:27:00.644932 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:27:00.927527 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table popular_by_category not found.
[0m20:27:00.928545 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)




select
    source_data.invoiceid,
    source_data.category,
    source_data.itemid,
    source_data.referral
FROM source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
        
spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
        
        
[0m20:27:00.928545 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:27:00.929535 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:27:00.929535 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:27:00.930525 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)




select
    source_data.invoiceid,
    source_data.category,
    source_data.itemid,
    source_data.referral
FROM source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")

[0m20:27:28.332841 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""with source_data as (\n    SELECT\n        invoiceid,\n        category,\n        itemid,\n        referral\n    FROM hudidb.order\n\n)\n\n\n\n\nselect\n    source_data.invoiceid,\n    source_data.category,\n    source_data.itemid,\n    source_data.referral\nFROM source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'popular_by_category\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'popular_by_category\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'popular_by_category\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'popular_by_category\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")\n\nspark.sql("""REFRESH TABLE hudidb.popular_by_category""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 7, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': 'An error occurred while calling o115.save.\n: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)\n\tat org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)\n\tat org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)\n\tat org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)\n\tat org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:77)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 60 more\n', 'Traceback': ['Traceback (most recent call last):\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1109, in save\n    self._jwrite.save(path)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', 'py4j.protocol.Py4JJavaError: An error occurred while calling o115.save.\n: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)\n\tat org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)\n\tat org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)\n\tat org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)\n\tat org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:77)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 60 more\n\n']}, 'Progress': 1.0, 'StartedOn': 1671740822247, 'CompletedOn': 1671740848663}, 'ResponseMetadata': {'RequestId': '4816901b-b987-47c7-b539-5504b3d70b73', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:27:29 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '14407', 'connection': 'keep-alive', 'x-amzn-requestid': '4816901b-b987-47c7-b539-5504b3d70b73'}, 'RetryAttempts': 0}}
[0m20:27:28.332841 [debug] [Thread-1 (]: Glue adapter: status = error
[0m20:27:28.333836 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)




select
    source_data.invoiceid,
    source_data.category,
    source_data.itemid,
    source_data.referral
FROM source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
, Py4JJavaError: An error occurred while calling o115.save.
: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)
	at org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)
	at org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)
	at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)
	at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)
	at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)
	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 60 more

[0m20:27:28.340952 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)




select
    source_data.invoiceid,
    source_data.category,
    source_data.itemid,
    source_data.referral
FROM source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
, Py4JJavaError: An error occurred while calling o115.save.
: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)
	at org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)
	at org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)
	at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)
	at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)
	at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)
	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 60 more

[0m20:27:28.342007 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""with source_data as (
      SELECT
          invoiceid,
          category,
          itemid,
          referral
      FROM hudidb.order
  
  )
  
  
  
  
  select
      source_data.invoiceid,
      source_data.category,
      source_data.itemid,
      source_data.referral
  FROM source_data""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if None is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
  
  spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
  SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
  , Py4JJavaError: An error occurred while calling o115.save.
  : java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem
  	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)
  	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)
  	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
  	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
  	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
  	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
  	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)
  	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)
  	at org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)
  	at org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)
  	at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)
  	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)
  	at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)
  	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)
  	at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)
  	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)
  	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)
  	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
  	at org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)
  	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)
  	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)
  	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
  	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
  	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
  	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
  	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
  	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
  	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
  	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  Caused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem
  	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
  	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
  	... 60 more
  
[0m20:27:28.373823 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.popular_by_category"
[0m20:27:28.374819 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m20:27:28.374819 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 
[0m20:27:28.375820 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:27:28.375820 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:27:28.375820 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:27:28.375820 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:27:28.375820 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 ''')
[0m20:27:29.773129 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\nselect * from hudidb.popular_by_category limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 8, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: hudidb.popular_by_category; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671740849688, 'CompletedOn': 1671740850019}, 'ResponseMetadata': {'RequestId': 'af389a45-aabb-4055-92e7-dea963f20660', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:27:30 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1458', 'connection': 'keep-alive', 'x-amzn-requestid': 'af389a45-aabb-4055-92e7-dea963f20660'}, 'RetryAttempts': 0}}
[0m20:27:29.774121 [debug] [Thread-1 (]: Glue adapter: status = error
[0m20:27:29.774121 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false

[0m20:27:29.777057 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false

[0m20:27:29.778059 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 
[0m20:27:29.778059 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: ROLLBACK
[0m20:27:29.779134 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m20:27:29.779134 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: Close
[0m20:27:29.779134 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m20:27:29.780055 [debug] [Thread-1 (]: finished collecting timing info
[0m20:27:29.781058 [debug] [Thread-1 (]: Database Error in model popular_by_category (models\metrics\popular_by_category.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
  select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [hudidb, popular_by_category], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m20:27:29.781058 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ee2f09ae-4b03-4d6e-85d5-a492f04b7e24', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021591522F20>]}
[0m20:27:29.782052 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model hudidb.popular_by_category ......... [[31mERROR[0m in 42.86s]
[0m20:27:29.783051 [debug] [Thread-1 (]: Finished running node model.dbtglue.popular_by_category
[0m20:27:29.785053 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:27:29.785053 [debug] [MainThread]: On master: ROLLBACK
[0m20:27:29.785053 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:27:29.785053 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m20:27:29.959098 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m20:27:29.959098 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m20:27:29.960097 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-0d8e04b5-82e2-4912-ab2e-341bbc81b57f
[0m20:27:31.564854 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m20:27:31.564854 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m20:27:31.564854 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m20:27:31.565852 [debug] [MainThread]: On master: ROLLBACK
[0m20:27:31.565852 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m20:27:31.565852 [debug] [MainThread]: On master: Close
[0m20:27:31.565852 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m20:27:31.566853 [info ] [MainThread]: 
[0m20:27:31.566853 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 53.90 seconds (113.90s).
[0m20:27:31.567845 [debug] [MainThread]: Glue adapter: cleanup called
[0m20:27:31.817228 [info ] [MainThread]: 
[0m20:27:31.818200 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:27:31.819030 [info ] [MainThread]: 
[0m20:27:31.820054 [error] [MainThread]: [33mDatabase Error in model popular_by_category (models\metrics\popular_by_category.sql)[0m
[0m20:27:31.820054 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
[0m20:27:31.821035 [error] [MainThread]:   select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
[0m20:27:31.821035 [error] [MainThread]:   'GlobalLimit 1
[0m20:27:31.821035 [error] [MainThread]:   +- 'LocalLimit 1
[0m20:27:31.822034 [error] [MainThread]:      +- 'Project [*]
[0m20:27:31.822034 [error] [MainThread]:         +- 'UnresolvedRelation [hudidb, popular_by_category], [], false
[0m20:27:31.822034 [error] [MainThread]:   
[0m20:27:31.823034 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m20:27:31.823034 [info ] [MainThread]: 
[0m20:27:31.823034 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:27:31.824034 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215906FD9C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021591573BB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000215915A0400>]}
[0m20:27:31.824034 [debug] [MainThread]: Flushing usage events
[0m20:27:32.438123 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 20:35:37.163087 | 5335dd49-0558-4e39-b11d-b68007bb5f2d ==============================
[0m20:35:37.163087 [info ] [MainThread]: Running with dbt=1.3.1
[0m20:35:37.164088 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:35:37.164088 [debug] [MainThread]: Tracking: tracking
[0m20:35:37.185263 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015DE21D4A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015DE21D5DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015DE21D7700>]}
[0m20:35:37.211260 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m20:35:37.212260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5335dd49-0558-4e39-b11d-b68007bb5f2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015DE1C5A440>]}
[0m20:35:37.261253 [debug] [MainThread]: Parsing macros\adapters.sql
[0m20:35:37.280255 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m20:35:37.281340 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m20:35:37.282310 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m20:35:37.285321 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m20:35:37.297262 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m20:35:37.298254 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m20:35:37.305261 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m20:35:37.308260 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m20:35:37.311260 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m20:35:37.312253 [debug] [MainThread]: Parsing macros\adapters.sql
[0m20:35:37.340253 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m20:35:37.343253 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m20:35:37.351253 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m20:35:37.370252 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m20:35:37.374257 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m20:35:37.375254 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m20:35:37.377254 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m20:35:37.383266 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m20:35:37.390261 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m20:35:37.394255 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m20:35:37.395258 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m20:35:37.395258 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m20:35:37.396258 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m20:35:37.396258 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m20:35:37.398258 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m20:35:37.398258 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m20:35:37.399257 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m20:35:37.403255 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m20:35:37.413255 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m20:35:37.414257 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m20:35:37.416258 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m20:35:37.416258 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m20:35:37.427255 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m20:35:37.435256 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m20:35:37.437258 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m20:35:37.439270 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m20:35:37.445257 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m20:35:37.449256 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m20:35:37.460256 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m20:35:37.462258 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m20:35:37.465258 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m20:35:37.472257 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m20:35:37.476256 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m20:35:37.477259 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m20:35:37.478258 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m20:35:37.479258 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m20:35:37.479258 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m20:35:37.481259 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m20:35:37.482259 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m20:35:37.485258 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m20:35:37.486259 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m20:35:37.490257 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m20:35:37.496256 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m20:35:37.504257 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m20:35:37.505261 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m20:35:37.516256 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m20:35:37.529255 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m20:35:37.534256 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m20:35:37.537256 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m20:35:37.541256 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m20:35:37.544256 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m20:35:37.546259 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m20:35:37.547257 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m20:35:37.551255 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m20:35:37.565256 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m20:35:37.571929 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m20:35:37.580670 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m20:35:37.587641 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m20:35:37.588638 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m20:35:37.598568 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m20:35:37.600626 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m20:35:37.602630 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m20:35:37.603637 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m20:35:37.607635 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m20:35:37.608637 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m20:35:37.609628 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m20:35:37.609628 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m20:35:37.610633 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m20:35:37.611629 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m20:35:37.612625 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m20:35:37.613642 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m20:35:37.617646 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m20:35:37.618644 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m20:35:37.619624 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m20:35:37.620634 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m20:35:37.620634 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m20:35:37.621625 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m20:35:37.622624 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m20:35:37.623649 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m20:35:37.624628 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m20:35:37.625634 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m20:35:37.626580 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m20:35:37.627575 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m20:35:37.628574 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m20:35:37.629574 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m20:35:37.630567 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m20:35:37.631577 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m20:35:37.632577 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m20:35:37.919598 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m20:35:37.952604 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'gold_category_invoice_count' in the 'models' section of file 'models\metrics\schema.yml'
[0m20:35:37.980597 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5335dd49-0558-4e39-b11d-b68007bb5f2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015DDF0871F0>]}
[0m20:35:37.987621 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5335dd49-0558-4e39-b11d-b68007bb5f2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015DE22594E0>]}
[0m20:35:37.987621 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m20:35:37.988605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5335dd49-0558-4e39-b11d-b68007bb5f2d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015DDF18D390>]}
[0m20:35:37.989605 [info ] [MainThread]: 
[0m20:35:37.990597 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:35:37.991597 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m20:35:37.991597 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:35:37.991597 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:35:37.992604 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m20:35:37.992604 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m20:36:28.780935 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m20:36:28.780935 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-e6bcce75-0810-472e-942b-cac3512d0d9b
[0m20:36:37.307692 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m20:36:37.530661 [debug] [ThreadPool]: On list_hudidb: Close
[0m20:36:37.531605 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m20:36:37.533674 [debug] [MainThread]: Glue adapter: cleanup called
[0m20:36:37.773129 [debug] [MainThread]: Flushing usage events
[0m20:36:37.972534 [debug] [MainThread]: Glue adapter: cleanup called
[0m20:36:38.047807 [info ] [MainThread]: ctrl-c


============================== 2022-12-22 20:40:57.984716 | f0902db9-985d-4095-94a6-1b156e1d3e95 ==============================
[0m20:40:57.984716 [info ] [MainThread]: Running with dbt=1.3.1
[0m20:40:57.985719 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:40:57.985719 [debug] [MainThread]: Tracking: tracking
[0m20:40:58.008714 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0702149D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B070216B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B070217790>]}
[0m20:40:58.095720 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:40:58.096720 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\popular_by_category.sql
[0m20:40:58.109723 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m20:40:58.135722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f0902db9-985d-4095-94a6-1b156e1d3e95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B070484DF0>]}
[0m20:40:58.143722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f0902db9-985d-4095-94a6-1b156e1d3e95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B07040BC10>]}
[0m20:40:58.143722 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m20:40:58.144724 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f0902db9-985d-4095-94a6-1b156e1d3e95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0703C1540>]}
[0m20:40:58.145721 [info ] [MainThread]: 
[0m20:40:58.146725 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:40:58.147721 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m20:40:58.148721 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:40:58.148721 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:40:58.148721 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m20:40:58.148721 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m20:41:37.011712 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m20:41:37.012709 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9aa2663c-ae6f-4dc0-bdfc-0b6ce6476f18
[0m20:41:46.468898 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m20:41:47.046408 [debug] [ThreadPool]: On list_hudidb: Close
[0m20:41:47.046408 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m20:41:47.048922 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m20:41:47.048922 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:41:47.048922 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:41:47.250919 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m20:41:47.250919 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m20:41:47.251921 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9aa2663c-ae6f-4dc0-bdfc-0b6ce6476f18
[0m20:41:49.153831 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m20:41:49.421504 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m20:41:49.421504 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m20:41:49.422505 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f0902db9-985d-4095-94a6-1b156e1d3e95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B0703F2D70>]}
[0m20:41:49.423505 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m20:41:49.423505 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m20:41:49.424505 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:41:49.424505 [info ] [MainThread]: 
[0m20:41:49.429520 [debug] [Thread-1 (]: Began running node model.dbtglue.popular_by_category
[0m20:41:49.429520 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.popular_by_category .................. [RUN]
[0m20:41:49.430505 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.popular_by_category"
[0m20:41:49.430505 [debug] [Thread-1 (]: Began compiling node model.dbtglue.popular_by_category
[0m20:41:49.430505 [debug] [Thread-1 (]: Compiling model.dbtglue.popular_by_category
[0m20:41:49.435506 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.popular_by_category"
[0m20:41:49.436505 [debug] [Thread-1 (]: finished collecting timing info
[0m20:41:49.436505 [debug] [Thread-1 (]: Began executing node model.dbtglue.popular_by_category
[0m20:41:49.469498 [debug] [Thread-1 (]: finished collecting timing info
[0m20:41:49.470505 [debug] [Thread-1 (]: Compilation Error in model popular_by_category (models\metrics\popular_by_category.sql)
  Invalid incremental strategy provided: upsert
      Expected one of: 'append', 'merge', 'insert_overwrite'
  
  > in macro dbt_spark_validate_get_incremental_strategy (macros\materializations\incremental\validate.sql)
  > called by macro materialization_incremental_glue (macros\materializations\incremental\incremental.sql)
  > called by model popular_by_category (models\metrics\popular_by_category.sql)
[0m20:41:49.471506 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f0902db9-985d-4095-94a6-1b156e1d3e95', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B070E425F0>]}
[0m20:41:49.471506 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model hudidb.popular_by_category ......... [[31mERROR[0m in 0.04s]
[0m20:41:49.472505 [debug] [Thread-1 (]: Finished running node model.dbtglue.popular_by_category
[0m20:41:49.474504 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:41:49.474504 [debug] [MainThread]: On master: ROLLBACK
[0m20:41:49.474504 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:41:49.475505 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m20:41:49.637519 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m20:41:49.637519 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m20:41:49.637519 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9aa2663c-ae6f-4dc0-bdfc-0b6ce6476f18
[0m20:41:51.235648 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m20:41:51.236640 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m20:41:51.237637 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m20:41:51.238636 [debug] [MainThread]: On master: ROLLBACK
[0m20:41:51.238636 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m20:41:51.239636 [debug] [MainThread]: On master: Close
[0m20:41:51.239636 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m20:41:51.241633 [info ] [MainThread]: 
[0m20:41:51.242634 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 0 minutes and 53.09 seconds (53.09s).
[0m20:41:51.243635 [debug] [MainThread]: Glue adapter: cleanup called
[0m20:41:51.452422 [info ] [MainThread]: 
[0m20:41:51.452422 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:41:51.453424 [info ] [MainThread]: 
[0m20:41:51.454422 [error] [MainThread]: [33mCompilation Error in model popular_by_category (models\metrics\popular_by_category.sql)[0m
[0m20:41:51.455422 [error] [MainThread]:   Invalid incremental strategy provided: upsert
[0m20:41:51.455422 [error] [MainThread]:       Expected one of: 'append', 'merge', 'insert_overwrite'
[0m20:41:51.456424 [error] [MainThread]:   
[0m20:41:51.456922 [error] [MainThread]:   > in macro dbt_spark_validate_get_incremental_strategy (macros\materializations\incremental\validate.sql)
[0m20:41:51.457358 [error] [MainThread]:   > called by macro materialization_incremental_glue (macros\materializations\incremental\incremental.sql)
[0m20:41:51.457358 [error] [MainThread]:   > called by model popular_by_category (models\metrics\popular_by_category.sql)
[0m20:41:51.457358 [info ] [MainThread]: 
[0m20:41:51.458366 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:41:51.458366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B070E76740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B070E766E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B070E769E0>]}
[0m20:41:51.458366 [debug] [MainThread]: Flushing usage events
[0m20:41:51.683826 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 20:44:38.356014 | 9087ce1f-3efe-45f4-8fcd-c76ccba43c11 ==============================
[0m20:44:38.356014 [info ] [MainThread]: Running with dbt=1.3.1
[0m20:44:38.357014 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:44:38.357014 [debug] [MainThread]: Tracking: tracking
[0m20:44:38.377044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AE2F49D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AE2F6B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AE2F7790>]}
[0m20:44:38.455051 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:44:38.456051 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\popular_by_category.sql
[0m20:44:38.469043 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m20:44:38.493042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9087ce1f-3efe-45f4-8fcd-c76ccba43c11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AE558DF0>]}
[0m20:44:38.499043 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9087ce1f-3efe-45f4-8fcd-c76ccba43c11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AE4EBC10>]}
[0m20:44:38.500042 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m20:44:38.500042 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9087ce1f-3efe-45f4-8fcd-c76ccba43c11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000240AE4A1540>]}
[0m20:44:38.501042 [info ] [MainThread]: 
[0m20:44:38.502042 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:44:38.504042 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m20:44:38.504042 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:44:38.504042 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:44:38.504042 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m20:44:38.504042 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called


============================== 2022-12-22 20:45:10.244196 | 237890ce-1c9c-4b0d-8bd1-411cdceb6db5 ==============================
[0m20:45:10.244196 [info ] [MainThread]: Running with dbt=1.3.1
[0m20:45:10.245187 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:45:10.245187 [debug] [MainThread]: Tracking: tracking
[0m20:45:10.266187 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B11C25630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B11C25900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B11C27670>]}
[0m20:45:10.337194 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:45:10.337194 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\popular_by_category.sql
[0m20:45:10.349186 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m20:45:10.371191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '237890ce-1c9c-4b0d-8bd1-411cdceb6db5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B11E8CD90>]}
[0m20:45:10.377191 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '237890ce-1c9c-4b0d-8bd1-411cdceb6db5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B11E1BF10>]}
[0m20:45:10.377191 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m20:45:10.378504 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '237890ce-1c9c-4b0d-8bd1-411cdceb6db5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000027B11DD14E0>]}
[0m20:45:10.378504 [info ] [MainThread]: 
[0m20:45:10.379564 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:45:10.381576 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m20:45:10.381576 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:45:10.381576 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:45:10.381576 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m20:45:10.382528 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called


============================== 2022-12-22 20:45:30.701314 | 9b970d1f-c1d7-47a7-a884-c52be61a7e33 ==============================
[0m20:45:30.701314 [info ] [MainThread]: Running with dbt=1.3.1
[0m20:45:30.702312 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:45:30.703314 [debug] [MainThread]: Tracking: tracking
[0m20:45:30.728308 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C7E2055D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C7E205C00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C7E204910>]}
[0m20:45:30.823509 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m20:45:30.824515 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\popular_by_category.sql
[0m20:45:30.839505 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m20:45:30.873513 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b970d1f-c1d7-47a7-a884-c52be61a7e33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C7E46CD60>]}
[0m20:45:30.880519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b970d1f-c1d7-47a7-a884-c52be61a7e33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C7E3FBEE0>]}
[0m20:45:30.881513 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m20:45:30.882514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b970d1f-c1d7-47a7-a884-c52be61a7e33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C7E3B14B0>]}
[0m20:45:30.883513 [info ] [MainThread]: 
[0m20:45:30.884513 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:45:30.885516 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m20:45:30.886516 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:45:30.886516 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:45:30.886516 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m20:45:30.886516 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m20:46:26.586850 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m20:46:26.587852 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-30376d98-68dc-4561-89e5-c581390cca12
[0m20:46:36.364321 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m20:46:36.720046 [debug] [ThreadPool]: On list_hudidb: Close
[0m20:46:36.720046 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m20:46:36.721466 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m20:46:36.722548 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:46:36.722548 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:46:36.920949 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m20:46:36.920949 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m20:46:36.920949 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-30376d98-68dc-4561-89e5-c581390cca12
[0m20:46:38.551754 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m20:46:38.812470 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m20:46:38.813461 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m20:46:38.814392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b970d1f-c1d7-47a7-a884-c52be61a7e33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C7E3D2380>]}
[0m20:46:38.814392 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m20:46:38.814392 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m20:46:38.815423 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:46:38.815423 [info ] [MainThread]: 
[0m20:46:38.821073 [debug] [Thread-1 (]: Began running node model.dbtglue.popular_by_category
[0m20:46:38.821073 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.popular_by_category .................. [RUN]
[0m20:46:38.822073 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.popular_by_category"
[0m20:46:38.822073 [debug] [Thread-1 (]: Began compiling node model.dbtglue.popular_by_category
[0m20:46:38.822073 [debug] [Thread-1 (]: Compiling model.dbtglue.popular_by_category
[0m20:46:38.827074 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.popular_by_category"
[0m20:46:38.828072 [debug] [Thread-1 (]: finished collecting timing info
[0m20:46:38.828072 [debug] [Thread-1 (]: Began executing node model.dbtglue.popular_by_category
[0m20:46:38.847071 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:46:38.847071 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m20:46:39.029770 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m20:46:39.031717 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m20:46:39.031717 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-30376d98-68dc-4561-89e5-c581390cca12
[0m20:46:40.644798 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:46:40.857661 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table popular_by_category not found.
[0m20:46:40.866718 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m20:46:40.866718 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m20:46:40.867722 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m20:46:40.867722 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:46:40.867722 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:46:40.867722 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:46:40.867722 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:46:40.868654 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m20:46:51.332272 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671742001064, 'CompletedOn': 1671742010574}, 'ResponseMetadata': {'RequestId': '68348a39-6416-41d8-8487-5da5ed2d6070', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:46:51 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '708', 'connection': 'keep-alive', 'x-amzn-requestid': '68348a39-6416-41d8-8487-5da5ed2d6070'}, 'RetryAttempts': 0}}
[0m20:46:51.333268 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m20:46:51.334285 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m20:46:51.334285 [debug] [Thread-1 (]: SQL status: OK in 10.47 seconds
[0m20:46:51.339245 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:46:51.341246 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:46:51.687404 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table popular_by_category not found.
[0m20:46:51.687404 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)








select
    *
from    source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
        
spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
        
        
[0m20:46:51.687404 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:46:51.688503 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:46:51.688503 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:46:51.688503 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)








select
    *
from    source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")

[0m20:47:17.455551 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""with source_data as (\n    SELECT\n        invoiceid,\n        category,\n        itemid,\n        referral\n    FROM hudidb.order\n\n)\n\n\n\n\n\n\n\n\nselect\n    *\nfrom    source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'popular_by_category\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'popular_by_category\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'popular_by_category\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'popular_by_category\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")\n\nspark.sql("""REFRESH TABLE hudidb.popular_by_category""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 7, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': 'An error occurred while calling o115.save.\n: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)\n\tat org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)\n\tat org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)\n\tat org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)\n\tat org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:77)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 60 more\n', 'Traceback': ['Traceback (most recent call last):\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1109, in save\n    self._jwrite.save(path)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', 'py4j.protocol.Py4JJavaError: An error occurred while calling o115.save.\n: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)\n\tat org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)\n\tat org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)\n\tat org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)\n\tat org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:77)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 60 more\n\n']}, 'Progress': 1.0, 'StartedOn': 1671742011810, 'CompletedOn': 1671742036334}, 'ResponseMetadata': {'RequestId': '016bbc41-f53b-46ae-916d-5886b57960eb', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:47:17 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '14319', 'connection': 'keep-alive', 'x-amzn-requestid': '016bbc41-f53b-46ae-916d-5886b57960eb'}, 'RetryAttempts': 0}}
[0m20:47:17.455551 [debug] [Thread-1 (]: Glue adapter: status = error
[0m20:47:17.456552 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)








select
    *
from    source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
, Py4JJavaError: An error occurred while calling o115.save.
: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)
	at org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)
	at org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)
	at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)
	at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)
	at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)
	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 60 more

[0m20:47:17.460556 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order

)








select
    *
from    source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
, Py4JJavaError: An error occurred while calling o115.save.
: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)
	at org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)
	at org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)
	at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)
	at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)
	at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)
	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 60 more

[0m20:47:17.461555 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""with source_data as (
      SELECT
          invoiceid,
          category,
          itemid,
          referral
      FROM hudidb.order
  
  )
  
  
  
  
  
  
  
  
  select
      *
  from    source_data""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if None is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
  
  spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
  SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
  , Py4JJavaError: An error occurred while calling o115.save.
  : java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem
  	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)
  	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)
  	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
  	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
  	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
  	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
  	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)
  	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)
  	at org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)
  	at org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)
  	at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)
  	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)
  	at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)
  	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)
  	at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)
  	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)
  	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)
  	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
  	at org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)
  	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)
  	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)
  	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
  	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
  	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
  	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
  	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
  	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
  	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
  	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  Caused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem
  	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
  	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
  	... 60 more
  
[0m20:47:17.468549 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.popular_by_category"
[0m20:47:17.469551 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m20:47:17.469551 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 
[0m20:47:17.469551 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:47:17.469551 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:47:17.470548 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:47:17.470548 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:47:17.470548 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 ''')
[0m20:47:18.855883 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\nselect * from hudidb.popular_by_category limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 8, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: hudidb.popular_by_category; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671742037597, 'CompletedOn': 1671742038009}, 'ResponseMetadata': {'RequestId': 'c63d317b-3414-405b-a7b9-7c831aa63291', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:47:18 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1458', 'connection': 'keep-alive', 'x-amzn-requestid': 'c63d317b-3414-405b-a7b9-7c831aa63291'}, 'RetryAttempts': 0}}
[0m20:47:18.856984 [debug] [Thread-1 (]: Glue adapter: status = error
[0m20:47:18.856984 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false

[0m20:47:18.859940 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false

[0m20:47:18.860945 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 
[0m20:47:18.861948 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: ROLLBACK
[0m20:47:18.861948 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m20:47:18.862939 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: Close
[0m20:47:18.863937 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m20:47:18.863937 [debug] [Thread-1 (]: finished collecting timing info
[0m20:47:18.865941 [debug] [Thread-1 (]: Database Error in model popular_by_category (models\metrics\popular_by_category.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
  select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [hudidb, popular_by_category], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m20:47:18.865941 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b970d1f-c1d7-47a7-a884-c52be61a7e33', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C7EFBFA60>]}
[0m20:47:18.866957 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model hudidb.popular_by_category ......... [[31mERROR[0m in 40.04s]
[0m20:47:18.868936 [debug] [Thread-1 (]: Finished running node model.dbtglue.popular_by_category
[0m20:47:18.869866 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:47:18.869866 [debug] [MainThread]: On master: ROLLBACK
[0m20:47:18.870867 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:47:18.870867 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m20:47:19.061027 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m20:47:19.061027 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m20:47:19.061027 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-30376d98-68dc-4561-89e5-c581390cca12
[0m20:47:20.713530 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m20:47:20.714526 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m20:47:20.714526 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m20:47:20.714526 [debug] [MainThread]: On master: ROLLBACK
[0m20:47:20.714526 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m20:47:20.714526 [debug] [MainThread]: On master: Close
[0m20:47:20.715526 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m20:47:20.715526 [info ] [MainThread]: 
[0m20:47:20.716523 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 49.83 seconds (109.83s).
[0m20:47:20.716523 [debug] [MainThread]: Glue adapter: cleanup called
[0m20:47:20.892482 [info ] [MainThread]: 
[0m20:47:20.893710 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:47:20.894360 [info ] [MainThread]: 
[0m20:47:20.895366 [error] [MainThread]: [33mDatabase Error in model popular_by_category (models\metrics\popular_by_category.sql)[0m
[0m20:47:20.895366 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
[0m20:47:20.896365 [error] [MainThread]:   select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
[0m20:47:20.896365 [error] [MainThread]:   'GlobalLimit 1
[0m20:47:20.896365 [error] [MainThread]:   +- 'LocalLimit 1
[0m20:47:20.897365 [error] [MainThread]:      +- 'Project [*]
[0m20:47:20.897365 [error] [MainThread]:         +- 'UnresolvedRelation [hudidb, popular_by_category], [], false
[0m20:47:20.897365 [error] [MainThread]:   
[0m20:47:20.898364 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m20:47:20.899366 [info ] [MainThread]: 
[0m20:47:20.899366 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:47:20.899366 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C7E3E24D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C7F00FB50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000016C7F0386D0>]}
[0m20:47:20.900365 [debug] [MainThread]: Flushing usage events
[0m20:47:21.118097 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 20:51:37.934779 | 85562c2f-5f28-432d-90f8-32621c7d8669 ==============================
[0m20:51:37.934779 [info ] [MainThread]: Running with dbt=1.3.1
[0m20:51:37.935779 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m20:51:37.935779 [debug] [MainThread]: Tracking: tracking
[0m20:51:37.959771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A1DF94A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A1DF95DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A1DF97700>]}
[0m20:51:38.041779 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m20:51:38.042779 [debug] [MainThread]: Partial parsing: update schema file: dbtglue://models\metrics\schema.yml
[0m20:51:38.042779 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\popular_by_category.sql
[0m20:51:38.054771 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m20:51:38.069778 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'gold_category_invoice_count' in the 'models' section of file 'models\metrics\schema.yml'
[0m20:51:38.085779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '85562c2f-5f28-432d-90f8-32621c7d8669', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A1E184EB0>]}
[0m20:51:38.092779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '85562c2f-5f28-432d-90f8-32621c7d8669', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A1E153D90>]}
[0m20:51:38.092779 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m20:51:38.093778 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85562c2f-5f28-432d-90f8-32621c7d8669', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A1E141510>]}
[0m20:51:38.094778 [info ] [MainThread]: 
[0m20:51:38.095779 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:51:38.097779 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m20:51:38.097779 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:51:38.097779 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:51:38.098778 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m20:51:38.098778 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m20:52:12.381988 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m20:52:12.381988 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-e57eb215-e1e9-4ffc-a701-3d4aef675b60
[0m20:52:22.158083 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m20:52:22.676532 [debug] [ThreadPool]: On list_hudidb: Close
[0m20:52:22.676532 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m20:52:22.678524 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m20:52:22.678524 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m20:52:22.678524 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m20:52:22.870674 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m20:52:22.871687 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m20:52:22.871687 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-e57eb215-e1e9-4ffc-a701-3d4aef675b60
[0m20:52:24.518185 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m20:52:24.743698 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m20:52:24.743698 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m20:52:24.744698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '85562c2f-5f28-432d-90f8-32621c7d8669', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A1E17D990>]}
[0m20:52:24.744698 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m20:52:24.744698 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m20:52:24.745793 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m20:52:24.745793 [info ] [MainThread]: 
[0m20:52:24.750700 [debug] [Thread-1 (]: Began running node model.dbtglue.popular_by_category
[0m20:52:24.751698 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.popular_by_category .................. [RUN]
[0m20:52:24.751698 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.popular_by_category"
[0m20:52:24.752709 [debug] [Thread-1 (]: Began compiling node model.dbtglue.popular_by_category
[0m20:52:24.752709 [debug] [Thread-1 (]: Compiling model.dbtglue.popular_by_category
[0m20:52:24.755705 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.popular_by_category"
[0m20:52:24.755705 [debug] [Thread-1 (]: finished collecting timing info
[0m20:52:24.756704 [debug] [Thread-1 (]: Began executing node model.dbtglue.popular_by_category
[0m20:52:24.780710 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m20:52:24.781705 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m20:52:24.928803 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m20:52:24.928803 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m20:52:24.929810 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-e57eb215-e1e9-4ffc-a701-3d4aef675b60
[0m20:52:27.402086 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:52:27.649184 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table popular_by_category not found.
[0m20:52:27.656176 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m20:52:27.656176 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m20:52:27.656176 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m20:52:27.657176 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:52:27.657176 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:52:27.657176 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:52:27.657176 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:52:27.657176 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m20:52:42.393183 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671742347798, 'CompletedOn': 1671742360969}, 'ResponseMetadata': {'RequestId': 'bcb86f5a-40ed-4474-b712-90015995f5bd', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:52:42 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '708', 'connection': 'keep-alive', 'x-amzn-requestid': 'bcb86f5a-40ed-4474-b712-90015995f5bd'}, 'RetryAttempts': 0}}
[0m20:52:42.394179 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m20:52:42.395178 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m20:52:42.395178 [debug] [Thread-1 (]: SQL status: OK in 14.74 seconds
[0m20:52:42.401170 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:52:42.403170 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:52:42.732148 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table popular_by_category not found.
[0m20:52:42.733150 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order
)





select
    *
from    source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
        
spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
        
        
[0m20:52:42.733150 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:52:42.733150 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:52:42.733150 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:52:42.733150 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order
)





select
    *
from    source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")

[0m20:53:05.893829 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""with source_data as (\n    SELECT\n        invoiceid,\n        category,\n        itemid,\n        referral\n    FROM hudidb.order\n)\n\n\n\n\n\nselect\n    *\nfrom    source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'popular_by_category\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'popular_by_category\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'popular_by_category\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'popular_by_category\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")\n\nspark.sql("""REFRESH TABLE hudidb.popular_by_category""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 7, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': 'An error occurred while calling o115.save.\n: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)\n\tat org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)\n\tat org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)\n\tat org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)\n\tat org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:77)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 60 more\n', 'Traceback': ['Traceback (most recent call last):\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1109, in save\n    self._jwrite.save(path)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', 'py4j.protocol.Py4JJavaError: An error occurred while calling o115.save.\n: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem\n\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)\n\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)\n\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)\n\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)\n\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)\n\tat org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)\n\tat org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)\n\tat org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)\n\tat org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)\n\tat org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)\n\tat org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:77)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:418)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:351)\n\t... 60 more\n\n']}, 'Progress': 1.0, 'StartedOn': 1671742362945, 'CompletedOn': 1671742385307}, 'ResponseMetadata': {'RequestId': '95a3d594-b721-41b2-b7df-d8c4a6a09f1a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:53:05 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '14311', 'connection': 'keep-alive', 'x-amzn-requestid': '95a3d594-b721-41b2-b7df-d8c4a6a09f1a'}, 'RetryAttempts': 0}}
[0m20:53:05.894828 [debug] [Thread-1 (]: Glue adapter: status = error
[0m20:53:05.894828 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order
)





select
    *
from    source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
, Py4JJavaError: An error occurred while calling o115.save.
: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)
	at org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)
	at org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)
	at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)
	at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)
	at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)
	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 60 more

[0m20:53:05.899836 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""with source_data as (
    SELECT
        invoiceid,
        category,
        itemid,
        referral
    FROM hudidb.order
)





select
    *
from    source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")

spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
, Py4JJavaError: An error occurred while calling o115.save.
: java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)
	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)
	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)
	at org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)
	at org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)
	at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)
	at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)
	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)
	at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)
	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
	at org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)
	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem
	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 60 more

[0m20:53:05.899836 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""with source_data as (
      SELECT
          invoiceid,
          category,
          itemid,
          referral
      FROM hudidb.order
  )
  
  
  
  
  
  select
      *
  from    source_data""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if None is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'popular_by_category', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'popular_by_category', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://glue-learn-begineers/views//hudidb/popular_by_category/")
  
  spark.sql("""REFRESH TABLE hudidb.popular_by_category""")
  SqlWrapper2.execute("""SELECT * FROM hudidb.popular_by_category LIMIT 1""")
  , Py4JJavaError: An error occurred while calling o115.save.
  : java.lang.NoClassDefFoundError: org/apache/calcite/rel/type/RelDataTypeSystem
  	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get(SemanticAnalyzerFactory.java:318)
  	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:484)
  	at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317)
  	at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457)
  	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237)
  	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227)
  	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.updateHiveSQLs(HiveQueryDDLExecutor.java:94)
  	at org.apache.hudi.hive.ddl.HiveQueryDDLExecutor.runSQL(HiveQueryDDLExecutor.java:85)
  	at org.apache.hudi.hive.ddl.QueryBasedDDLExecutor.createTable(QueryBasedDDLExecutor.java:82)
  	at org.apache.hudi.hive.HoodieHiveClient.createTable(HoodieHiveClient.java:191)
  	at org.apache.hudi.hive.HiveSyncTool.syncSchema(HiveSyncTool.java:237)
  	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:182)
  	at org.apache.hudi.hive.HiveSyncTool.doSync(HiveSyncTool.java:131)
  	at org.apache.hudi.hive.HiveSyncTool.syncHoodieTable(HiveSyncTool.java:117)
  	at org.apache.hudi.HoodieSparkSqlWriter$.syncHive(HoodieSparkSqlWriter.scala:520)
  	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2(HoodieSparkSqlWriter.scala:577)
  	at org.apache.hudi.HoodieSparkSqlWriter$.$anonfun$metaSync$2$adapted(HoodieSparkSqlWriter.scala:573)
  	at scala.collection.mutable.HashSet.foreach(HashSet.scala:77)
  	at org.apache.hudi.HoodieSparkSqlWriter$.metaSync(HoodieSparkSqlWriter.scala:573)
  	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:474)
  	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:152)
  	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
  	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
  	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
  	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
  	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
  	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
  	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
  	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  Caused by: java.lang.ClassNotFoundException: org.apache.calcite.rel.type.RelDataTypeSystem
  	at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
  	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
  	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
  	... 60 more
  
[0m20:53:05.907836 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.popular_by_category"
[0m20:53:05.908829 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.popular_by_category"
[0m20:53:05.908829 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 
[0m20:53:05.908829 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m20:53:05.909828 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m20:53:05.909828 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m20:53:05.909828 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m20:53:05.909828 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 ''')
[0m20:53:07.311970 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */\nselect * from hudidb.popular_by_category limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 8, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: hudidb.popular_by_category; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671742386056, 'CompletedOn': 1671742386415}, 'ResponseMetadata': {'RequestId': '7c34d107-0640-442e-8e45-e5bbba5c4ab2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 20:53:07 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1458', 'connection': 'keep-alive', 'x-amzn-requestid': '7c34d107-0640-442e-8e45-e5bbba5c4ab2'}, 'RetryAttempts': 0}}
[0m20:53:07.312961 [debug] [Thread-1 (]: Glue adapter: status = error
[0m20:53:07.312961 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false

[0m20:53:07.315969 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, popular_by_category], [], false

[0m20:53:07.315969 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
select * from hudidb.popular_by_category limit 1 
[0m20:53:07.316963 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: ROLLBACK
[0m20:53:07.317911 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m20:53:07.317911 [debug] [Thread-1 (]: On model.dbtglue.popular_by_category: Close
[0m20:53:07.317911 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m20:53:07.318910 [debug] [Thread-1 (]: finished collecting timing info
[0m20:53:07.319897 [debug] [Thread-1 (]: Database Error in model popular_by_category (models\metrics\popular_by_category.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
  select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [hudidb, popular_by_category], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m20:53:07.319897 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '85562c2f-5f28-432d-90f8-32621c7d8669', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A1EC804F0>]}
[0m20:53:07.319897 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model hudidb.popular_by_category ......... [[31mERROR[0m in 42.57s]
[0m20:53:07.320897 [debug] [Thread-1 (]: Finished running node model.dbtglue.popular_by_category
[0m20:53:07.322897 [debug] [MainThread]: Acquiring new glue connection "master"
[0m20:53:07.322897 [debug] [MainThread]: On master: ROLLBACK
[0m20:53:07.322897 [debug] [MainThread]: Opening a new connection, currently in state init
[0m20:53:07.322897 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m20:53:08.073730 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m20:53:08.074731 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m20:53:08.074731 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-e57eb215-e1e9-4ffc-a701-3d4aef675b60
[0m20:53:09.726421 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m20:53:09.727422 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m20:53:09.728413 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m20:53:09.728413 [debug] [MainThread]: On master: ROLLBACK
[0m20:53:09.729416 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m20:53:09.730346 [debug] [MainThread]: On master: Close
[0m20:53:09.730346 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m20:53:09.731436 [info ] [MainThread]: 
[0m20:53:09.732401 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 31.64 seconds (91.64s).
[0m20:53:09.733348 [debug] [MainThread]: Glue adapter: cleanup called
[0m20:53:09.951285 [info ] [MainThread]: 
[0m20:53:09.952276 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m20:53:09.953203 [info ] [MainThread]: 
[0m20:53:09.953203 [error] [MainThread]: [33mDatabase Error in model popular_by_category (models\metrics\popular_by_category.sql)[0m
[0m20:53:09.954201 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.popular_by_category"} */
[0m20:53:09.954201 [error] [MainThread]:   select * from hudidb.popular_by_category limit 1 '''), AnalysisException: Table or view not found: hudidb.popular_by_category; line 2 pos 14;
[0m20:53:09.955200 [error] [MainThread]:   'GlobalLimit 1
[0m20:53:09.955200 [error] [MainThread]:   +- 'LocalLimit 1
[0m20:53:09.955200 [error] [MainThread]:      +- 'Project [*]
[0m20:53:09.956200 [error] [MainThread]:         +- 'UnresolvedRelation [hudidb, popular_by_category], [], false
[0m20:53:09.956200 [error] [MainThread]:   
[0m20:53:09.956200 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\popular_by_category.sql
[0m20:53:09.957200 [info ] [MainThread]: 
[0m20:53:09.957200 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m20:53:09.957200 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A1ED40730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A1ED40790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023A1ED8F730>]}
[0m20:53:09.958200 [debug] [MainThread]: Flushing usage events
[0m20:53:10.211150 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 21:00:46.098322 | 58ce827d-7aa5-4370-9b74-f70655c6675d ==============================
[0m21:00:46.098322 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:00:46.099606 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:00:46.099606 [debug] [MainThread]: Tracking: tracking
[0m21:00:46.119320 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000274E9A54970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000274E9A54490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000274E9A577F0>]}
[0m21:00:46.145387 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m21:00:46.146335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '58ce827d-7aa5-4370-9b74-f70655c6675d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000274E94D4EB0>]}
[0m21:00:46.196320 [debug] [MainThread]: Parsing macros\adapters.sql
[0m21:00:46.218322 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m21:00:46.220325 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m21:00:46.221325 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m21:00:46.225322 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m21:00:46.239322 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m21:00:46.240336 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m21:00:46.248322 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m21:00:46.251323 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m21:00:46.254322 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m21:00:46.255325 [debug] [MainThread]: Parsing macros\adapters.sql
[0m21:00:46.288323 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m21:00:46.291324 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m21:00:46.298323 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m21:00:46.318322 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m21:00:46.323323 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m21:00:46.324326 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m21:00:46.326324 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m21:00:46.332328 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m21:00:46.338323 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m21:00:46.343322 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m21:00:46.343322 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m21:00:46.344332 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m21:00:46.345328 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m21:00:46.345328 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m21:00:46.346325 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m21:00:46.347325 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m21:00:46.347325 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m21:00:46.352323 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m21:00:46.361322 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m21:00:46.363326 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m21:00:46.364326 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m21:00:46.365325 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m21:00:46.375323 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m21:00:46.383323 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m21:00:46.384325 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m21:00:46.387325 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m21:00:46.392324 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m21:00:46.396324 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m21:00:46.407323 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m21:00:46.409325 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m21:00:46.412325 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m21:00:46.418324 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m21:00:46.423324 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m21:00:46.424326 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m21:00:46.425325 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m21:00:46.426325 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m21:00:46.426325 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m21:00:46.427328 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m21:00:46.429326 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m21:00:46.431325 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m21:00:46.433326 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m21:00:46.436323 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m21:00:46.442324 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m21:00:46.450412 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m21:00:46.451407 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m21:00:46.461379 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m21:00:46.471327 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m21:00:46.476327 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m21:00:46.478327 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m21:00:46.482321 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m21:00:46.484321 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m21:00:46.486322 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m21:00:46.486322 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m21:00:46.490321 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m21:00:46.502320 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m21:00:46.507319 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m21:00:46.517320 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m21:00:46.526321 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m21:00:46.528321 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m21:00:46.541320 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m21:00:46.543319 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m21:00:46.547382 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m21:00:46.548390 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m21:00:46.552378 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m21:00:46.553391 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m21:00:46.554393 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m21:00:46.555379 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m21:00:46.556379 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m21:00:46.557377 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m21:00:46.558321 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m21:00:46.558321 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m21:00:46.563320 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m21:00:46.563320 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m21:00:46.564321 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m21:00:46.565320 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m21:00:46.566321 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m21:00:46.567321 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m21:00:46.567321 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m21:00:46.568321 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m21:00:46.569321 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m21:00:46.570320 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m21:00:46.572321 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m21:00:46.573321 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m21:00:46.574321 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m21:00:46.575321 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m21:00:46.576321 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m21:00:46.577321 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m21:00:46.578321 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m21:00:46.865330 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m21:00:46.896319 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'gold_category_invoice_count' in the 'models' section of file 'models\metrics\schema.yml'
[0m21:00:46.920399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '58ce827d-7aa5-4370-9b74-f70655c6675d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000274E69D7790>]}
[0m21:00:46.926410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '58ce827d-7aa5-4370-9b74-f70655c6675d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000274E9AD5540>]}
[0m21:00:46.926410 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:00:46.927325 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '58ce827d-7aa5-4370-9b74-f70655c6675d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000274E6B73AF0>]}
[0m21:00:46.928321 [info ] [MainThread]: 
[0m21:00:46.929321 [debug] [MainThread]: Acquiring new glue connection "master"
[0m21:00:46.930351 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m21:00:46.930351 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:00:46.930351 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m21:00:46.931380 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m21:00:46.931380 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m21:05:48.644285 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : GlueSession took more than 300 seconds to start
[0m21:05:48.645290 [debug] [ThreadPool]: On list_hudidb: No close available on handle
[0m21:05:48.647286 [debug] [MainThread]: Glue adapter: cleanup called
[0m21:05:48.647286 [debug] [MainThread]: Flushing usage events
[0m21:05:48.861278 [debug] [MainThread]: Glue adapter: cleanup called
[0m21:05:48.861278 [info ] [MainThread]: ctrl-c


============================== 2022-12-22 21:11:35.644741 | 6e0683cc-1f23-4b0f-96b3-45ff7b9d3896 ==============================
[0m21:11:35.644741 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:11:35.645743 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:11:35.646741 [debug] [MainThread]: Tracking: tracking
[0m21:11:35.667740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DC1F75630>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DC1F75900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DC1F77670>]}
[0m21:11:35.706033 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m21:11:35.707022 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6e0683cc-1f23-4b0f-96b3-45ff7b9d3896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DC19FA020>]}
[0m21:11:35.761544 [debug] [MainThread]: Parsing macros\adapters.sql
[0m21:11:35.783544 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m21:11:35.784545 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m21:11:35.785537 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m21:11:35.789544 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m21:11:35.803545 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m21:11:35.804545 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m21:11:35.812545 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m21:11:35.815545 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m21:11:35.818539 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m21:11:35.820546 [debug] [MainThread]: Parsing macros\adapters.sql
[0m21:11:35.852546 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m21:11:35.855546 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m21:11:35.862545 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m21:11:35.882540 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m21:11:35.888538 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m21:11:35.889540 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m21:11:35.892545 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m21:11:35.899546 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m21:11:35.906545 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m21:11:35.909545 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m21:11:35.910538 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m21:11:35.910538 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m21:11:35.911537 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m21:11:35.911537 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m21:11:35.912537 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m21:11:35.913537 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m21:11:35.913537 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m21:11:35.917545 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m21:11:35.925545 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m21:11:35.926537 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m21:11:35.927545 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m21:11:35.928545 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m21:11:35.937545 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m21:11:35.943545 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m21:11:35.945545 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m21:11:35.947545 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m21:11:35.952545 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m21:11:35.955545 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m21:11:35.965545 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m21:11:35.967545 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m21:11:35.969545 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m21:11:35.975545 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m21:11:35.979545 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m21:11:35.980544 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m21:11:35.981545 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m21:11:35.982537 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m21:11:35.982537 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m21:11:35.984545 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m21:11:35.985545 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m21:11:35.987544 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m21:11:35.989546 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m21:11:35.992545 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m21:11:35.998539 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m21:11:36.004538 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m21:11:36.005542 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m21:11:36.015540 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m21:11:36.026537 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m21:11:36.030537 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m21:11:36.033537 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m21:11:36.036537 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m21:11:36.039537 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m21:11:36.041569 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m21:11:36.042608 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m21:11:36.045605 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m21:11:36.056610 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m21:11:36.061601 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m21:11:36.069607 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m21:11:36.077614 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m21:11:36.078564 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m21:11:36.090545 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m21:11:36.092544 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m21:11:36.095544 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m21:11:36.096537 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m21:11:36.101545 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m21:11:36.102546 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m21:11:36.103545 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m21:11:36.104545 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m21:11:36.105548 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m21:11:36.106538 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m21:11:36.107541 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m21:11:36.108551 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m21:11:36.113544 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m21:11:36.114547 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m21:11:36.115546 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m21:11:36.116545 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m21:11:36.117545 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m21:11:36.117545 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m21:11:36.118546 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m21:11:36.119546 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m21:11:36.121549 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m21:11:36.121549 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m21:11:36.123546 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m21:11:36.124547 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m21:11:36.125537 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m21:11:36.126537 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m21:11:36.127537 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m21:11:36.128537 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m21:11:36.130544 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m21:11:36.412548 [debug] [MainThread]: 1699: static parser successfully parsed metrics\popular_by_category.sql
[0m21:11:36.443537 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'gold_category_invoice_count' in the 'models' section of file 'models\metrics\schema.yml'
[0m21:11:36.465596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6e0683cc-1f23-4b0f-96b3-45ff7b9d3896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DBEE87700>]}
[0m21:11:36.471576 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6e0683cc-1f23-4b0f-96b3-45ff7b9d3896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DBEF61450>]}
[0m21:11:36.471576 [info ] [MainThread]: Found 1 model, 0 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:11:36.472538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6e0683cc-1f23-4b0f-96b3-45ff7b9d3896', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DBEE867A0>]}
[0m21:11:36.473538 [info ] [MainThread]: 
[0m21:11:36.474544 [debug] [MainThread]: Acquiring new glue connection "master"
[0m21:11:36.475545 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m21:11:36.475545 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:11:36.476537 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m21:11:36.476537 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m21:11:36.476537 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m21:16:38.124138 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : GlueSession took more than 300 seconds to start
[0m21:16:38.125210 [debug] [ThreadPool]: On list_hudidb: No close available on handle
[0m21:16:38.126203 [debug] [MainThread]: Glue adapter: cleanup called
[0m21:16:38.126203 [error] [MainThread]: Encountered an error:
Database Error
  Got an error when attempting to open a GlueSessions: GlueSession took more than 300 seconds to start
[0m21:16:38.127140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DC2173850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DC2172E60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000010DC2171A20>]}
[0m21:16:38.128146 [debug] [MainThread]: Flushing usage events
[0m21:16:38.315973 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 21:21:43.964681 | be6e55ba-df4f-4718-a72a-8dc975a9c8fe ==============================
[0m21:21:43.964681 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:21:43.965683 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:21:43.965683 [debug] [MainThread]: Tracking: tracking
[0m21:21:43.989682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAD80F4A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAD80F5DE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAD80F7700>]}
[0m21:21:44.014775 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m21:21:44.015681 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'be6e55ba-df4f-4718-a72a-8dc975a9c8fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAD7B69F60>]}
[0m21:21:44.064687 [debug] [MainThread]: Parsing macros\adapters.sql
[0m21:21:44.086681 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m21:21:44.087681 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m21:21:44.088681 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m21:21:44.092683 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m21:21:44.106683 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m21:21:44.106683 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m21:21:44.114684 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m21:21:44.117681 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m21:21:44.119682 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m21:21:44.120683 [debug] [MainThread]: Parsing macros\adapters.sql
[0m21:21:44.151681 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m21:21:44.153681 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m21:21:44.161684 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m21:21:44.179684 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m21:21:44.184686 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m21:21:44.185682 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m21:21:44.186681 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m21:21:44.192681 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m21:21:44.199682 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m21:21:44.203681 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m21:21:44.203681 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m21:21:44.204685 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m21:21:44.204685 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m21:21:44.205681 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m21:21:44.206681 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m21:21:44.207681 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m21:21:44.207681 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m21:21:44.211681 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m21:21:44.221681 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m21:21:44.222681 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m21:21:44.224681 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m21:21:44.224681 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m21:21:44.234681 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m21:21:44.242681 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m21:21:44.243681 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m21:21:44.246682 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m21:21:44.251682 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m21:21:44.255680 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m21:21:44.266680 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m21:21:44.268681 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m21:21:44.271681 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m21:21:44.277681 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m21:21:44.282686 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m21:21:44.283681 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m21:21:44.284681 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m21:21:44.285682 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m21:21:44.285682 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m21:21:44.287681 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m21:21:44.288681 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m21:21:44.290681 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m21:21:44.292682 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m21:21:44.295682 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m21:21:44.302681 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m21:21:44.311681 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m21:21:44.313681 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m21:21:44.326681 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m21:21:44.344679 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m21:21:44.349679 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m21:21:44.352682 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m21:21:44.357681 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m21:21:44.360687 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m21:21:44.361690 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m21:21:44.363680 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m21:21:44.366687 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m21:21:44.378681 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m21:21:44.383680 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m21:21:44.392679 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m21:21:44.401683 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m21:21:44.402683 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m21:21:44.415930 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m21:21:44.416929 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m21:21:44.420932 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m21:21:44.421930 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m21:21:44.425936 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m21:21:44.426929 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m21:21:44.427928 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m21:21:44.428928 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m21:21:44.429928 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m21:21:44.430929 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m21:21:44.431929 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m21:21:44.432929 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m21:21:44.436939 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m21:21:44.438930 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m21:21:44.439930 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m21:21:44.439930 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m21:21:44.440929 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m21:21:44.441929 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m21:21:44.442929 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m21:21:44.443929 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m21:21:44.444929 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m21:21:44.445929 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m21:21:44.447930 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m21:21:44.448929 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m21:21:44.449933 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m21:21:44.450929 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m21:21:44.451930 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m21:21:44.452930 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m21:21:44.454929 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m21:21:44.755172 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_insert_overwrite_table.sql
[0m21:21:44.764173 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_insert_table.sql
[0m21:21:44.767173 [debug] [MainThread]: 1603: static parser failed on metrics\hudi_upsert_partitioned_cow_table.sql
[0m21:21:44.771173 [debug] [MainThread]: 1602: parser fallback to jinja rendering on metrics\hudi_upsert_partitioned_cow_table.sql
[0m21:21:44.772172 [debug] [MainThread]: 1603: static parser failed on metrics\hudi_upsert_partitioned_mor_table.sql
[0m21:21:44.776178 [debug] [MainThread]: 1602: parser fallback to jinja rendering on metrics\hudi_upsert_partitioned_mor_table.sql
[0m21:21:44.778198 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_upsert_table.sql
[0m21:21:44.802177 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'hudi_upsert_paritioned_cow_table' in the 'models' section of file 'models\metrics\schema.yml'
[0m21:21:44.806241 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'hudi_upsert_paritioned_mor_table' in the 'models' section of file 'models\metrics\schema.yml'
[0m21:21:44.846178 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbtglue.unique_hudi_upsert_paritioned_cow_table_id.4d181cf753' (models\metrics\schema.yml) depends on a node named 'hudi_upsert_paritioned_cow_table' which was not found
[0m21:21:44.848175 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbtglue.not_null_hudi_upsert_paritioned_cow_table_id.ada239239a' (models\metrics\schema.yml) depends on a node named 'hudi_upsert_paritioned_cow_table' which was not found
[0m21:21:44.848175 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbtglue.not_null_hudi_upsert_paritioned_cow_table_name.88e08fdc42' (models\metrics\schema.yml) depends on a node named 'hudi_upsert_paritioned_cow_table' which was not found
[0m21:21:44.849176 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbtglue.not_null_hudi_upsert_paritioned_cow_table_ts.a7ad8bc4b0' (models\metrics\schema.yml) depends on a node named 'hudi_upsert_paritioned_cow_table' which was not found
[0m21:21:44.849176 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbtglue.not_null_hudi_upsert_paritioned_cow_table_datestr.38df5c2fa5' (models\metrics\schema.yml) depends on a node named 'hudi_upsert_paritioned_cow_table' which was not found
[0m21:21:44.850173 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbtglue.unique_hudi_upsert_paritioned_mor_table_id.2275cc547d' (models\metrics\schema.yml) depends on a node named 'hudi_upsert_paritioned_mor_table' which was not found
[0m21:21:44.850173 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbtglue.not_null_hudi_upsert_paritioned_mor_table_id.8024bdb8e1' (models\metrics\schema.yml) depends on a node named 'hudi_upsert_paritioned_mor_table' which was not found
[0m21:21:44.851172 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbtglue.not_null_hudi_upsert_paritioned_mor_table_name.b0e50be053' (models\metrics\schema.yml) depends on a node named 'hudi_upsert_paritioned_mor_table' which was not found
[0m21:21:44.851172 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbtglue.not_null_hudi_upsert_paritioned_mor_table_ts.34e0b53e13' (models\metrics\schema.yml) depends on a node named 'hudi_upsert_paritioned_mor_table' which was not found
[0m21:21:44.852174 [warn ] [MainThread]: [[33mWARNING[0m]: Test 'test.dbtglue.not_null_hudi_upsert_paritioned_mor_table_datestr.67138e54ab' (models\metrics\schema.yml) depends on a node named 'hudi_upsert_paritioned_mor_table' which was not found
[0m21:21:44.884172 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'be6e55ba-df4f-4718-a72a-8dc975a9c8fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAD82DE9B0>]}
[0m21:21:44.895173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'be6e55ba-df4f-4718-a72a-8dc975a9c8fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAD82DEB90>]}
[0m21:21:44.895173 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m21:21:44.896173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'be6e55ba-df4f-4718-a72a-8dc975a9c8fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAD50BFB50>]}
[0m21:21:44.898172 [info ] [MainThread]: 
[0m21:21:44.899174 [debug] [MainThread]: Acquiring new glue connection "master"
[0m21:21:44.900173 [debug] [ThreadPool]: Acquiring new glue connection "list_sampledb"
[0m21:21:44.901173 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:21:44.901173 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m21:21:44.902175 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m21:21:44.902175 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m21:26:46.012207 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : GlueSession took more than 300 seconds to start
[0m21:26:46.013278 [debug] [ThreadPool]: On list_sampledb: No close available on handle
[0m21:26:46.014306 [debug] [MainThread]: Glue adapter: cleanup called
[0m21:26:46.014306 [error] [MainThread]: Encountered an error:
Database Error
  Got an error when attempting to open a GlueSessions: GlueSession took more than 300 seconds to start
[0m21:26:46.015298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAD80F7EB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAD80F60E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BAD80F5D20>]}
[0m21:26:46.015298 [debug] [MainThread]: Flushing usage events
[0m21:26:46.202539 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 21:27:52.191808 | 193a3f1e-f466-43bb-8304-f47beed75f9a ==============================
[0m21:27:52.191808 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:27:52.192808 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:27:52.193814 [debug] [MainThread]: Tracking: tracking
[0m21:27:52.214810 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1EACC4970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1EACC4490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1EACC77F0>]}
[0m21:27:52.294808 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:27:52.294808 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:27:52.300812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '193a3f1e-f466-43bb-8304-f47beed75f9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1EAF0D660>]}
[0m21:27:52.308800 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '193a3f1e-f466-43bb-8304-f47beed75f9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1EAED5330>]}
[0m21:27:52.308800 [info ] [MainThread]: Found 5 models, 10 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m21:27:52.309801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '193a3f1e-f466-43bb-8304-f47beed75f9a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1EAED4F70>]}
[0m21:27:52.311808 [info ] [MainThread]: 
[0m21:27:52.311808 [debug] [MainThread]: Acquiring new glue connection "master"
[0m21:27:52.313811 [debug] [ThreadPool]: Acquiring new glue connection "list_sampledb"
[0m21:27:52.313811 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:27:52.314800 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m21:27:52.314800 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m21:27:52.314800 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called


============================== 2022-12-22 21:29:20.197993 | eb7ecf9f-2181-4cf2-a8a7-94cd295e178b ==============================
[0m21:29:20.197993 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:29:20.198993 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:29:20.199992 [debug] [MainThread]: Tracking: tracking
[0m21:29:20.222993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020118BA4970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020118BA4490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020118BA77F0>]}
[0m21:29:20.308993 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:29:20.309994 [debug] [MainThread]: Partial parsing: update schema file: dbtglue://models\metrics\schema.yml
[0m21:29:20.324995 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_insert_overwrite_table.sql
[0m21:29:20.335994 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_upsert_table.sql
[0m21:29:20.359059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eb7ecf9f-2181-4cf2-a8a7-94cd295e178b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020118E48160>]}
[0m21:29:20.370000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eb7ecf9f-2181-4cf2-a8a7-94cd295e178b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020118DB5570>]}
[0m21:29:20.370999 [info ] [MainThread]: Found 5 models, 2 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m21:29:20.371998 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb7ecf9f-2181-4cf2-a8a7-94cd295e178b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020118DB5420>]}
[0m21:29:20.372993 [info ] [MainThread]: 
[0m21:29:20.373994 [debug] [MainThread]: Acquiring new glue connection "master"
[0m21:29:20.375993 [debug] [ThreadPool]: Acquiring new glue connection "list_sampledb"
[0m21:29:20.375993 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:29:20.375993 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m21:29:20.376997 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m21:29:20.376997 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m21:34:21.178492 [error] [ThreadPool]: Glue adapter: Got an error when attempting to open a GlueSession : GlueSession took more than 300 seconds to start
[0m21:34:21.179466 [debug] [ThreadPool]: On list_sampledb: No close available on handle
[0m21:34:21.180475 [debug] [MainThread]: Glue adapter: cleanup called
[0m21:34:21.180475 [error] [MainThread]: Encountered an error:
Database Error
  Got an error when attempting to open a GlueSessions: GlueSession took more than 300 seconds to start
[0m21:34:21.181465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020118BC7250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020118BC5AB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020118BC54B0>]}
[0m21:34:21.181465 [debug] [MainThread]: Flushing usage events
[0m21:34:21.341311 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 21:37:38.119334 | d1c6bdef-bf8c-41c4-b84e-03e75f282b94 ==============================
[0m21:37:38.119334 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:37:38.120336 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\IdeaProjects\\DBT\\lab2\\dbtgluen\\profile', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:37:38.120336 [debug] [MainThread]: Tracking: tracking
[0m21:37:38.142804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220ACEC49D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220ACEC6B90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220ACEC7790>]}
[0m21:37:38.169795 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m21:37:38.170841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd1c6bdef-bf8c-41c4-b84e-03e75f282b94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220AC949F60>]}
[0m21:37:38.221852 [debug] [MainThread]: Parsing macros\adapters.sql
[0m21:37:38.243864 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m21:37:38.244863 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m21:37:38.245797 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m21:37:38.249863 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m21:37:38.262868 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m21:37:38.263797 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m21:37:38.271802 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m21:37:38.274802 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m21:37:38.277802 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m21:37:38.278803 [debug] [MainThread]: Parsing macros\adapters.sql
[0m21:37:38.306797 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m21:37:38.308798 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m21:37:38.314797 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m21:37:38.331796 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m21:37:38.335796 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m21:37:38.336799 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m21:37:38.337799 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m21:37:38.342796 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m21:37:38.348866 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m21:37:38.351852 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m21:37:38.352856 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m21:37:38.352856 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m21:37:38.353853 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m21:37:38.353853 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m21:37:38.354840 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m21:37:38.355797 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m21:37:38.355797 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m21:37:38.359854 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m21:37:38.367802 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m21:37:38.368803 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m21:37:38.369795 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m21:37:38.370794 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m21:37:38.379802 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m21:37:38.385802 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m21:37:38.387802 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m21:37:38.389807 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m21:37:38.394806 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m21:37:38.398802 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m21:37:38.408802 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m21:37:38.410802 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m21:37:38.413802 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m21:37:38.419802 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m21:37:38.423802 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m21:37:38.424795 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m21:37:38.425794 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m21:37:38.426794 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m21:37:38.427794 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m21:37:38.428794 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m21:37:38.429794 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m21:37:38.431794 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m21:37:38.433794 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m21:37:38.436795 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m21:37:38.442859 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m21:37:38.450862 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m21:37:38.451871 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m21:37:38.462875 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m21:37:38.474863 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m21:37:38.480860 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m21:37:38.482862 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m21:37:38.487802 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m21:37:38.489805 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m21:37:38.491804 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m21:37:38.493803 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m21:37:38.497802 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m21:37:38.510802 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m21:37:38.515802 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m21:37:38.524802 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m21:37:38.533802 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m21:37:38.534803 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m21:37:38.546804 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m21:37:38.548804 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m21:37:38.551857 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m21:37:38.553869 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m21:37:38.557859 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m21:37:38.558807 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m21:37:38.559838 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m21:37:38.560796 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m21:37:38.562868 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m21:37:38.563861 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m21:37:38.564859 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m21:37:38.564859 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m21:37:38.570795 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m21:37:38.571797 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m21:37:38.572844 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m21:37:38.573873 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m21:37:38.574865 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m21:37:38.575816 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m21:37:38.576803 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m21:37:38.576803 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m21:37:38.578803 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m21:37:38.579807 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m21:37:38.581802 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m21:37:38.582803 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m21:37:38.583795 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m21:37:38.584796 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m21:37:38.585796 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m21:37:38.586796 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m21:37:38.588795 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m21:37:38.876795 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_insert_overwrite_table.sql
[0m21:37:38.885802 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_insert_table.sql
[0m21:37:38.887802 [debug] [MainThread]: 1603: static parser failed on metrics\hudi_upsert_partitioned_cow_table.sql
[0m21:37:38.891802 [debug] [MainThread]: 1602: parser fallback to jinja rendering on metrics\hudi_upsert_partitioned_cow_table.sql
[0m21:37:38.892805 [debug] [MainThread]: 1603: static parser failed on metrics\hudi_upsert_partitioned_mor_table.sql
[0m21:37:38.896802 [debug] [MainThread]: 1602: parser fallback to jinja rendering on metrics\hudi_upsert_partitioned_mor_table.sql
[0m21:37:38.898802 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_upsert_table.sql
[0m21:37:38.955802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd1c6bdef-bf8c-41c4-b84e-03e75f282b94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220AD0C1B10>]}
[0m21:37:38.962802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd1c6bdef-bf8c-41c4-b84e-03e75f282b94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220AD0C0A90>]}
[0m21:37:38.963795 [info ] [MainThread]: Found 5 models, 2 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m21:37:38.963795 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd1c6bdef-bf8c-41c4-b84e-03e75f282b94', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000220ACEDC130>]}
[0m21:37:38.964795 [info ] [MainThread]: 
[0m21:37:38.965799 [debug] [MainThread]: Acquiring new glue connection "master"
[0m21:37:38.967804 [debug] [ThreadPool]: Acquiring new glue connection "list_sampledb"
[0m21:37:38.967804 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:37:38.968795 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m21:37:38.968795 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m21:37:38.968795 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called


============================== 2022-12-22 21:42:43.793733 | ad3ff270-9fef-487d-a518-fec1ee9ca7db ==============================
[0m21:42:43.793733 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:42:43.794725 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'clean', 'indirect_selection': 'eager'}
[0m21:42:43.794725 [debug] [MainThread]: Tracking: tracking
[0m21:42:43.816733 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA4C1CB2E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA4C254B20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA4C255300>]}
[0m21:42:43.821725 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA4C254B20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA4C256C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DA4C256D10>]}
[0m21:42:43.822725 [debug] [MainThread]: Flushing usage events


============================== 2022-12-22 21:43:52.553463 | 657dedf0-3819-4dc1-8e44-4dc3bdfef5c7 ==============================
[0m21:43:52.553463 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:43:52.553463 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m21:43:52.553463 [debug] [MainThread]: Tracking: tracking
[0m21:43:52.574463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D34569210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D3456A650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D34569FF0>]}
[0m21:43:52.591464 [debug] [MainThread]: Executing "git --help"
[0m21:43:52.646530 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m21:43:52.646530 [debug] [MainThread]: STDERR: "b''"
[0m21:43:52.648463 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D3451B250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D345A5540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020D345A6B00>]}
[0m21:43:52.648463 [debug] [MainThread]: Flushing usage events


============================== 2022-12-22 21:45:08.919280 | 9419916e-20a3-4fc1-84d8-8e07ca930525 ==============================
[0m21:45:08.919280 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:45:08.920280 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m21:45:08.920280 [debug] [MainThread]: Tracking: tracking
[0m21:45:08.941281 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8FC3F9D50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8FC3F9180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C8FC3F9FF0>]}
[0m21:45:09.079280 [debug] [MainThread]: Executing "git --help"
[0m21:45:09.136273 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m21:45:09.136273 [debug] [MainThread]: STDERR: "b''"
[0m21:45:09.140271 [debug] [MainThread]: Acquiring new glue connection "debug"
[0m21:45:09.140271 [debug] [MainThread]: Using glue connection "debug"
[0m21:45:09.140271 [debug] [MainThread]: On debug: select 1 as id
[0m21:45:09.141281 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:45:09.141281 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m21:45:09.141281 [debug] [MainThread]: Glue adapter: No session present, starting one
[0m21:45:09.141281 [debug] [MainThread]: Glue adapter: GlueConnection _start_session called
[0m21:47:50.538583 [debug] [MainThread]: Flushing usage events
[0m21:47:50.702213 [debug] [MainThread]: Glue adapter: cleanup called
[0m21:47:50.703213 [info ] [MainThread]: ctrl-c


============================== 2022-12-22 21:48:40.064537 | 611d94aa-29de-4021-849c-1cd59ad8e94b ==============================
[0m21:48:40.064537 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:48:40.065535 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'clean', 'indirect_selection': 'eager'}
[0m21:48:40.065535 [debug] [MainThread]: Tracking: tracking
[0m21:48:40.086462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225F99963B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225F99593F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225F995B340>]}
[0m21:48:40.088464 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225F995B670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225F995A350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000225F9958C40>]}
[0m21:48:40.089458 [debug] [MainThread]: Flushing usage events


============================== 2022-12-22 21:49:14.104825 | 024979af-7cd1-4b81-aea0-bcd7514d009f ==============================
[0m21:49:14.105827 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:49:14.105827 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m21:49:14.105827 [debug] [MainThread]: Tracking: tracking
[0m21:49:14.127825 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017363B59930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017363B592D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017363B5A770>]}
[0m21:49:14.281737 [debug] [MainThread]: Executing "git --help"
[0m21:49:14.338738 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m21:49:14.339735 [debug] [MainThread]: STDERR: "b''"
[0m21:49:14.342799 [debug] [MainThread]: Acquiring new glue connection "debug"
[0m21:49:14.343771 [debug] [MainThread]: Using glue connection "debug"
[0m21:49:14.343771 [debug] [MainThread]: On debug: select 1 as id
[0m21:49:14.343771 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:49:14.343771 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m21:49:14.343771 [debug] [MainThread]: Glue adapter: No session present, starting one
[0m21:49:14.344803 [debug] [MainThread]: Glue adapter: GlueConnection _start_session called
[0m21:49:40.785801 [debug] [MainThread]: Flushing usage events
[0m21:49:40.959131 [debug] [MainThread]: Glue adapter: cleanup called
[0m21:49:40.959131 [info ] [MainThread]: ctrl-c


============================== 2022-12-22 21:50:20.732432 | b09c7275-1805-4aba-ab7f-f624a8acb996 ==============================
[0m21:50:20.732432 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:50:20.733425 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'clean', 'indirect_selection': 'eager'}
[0m21:50:20.733425 [debug] [MainThread]: Tracking: tracking
[0m21:50:20.760426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF29065540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF2902B2E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF2902A590>]}
[0m21:50:20.763427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF290281F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF2902B850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001DF2902A860>]}
[0m21:50:20.764426 [debug] [MainThread]: Flushing usage events


============================== 2022-12-22 21:50:27.023494 | 2eeacbd9-7483-4278-8f18-983a32b4f41b ==============================
[0m21:50:27.023494 [info ] [MainThread]: Running with dbt=1.3.1
[0m21:50:27.024494 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m21:50:27.024494 [debug] [MainThread]: Tracking: tracking
[0m21:50:27.044487 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EB2459780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EB245A020>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EB2459ED0>]}
[0m21:50:27.176869 [debug] [MainThread]: Executing "git --help"
[0m21:50:27.231863 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m21:50:27.232872 [debug] [MainThread]: STDERR: "b''"
[0m21:50:27.235862 [debug] [MainThread]: Acquiring new glue connection "debug"
[0m21:50:27.235862 [debug] [MainThread]: Using glue connection "debug"
[0m21:50:27.235862 [debug] [MainThread]: On debug: select 1 as id
[0m21:50:27.235862 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:50:27.236869 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m21:50:27.236869 [debug] [MainThread]: Glue adapter: No session present, starting one
[0m21:50:27.236869 [debug] [MainThread]: Glue adapter: GlueConnection _start_session called
[0m21:55:28.912850 [error] [MainThread]: Glue adapter: Got an error when attempting to open a GlueSession : GlueSession took more than 300 seconds to start
[0m21:55:28.913852 [debug] [MainThread]: Glue adapter: Unhandled error while running:
select 1 as id
[0m21:55:28.913852 [debug] [MainThread]: On debug: No close available on handle
[0m21:55:28.915844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EB2A11810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EB33FF280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018EB33FCBE0>]}
[0m21:55:28.915844 [debug] [MainThread]: Flushing usage events
[0m21:55:29.100979 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 22:03:27.679713 | a31615f1-da49-45ba-b4af-157e4caaab51 ==============================
[0m22:03:27.679713 [info ] [MainThread]: Running with dbt=1.3.1
[0m22:03:27.680713 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'clean', 'indirect_selection': 'eager'}
[0m22:03:27.680713 [debug] [MainThread]: Tracking: tracking
[0m22:03:27.700713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177EC515540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177EC4DB2E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177EC4DA590>]}
[0m22:03:27.701715 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177EC4D81F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177EC4DB850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000177EC4DA860>]}
[0m22:03:27.702714 [debug] [MainThread]: Flushing usage events


============================== 2022-12-22 22:03:35.344146 | 5b097614-2341-4b09-9902-2854664c878d ==============================
[0m22:03:35.344146 [info ] [MainThread]: Running with dbt=1.3.1
[0m22:03:35.345146 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m22:03:35.345146 [debug] [MainThread]: Tracking: tracking
[0m22:03:35.366147 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250BE349DB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250BE3487C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000250BE349600>]}
[0m22:03:35.509144 [debug] [MainThread]: Executing "git --help"
[0m22:03:35.573146 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:03:35.574145 [debug] [MainThread]: STDERR: "b''"
[0m22:03:35.577143 [debug] [MainThread]: Acquiring new glue connection "debug"
[0m22:03:35.577143 [debug] [MainThread]: Using glue connection "debug"
[0m22:03:35.577143 [debug] [MainThread]: On debug: select 1 as id
[0m22:03:35.578143 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:03:35.578143 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m22:03:35.578143 [debug] [MainThread]: Glue adapter: No session present, starting one
[0m22:03:35.578143 [debug] [MainThread]: Glue adapter: GlueConnection _start_session called
[0m22:05:13.266636 [debug] [MainThread]: Flushing usage events
[0m22:05:13.478078 [debug] [MainThread]: Glue adapter: cleanup called
[0m22:05:13.479078 [info ] [MainThread]: ctrl-c


============================== 2022-12-22 22:10:36.894048 | 66cc603a-ff8a-47af-9963-224484f919ec ==============================
[0m22:10:36.894048 [info ] [MainThread]: Running with dbt=1.3.1
[0m22:10:36.895057 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m22:10:36.895057 [debug] [MainThread]: Tracking: tracking
[0m22:10:36.919045 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF05653BE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF056526E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF05653D30>]}
[0m22:10:37.090044 [debug] [MainThread]: Executing "git --help"
[0m22:10:37.165045 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:10:37.165045 [debug] [MainThread]: STDERR: "b''"
[0m22:10:37.169045 [debug] [MainThread]: Acquiring new glue connection "debug"
[0m22:10:37.169045 [debug] [MainThread]: Using glue connection "debug"
[0m22:10:37.170046 [debug] [MainThread]: On debug: select 1 as id
[0m22:10:37.170046 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:10:37.170046 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m22:10:37.170046 [debug] [MainThread]: Glue adapter: No session present, starting one
[0m22:10:37.171045 [debug] [MainThread]: Glue adapter: GlueConnection _start_session called
[0m22:12:40.107099 [debug] [MainThread]: Flushing usage events
[0m22:12:40.291912 [debug] [MainThread]: Glue adapter: cleanup called
[0m22:12:40.291912 [info ] [MainThread]: ctrl-c


============================== 2022-12-22 22:14:56.820821 | d86def50-d59b-4b2c-9a73-f064af2e92e6 ==============================
[0m22:14:56.820821 [info ] [MainThread]: Running with dbt=1.3.1
[0m22:14:56.820821 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'clean', 'indirect_selection': 'eager'}
[0m22:14:56.821821 [debug] [MainThread]: Tracking: tracking
[0m22:14:56.842085 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D58E31510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D58E31060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D58E31360>]}
[0m22:14:56.844649 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D58E31210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D58E31180>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025D58E32710>]}
[0m22:14:56.845665 [debug] [MainThread]: Flushing usage events


============================== 2022-12-22 22:15:06.987701 | b631ab81-9738-4ef3-a283-e1a029a140c8 ==============================
[0m22:15:06.987701 [info ] [MainThread]: Running with dbt=1.3.1
[0m22:15:06.988701 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m22:15:06.988701 [debug] [MainThread]: Tracking: tracking
[0m22:15:07.007704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000134F0D43B80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000134F0D41750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000134F0D434F0>]}
[0m22:15:07.153694 [debug] [MainThread]: Executing "git --help"
[0m22:15:07.211695 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:15:07.211695 [debug] [MainThread]: STDERR: "b''"
[0m22:15:07.215696 [debug] [MainThread]: Acquiring new glue connection "debug"
[0m22:15:07.215696 [debug] [MainThread]: Using glue connection "debug"
[0m22:15:07.215696 [debug] [MainThread]: On debug: select 1 as id
[0m22:15:07.216695 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:15:07.216695 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m22:15:07.216695 [debug] [MainThread]: Glue adapter: No session present, starting one
[0m22:15:07.216695 [debug] [MainThread]: Glue adapter: GlueConnection _start_session called
[0m22:20:04.458505 [debug] [MainThread]: Flushing usage events
[0m22:20:04.638651 [debug] [MainThread]: Glue adapter: cleanup called
[0m22:20:04.638651 [info ] [MainThread]: ctrl-c


============================== 2022-12-22 22:20:25.019433 | c2463201-d59c-485c-868f-a249f52aea97 ==============================
[0m22:20:25.019433 [info ] [MainThread]: Running with dbt=1.3.1
[0m22:20:25.019433 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'clean', 'indirect_selection': 'eager'}
[0m22:20:25.020433 [debug] [MainThread]: Tracking: tracking
[0m22:20:25.041432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000162C53C1570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000162C53C10C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000162C53C13C0>]}
[0m22:20:25.044433 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000162C53C1270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000162C53C1090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000162C53C2E30>]}
[0m22:20:25.044433 [debug] [MainThread]: Flushing usage events


============================== 2022-12-22 22:20:33.940060 | e2a85345-0271-43cd-9ed8-6f4d262f0a2b ==============================
[0m22:20:33.940060 [info ] [MainThread]: Running with dbt=1.3.1
[0m22:20:33.941061 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m22:20:33.941061 [debug] [MainThread]: Tracking: tracking
[0m22:20:33.961285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E8A2A3C40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E8A2A2740>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022E8A2A3D90>]}
[0m22:20:34.103335 [debug] [MainThread]: Executing "git --help"
[0m22:20:34.157258 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:20:34.158250 [debug] [MainThread]: STDERR: "b''"
[0m22:20:34.160249 [debug] [MainThread]: Acquiring new glue connection "debug"
[0m22:20:34.160249 [debug] [MainThread]: Using glue connection "debug"
[0m22:20:34.161249 [debug] [MainThread]: On debug: select 1 as id
[0m22:20:34.161249 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:20:34.161249 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m22:20:34.161249 [debug] [MainThread]: Glue adapter: No session present, starting one
[0m22:20:34.161249 [debug] [MainThread]: Glue adapter: GlueConnection _start_session called
[0m22:22:49.587014 [debug] [MainThread]: Flushing usage events
[0m22:22:49.768508 [debug] [MainThread]: Glue adapter: cleanup called
[0m22:22:49.769516 [info ] [MainThread]: ctrl-c


============================== 2022-12-22 22:22:59.437550 | 76227a4e-f6cd-40bd-bd61-a71aaeb473ff ==============================
[0m22:22:59.437550 [info ] [MainThread]: Running with dbt=1.3.1
[0m22:22:59.438545 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m22:22:59.438545 [debug] [MainThread]: Tracking: tracking
[0m22:22:59.472494 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203038F3BB0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203038F1780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203038F3520>]}
[0m22:22:59.682083 [debug] [MainThread]: Executing "git --help"
[0m22:22:59.767952 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m22:22:59.768990 [debug] [MainThread]: STDERR: "b''"
[0m22:22:59.773912 [debug] [MainThread]: Acquiring new glue connection "debug"
[0m22:22:59.773912 [debug] [MainThread]: Using glue connection "debug"
[0m22:22:59.774909 [debug] [MainThread]: On debug: select 1 as id
[0m22:22:59.774909 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:22:59.775909 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m22:22:59.775909 [debug] [MainThread]: Glue adapter: No session present, starting one
[0m22:22:59.776901 [debug] [MainThread]: Glue adapter: GlueConnection _start_session called
[0m22:23:43.808828 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m22:23:43.809845 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-3aaba338-b0ec-4d27-af20-a0b408378d16
[0m22:23:53.148979 [debug] [MainThread]: Glue adapter: GlueConnection cursor called
[0m22:23:53.149979 [debug] [MainThread]: Glue adapter: GlueCursor execute called
[0m22:23:53.149979 [debug] [MainThread]: Glue adapter: GlueCursor remove_comments_header called
[0m22:23:53.149979 [debug] [MainThread]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:23:53.149979 [debug] [MainThread]: Glue adapter: client : SqlWrapper2.execute('''select 1 as id''')
[0m22:24:14.553869 [debug] [MainThread]: Glue adapter: {'Statement': {'Id': 2, 'Code': "SqlWrapper2.execute('''select 1 as id''')", 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"id": 1}}], "description": [{"name": "id", "type": "IntegerType"}]}'}, 'ExecutionCount': 2, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671747833360, 'CompletedOn': 1671747854077}, 'ResponseMetadata': {'RequestId': 'f15c44be-6ec4-40ab-b3a5-236e83064e3e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:24:14 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '398', 'connection': 'keep-alive', 'x-amzn-requestid': 'f15c44be-6ec4-40ab-b3a5-236e83064e3e'}, 'RetryAttempts': 0}}
[0m22:24:14.554869 [debug] [MainThread]: Glue adapter: status = ok
[0m22:24:14.554869 [debug] [MainThread]: Glue adapter: GlueCursor execute successfully
[0m22:24:14.554869 [debug] [MainThread]: SQL status: OK in 74.78 seconds
[0m22:24:14.555867 [debug] [MainThread]: On debug: Close
[0m22:24:14.555867 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m22:24:14.556867 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020303EBB730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203048AF220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000203048AD750>]}
[0m22:24:14.557865 [debug] [MainThread]: Flushing usage events
[0m22:24:15.345978 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 22:28:53.323095 | 637f4974-c6d6-487a-b030-5c5cf48d1f3c ==============================
[0m22:28:53.323095 [info ] [MainThread]: Running with dbt=1.3.1
[0m22:28:53.324094 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:28:53.324094 [debug] [MainThread]: Tracking: tracking
[0m22:28:53.346094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18D83D6C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18D83DB70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18D83D7B0>]}
[0m22:28:53.357096 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m22:28:53.358098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '637f4974-c6d6-487a-b030-5c5cf48d1f3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18D83FB80>]}
[0m22:28:53.419095 [debug] [MainThread]: Parsing macros\adapters.sql
[0m22:28:53.443095 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m22:28:53.444096 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m22:28:53.445103 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m22:28:53.449103 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m22:28:53.463101 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m22:28:53.464101 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m22:28:53.472101 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m22:28:53.476101 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m22:28:53.479101 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m22:28:53.480101 [debug] [MainThread]: Parsing macros\adapters.sql
[0m22:28:53.512097 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m22:28:53.515098 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m22:28:53.522097 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m22:28:53.541097 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m22:28:53.546097 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m22:28:53.547099 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m22:28:53.549098 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m22:28:53.555097 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m22:28:53.562100 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m22:28:53.566101 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m22:28:53.566101 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m22:28:53.567099 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m22:28:53.568105 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m22:28:53.568105 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m22:28:53.570100 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m22:28:53.570100 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m22:28:53.571100 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m22:28:53.575098 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m22:28:53.585096 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m22:28:53.586101 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m22:28:53.587099 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m22:28:53.588099 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m22:28:53.598099 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m22:28:53.605098 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m22:28:53.607100 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m22:28:53.609099 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m22:28:53.615097 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m22:28:53.618097 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m22:28:53.630097 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m22:28:53.632098 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m22:28:53.634098 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m22:28:53.641097 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m22:28:53.645102 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m22:28:53.646099 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m22:28:53.647108 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m22:28:53.648099 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m22:28:53.648099 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m22:28:53.650099 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m22:28:53.651099 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m22:28:53.653099 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m22:28:53.655099 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m22:28:53.658098 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m22:28:53.664097 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m22:28:53.672096 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m22:28:53.673100 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m22:28:53.684098 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m22:28:53.696104 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m22:28:53.701104 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m22:28:53.703104 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m22:28:53.707106 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m22:28:53.709104 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m22:28:53.710104 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m22:28:53.711104 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m22:28:53.715105 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m22:28:53.727094 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m22:28:53.732095 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m22:28:53.741095 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m22:28:53.749094 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m22:28:53.750093 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m22:28:53.760181 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m22:28:53.762163 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m22:28:53.765157 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m22:28:53.766163 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m22:28:53.770098 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m22:28:53.771098 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m22:28:53.771098 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m22:28:53.772098 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m22:28:53.773098 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m22:28:53.774098 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m22:28:53.775098 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m22:28:53.776098 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m22:28:53.780097 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m22:28:53.781098 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m22:28:53.782098 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m22:28:53.783098 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m22:28:53.783098 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m22:28:53.784098 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m22:28:53.785099 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m22:28:53.786098 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m22:28:53.787099 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m22:28:53.788098 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m22:28:53.790098 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m22:28:53.791098 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m22:28:53.791098 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m22:28:53.792099 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m22:28:53.793098 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m22:28:53.794099 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m22:28:53.796105 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m22:28:54.052650 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_insert_overwrite_table.sql
[0m22:28:54.061720 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_insert_table.sql
[0m22:28:54.063721 [debug] [MainThread]: 1603: static parser failed on metrics\hudi_upsert_partitioned_cow_table.sql
[0m22:28:54.067711 [debug] [MainThread]: 1602: parser fallback to jinja rendering on metrics\hudi_upsert_partitioned_cow_table.sql
[0m22:28:54.068723 [debug] [MainThread]: 1603: static parser failed on metrics\hudi_upsert_partitioned_mor_table.sql
[0m22:28:54.072727 [debug] [MainThread]: 1602: parser fallback to jinja rendering on metrics\hudi_upsert_partitioned_mor_table.sql
[0m22:28:54.074667 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_upsert_table.sql
[0m22:28:54.146650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '637f4974-c6d6-487a-b030-5c5cf48d1f3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18DA37F10>]}
[0m22:28:54.154651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '637f4974-c6d6-487a-b030-5c5cf48d1f3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18D83E680>]}
[0m22:28:54.154651 [info ] [MainThread]: Found 5 models, 2 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:28:54.155661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '637f4974-c6d6-487a-b030-5c5cf48d1f3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18D83F940>]}
[0m22:28:54.156720 [info ] [MainThread]: 
[0m22:28:54.157671 [debug] [MainThread]: Acquiring new glue connection "master"
[0m22:28:54.159651 [debug] [ThreadPool]: Acquiring new glue connection "list_sampledb"
[0m22:28:54.159651 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:28:54.159651 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m22:28:54.159651 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m22:28:54.160651 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m22:29:34.157559 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m22:29:34.158553 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-5b5eb211-37c6-4aec-95d5-49c34cfcf8c1
[0m22:29:42.933893 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m22:29:43.497248 [debug] [ThreadPool]: On list_sampledb: Close
[0m22:29:43.497248 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m22:29:43.499104 [debug] [ThreadPool]: Acquiring new glue connection "list_sampledb_sampledb"
[0m22:29:43.499104 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:29:43.499104 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m22:29:43.955372 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m22:29:43.955372 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m22:29:43.956368 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-5b5eb211-37c6-4aec-95d5-49c34cfcf8c1
[0m22:29:44.778680 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m22:29:45.310498 [debug] [ThreadPool]: On list_sampledb_sampledb: Close
[0m22:29:45.311496 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m22:29:45.313468 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '637f4974-c6d6-487a-b030-5c5cf48d1f3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18E4BA650>]}
[0m22:29:45.313468 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m22:29:45.313468 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m22:29:45.314477 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:29:45.314477 [info ] [MainThread]: 
[0m22:29:45.322484 [debug] [Thread-1 (]: Began running node model.dbtglue.hudi_insert_table
[0m22:29:45.322484 [info ] [Thread-1 (]: 1 of 5 START sql incremental model sampledb.hudi_insert_table .................. [RUN]
[0m22:29:45.323479 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.hudi_insert_table"
[0m22:29:45.323479 [debug] [Thread-1 (]: Began compiling node model.dbtglue.hudi_insert_table
[0m22:29:45.324476 [debug] [Thread-1 (]: Compiling model.dbtglue.hudi_insert_table
[0m22:29:45.326478 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.hudi_insert_table"
[0m22:29:45.330478 [debug] [Thread-1 (]: finished collecting timing info
[0m22:29:45.330478 [debug] [Thread-1 (]: Began executing node model.dbtglue.hudi_insert_table
[0m22:29:45.349545 [debug] [Thread-1 (]: finished collecting timing info
[0m22:29:45.350479 [debug] [Thread-1 (]: Compilation Error in model hudi_insert_table (models\metrics\hudi_insert_table.sql)
  Invalid incremental strategy provided: None
      Expected one of: 'append', 'merge', 'insert_overwrite'
  
  > in macro dbt_spark_validate_get_incremental_strategy (macros\materializations\incremental\validate.sql)
  > called by macro materialization_incremental_glue (macros\materializations\incremental\incremental.sql)
  > called by model hudi_insert_table (models\metrics\hudi_insert_table.sql)
[0m22:29:45.350479 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '637f4974-c6d6-487a-b030-5c5cf48d1f3c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18DF39690>]}
[0m22:29:45.350479 [error] [Thread-1 (]: 1 of 5 ERROR creating sql incremental model sampledb.hudi_insert_table ......... [[31mERROR[0m in 0.03s]
[0m22:29:45.351843 [debug] [Thread-1 (]: Finished running node model.dbtglue.hudi_insert_table
[0m22:29:45.352852 [debug] [Thread-1 (]: Began running node model.dbtglue.hudi_insert_overwrite_table
[0m22:29:45.352852 [info ] [Thread-1 (]: 2 of 5 SKIP relation sampledb.hudi_insert_overwrite_table ...................... [[33mSKIP[0m]
[0m22:29:45.353849 [debug] [Thread-1 (]: Finished running node model.dbtglue.hudi_insert_overwrite_table
[0m22:29:45.353849 [debug] [Thread-1 (]: Began running node model.dbtglue.hudi_upsert_table
[0m22:29:45.353849 [info ] [Thread-1 (]: 3 of 5 SKIP relation sampledb.hudi_upsert_table ................................ [[33mSKIP[0m]
[0m22:29:45.354848 [debug] [Thread-1 (]: Finished running node model.dbtglue.hudi_upsert_table
[0m22:29:45.355621 [debug] [Thread-1 (]: Began running node model.dbtglue.hudi_upsert_partitioned_cow_table
[0m22:29:45.355621 [info ] [Thread-1 (]: 4 of 5 SKIP relation sampledb.hudi_upsert_partitioned_cow_table ................ [[33mSKIP[0m]
[0m22:29:45.355621 [debug] [Thread-1 (]: Finished running node model.dbtglue.hudi_upsert_partitioned_cow_table
[0m22:29:45.355621 [debug] [Thread-1 (]: Began running node model.dbtglue.hudi_upsert_partitioned_mor_table
[0m22:29:45.356691 [info ] [Thread-1 (]: 5 of 5 SKIP relation sampledb.hudi_upsert_partitioned_mor_table ................ [[33mSKIP[0m]
[0m22:29:45.356691 [debug] [Thread-1 (]: Finished running node model.dbtglue.hudi_upsert_partitioned_mor_table
[0m22:29:45.357629 [debug] [MainThread]: Acquiring new glue connection "master"
[0m22:29:45.357629 [debug] [MainThread]: On master: ROLLBACK
[0m22:29:45.357629 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:29:45.358627 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m22:29:45.829821 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m22:29:45.830822 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m22:29:45.830822 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-5b5eb211-37c6-4aec-95d5-49c34cfcf8c1
[0m22:29:46.658231 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m22:29:46.659245 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m22:29:46.660240 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m22:29:46.660240 [debug] [MainThread]: On master: ROLLBACK
[0m22:29:46.661237 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m22:29:46.661237 [debug] [MainThread]: On master: Close
[0m22:29:46.662232 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m22:29:46.663229 [info ] [MainThread]: 
[0m22:29:46.665201 [info ] [MainThread]: Finished running 5 incremental models in 0 hours 0 minutes and 52.51 seconds (52.51s).
[0m22:29:46.665804 [debug] [MainThread]: Glue adapter: cleanup called
[0m22:29:46.946393 [info ] [MainThread]: 
[0m22:29:46.946393 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m22:29:46.947391 [info ] [MainThread]: 
[0m22:29:46.948389 [error] [MainThread]: [33mCompilation Error in model hudi_insert_table (models\metrics\hudi_insert_table.sql)[0m
[0m22:29:46.949390 [error] [MainThread]:   Invalid incremental strategy provided: None
[0m22:29:46.949390 [error] [MainThread]:       Expected one of: 'append', 'merge', 'insert_overwrite'
[0m22:29:46.950390 [error] [MainThread]:   
[0m22:29:46.950390 [error] [MainThread]:   > in macro dbt_spark_validate_get_incremental_strategy (macros\materializations\incremental\validate.sql)
[0m22:29:46.950390 [error] [MainThread]:   > called by macro materialization_incremental_glue (macros\materializations\incremental\incremental.sql)
[0m22:29:46.951391 [error] [MainThread]:   > called by model hudi_insert_table (models\metrics\hudi_insert_table.sql)
[0m22:29:46.951391 [info ] [MainThread]: 
[0m22:29:46.951391 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=4 TOTAL=5
[0m22:29:46.952391 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18E523E80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18E523FA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C18E029120>]}
[0m22:29:46.952391 [debug] [MainThread]: Flushing usage events
[0m22:29:47.137041 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 22:32:33.223069 | 0e1a8adc-724f-4c63-8ab3-20ac7b80975b ==============================
[0m22:32:33.223069 [info ] [MainThread]: Running with dbt=1.3.1
[0m22:32:33.224070 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m22:32:33.224070 [debug] [MainThread]: Tracking: tracking
[0m22:32:33.249068 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E6014A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E2F8BDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E6014490>]}
[0m22:32:33.336072 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m22:32:33.336072 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\hudi_insert_table.sql
[0m22:32:33.352077 [debug] [MainThread]: 1699: static parser successfully parsed metrics\hudi_insert_table.sql
[0m22:32:33.397067 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0e1a8adc-724f-4c63-8ab3-20ac7b80975b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E62039A0>]}
[0m22:32:33.404132 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0e1a8adc-724f-4c63-8ab3-20ac7b80975b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E6123FD0>]}
[0m22:32:33.405143 [info ] [MainThread]: Found 5 models, 2 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m22:32:33.406073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e1a8adc-724f-4c63-8ab3-20ac7b80975b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E61AB2E0>]}
[0m22:32:33.407070 [info ] [MainThread]: 
[0m22:32:33.407865 [debug] [MainThread]: Acquiring new glue connection "master"
[0m22:32:33.408880 [debug] [ThreadPool]: Acquiring new glue connection "list_sampledb"
[0m22:32:33.409921 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:32:33.409921 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m22:32:33.409921 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m22:32:33.409921 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m22:33:17.293611 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m22:33:17.294609 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9cd35a8c-690f-4d87-91a6-edbcea9747eb
[0m22:33:25.411936 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m22:33:25.897269 [debug] [ThreadPool]: On list_sampledb: Close
[0m22:33:25.897269 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m22:33:25.899198 [debug] [ThreadPool]: Acquiring new glue connection "list_sampledb_sampledb"
[0m22:33:25.899198 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m22:33:25.899198 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m22:33:26.389698 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m22:33:26.390686 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m22:33:26.390686 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9cd35a8c-690f-4d87-91a6-edbcea9747eb
[0m22:33:28.413101 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m22:33:28.990691 [debug] [ThreadPool]: On list_sampledb_sampledb: Close
[0m22:33:28.990691 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m22:33:28.991656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0e1a8adc-724f-4c63-8ab3-20ac7b80975b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E6230D00>]}
[0m22:33:28.991656 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m22:33:28.991656 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m22:33:28.992678 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m22:33:28.993617 [info ] [MainThread]: 
[0m22:33:28.998615 [debug] [Thread-1 (]: Began running node model.dbtglue.hudi_insert_table
[0m22:33:28.998615 [info ] [Thread-1 (]: 1 of 5 START sql incremental model sampledb.hudi_insert_table .................. [RUN]
[0m22:33:28.999623 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.hudi_insert_table"
[0m22:33:28.999623 [debug] [Thread-1 (]: Began compiling node model.dbtglue.hudi_insert_table
[0m22:33:29.000623 [debug] [Thread-1 (]: Compiling model.dbtglue.hudi_insert_table
[0m22:33:29.002616 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.hudi_insert_table"
[0m22:33:29.003624 [debug] [Thread-1 (]: finished collecting timing info
[0m22:33:29.003624 [debug] [Thread-1 (]: Began executing node model.dbtglue.hudi_insert_table
[0m22:33:29.022618 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m22:33:29.022618 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m22:33:29.511645 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m22:33:29.511645 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m22:33:29.511645 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9cd35a8c-690f-4d87-91a6-edbcea9747eb
[0m22:33:30.317638 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:33:30.882009 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table hudi_insert_table not found.
[0m22:33:30.888989 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m22:33:30.888989 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.hudi_insert_table"
[0m22:33:30.888989 [debug] [Thread-1 (]: On model.dbtglue.hudi_insert_table: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_table"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m22:33:30.888989 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:33:30.890007 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:33:30.890007 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:33:30.890007 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:33:30.890007 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_table"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m22:33:50.047839 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_table"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671748411095, 'CompletedOn': 1671748428853}, 'ResponseMetadata': {'RequestId': '020d57d1-4c4c-414b-ac49-bcf1e994e472', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:33:50 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '706', 'connection': 'keep-alive', 'x-amzn-requestid': '020d57d1-4c4c-414b-ac49-bcf1e994e472'}, 'RetryAttempts': 0}}
[0m22:33:50.048937 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m22:33:50.048937 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m22:33:50.048937 [debug] [Thread-1 (]: SQL status: OK in 19.16 seconds
[0m22:33:50.051910 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:33:50.055941 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:33:50.654663 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table hudi_insert_table not found.
[0m22:33:50.654663 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""

with source_data as (

    select format_number(rand()*1000, 0) as id
    union all
    select null as id

    )

select *
from source_data
where id is not null""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_insert_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_insert_table', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_insert_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_insert_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_table/")
        
spark.sql("""REFRESH TABLE sampledb.hudi_insert_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_insert_table LIMIT 1""")
        
        
[0m22:33:50.655645 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:33:50.655645 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:33:50.655645 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:33:50.656645 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""

with source_data as (

    select format_number(rand()*1000, 0) as id
    union all
    select null as id

    )

select *
from source_data
where id is not null""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_insert_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_insert_table', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_insert_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_insert_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_table/")

spark.sql("""REFRESH TABLE sampledb.hudi_insert_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_insert_table LIMIT 1""")

[0m22:34:16.247972 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\nwith source_data as (\n\n    select format_number(rand()*1000, 0) as id\n    union all\n    select null as id\n\n    )\n\nselect *\nfrom source_data\nwhere id is not null""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'hudi_insert_table\', \'hoodie.datasource.hive_sync.database\': \'sampledb\', \'hoodie.datasource.hive_sync.table\': \'hudi_insert_table\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_table/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'hudi_insert_table\', \'hoodie.datasource.hive_sync.database\': \'sampledb\', \'hoodie.datasource.hive_sync.table\': \'hudi_insert_table\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_table/")\n\nspark.sql("""REFRESH TABLE sampledb.hudi_insert_table""")\nSqlWrapper2.execute("""SELECT * FROM sampledb.hudi_insert_table LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221222223354435", "_hoodie_commit_seqno": "20221222223354435_0_1", "_hoodie_record_key": "908", "_hoodie_partition_path": "", "_hoodie_file_name": "2eb4060f-9445-4db4-b8f0-184da83d9e83-0_0-7-0_20221222223354435.parquet", "id": "908", "update_hudi_ts": "2022-12-22 22:33:56.720000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "id", "type": "StringType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671748431494, 'CompletedOn': 1671748455747}, 'ResponseMetadata': {'RequestId': '232b8365-2502-4cba-9375-6a6cdd4836f6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:34:16 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3508', 'connection': 'keep-alive', 'x-amzn-requestid': '232b8365-2502-4cba-9375-6a6cdd4836f6'}, 'RetryAttempts': 0}}
[0m22:34:16.247972 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m22:34:16.248908 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m22:34:16.248908 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.hudi_insert_table"
[0m22:34:16.250907 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.hudi_insert_table"
[0m22:34:16.250907 [debug] [Thread-1 (]: On model.dbtglue.hudi_insert_table: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_table"} */
select * from sampledb.hudi_insert_table limit 1 
[0m22:34:16.251918 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:16.251918 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:34:16.251918 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:34:16.251918 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:34:16.252915 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_table"} */
select * from sampledb.hudi_insert_table limit 1 ''')
[0m22:34:18.199602 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_table"} */\nselect * from sampledb.hudi_insert_table limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221222223354435", "_hoodie_commit_seqno": "20221222223354435_0_1", "_hoodie_record_key": "908", "_hoodie_partition_path": "", "_hoodie_file_name": "2eb4060f-9445-4db4-b8f0-184da83d9e83-0_0-7-0_20221222223354435.parquet", "id": "908", "update_hudi_ts": "2022-12-22 22:33:56.720000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "id", "type": "StringType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671748456489, 'CompletedOn': 1671748457460}, 'ResponseMetadata': {'RequestId': '7e46af0f-6a6c-41f1-96ef-b4c963cef704', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:34:18 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1295', 'connection': 'keep-alive', 'x-amzn-requestid': '7e46af0f-6a6c-41f1-96ef-b4c963cef704'}, 'RetryAttempts': 0}}
[0m22:34:18.199602 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m22:34:18.199602 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m22:34:18.200602 [debug] [Thread-1 (]: SQL status: OK in 1.95 seconds
[0m22:34:18.201602 [debug] [Thread-1 (]: finished collecting timing info
[0m22:34:18.201602 [debug] [Thread-1 (]: On model.dbtglue.hudi_insert_table: ROLLBACK
[0m22:34:18.201602 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m22:34:18.201602 [debug] [Thread-1 (]: On model.dbtglue.hudi_insert_table: Close
[0m22:34:18.201602 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m22:34:18.202601 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e1a8adc-724f-4c63-8ab3-20ac7b80975b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E6C1E0B0>]}
[0m22:34:18.202601 [info ] [Thread-1 (]: 1 of 5 OK created sql incremental model sampledb.hudi_insert_table ............. [[32mOK[0m in 49.20s]
[0m22:34:18.203603 [debug] [Thread-1 (]: Finished running node model.dbtglue.hudi_insert_table
[0m22:34:18.204601 [debug] [Thread-1 (]: Began running node model.dbtglue.hudi_insert_overwrite_table
[0m22:34:18.204601 [info ] [Thread-1 (]: 2 of 5 START sql incremental model sampledb.hudi_insert_overwrite_table ........ [RUN]
[0m22:34:18.205601 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.hudi_insert_overwrite_table"
[0m22:34:18.205601 [debug] [Thread-1 (]: Began compiling node model.dbtglue.hudi_insert_overwrite_table
[0m22:34:18.205601 [debug] [Thread-1 (]: Compiling model.dbtglue.hudi_insert_overwrite_table
[0m22:34:18.208601 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.hudi_insert_overwrite_table"
[0m22:34:18.209601 [debug] [Thread-1 (]: finished collecting timing info
[0m22:34:18.209601 [debug] [Thread-1 (]: Began executing node model.dbtglue.hudi_insert_overwrite_table
[0m22:34:18.210602 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:34:18.210602 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m22:34:18.696413 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m22:34:18.697414 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m22:34:18.697414 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9cd35a8c-690f-4d87-91a6-edbcea9747eb
[0m22:34:20.051365 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:20.614308 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table hudi_insert_overwrite_table not found.
[0m22:34:20.615307 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m22:34:20.615307 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.hudi_insert_overwrite_table"
[0m22:34:20.615307 [debug] [Thread-1 (]: On model.dbtglue.hudi_insert_overwrite_table: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_overwrite_table"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m22:34:20.615307 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:20.616312 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:34:20.616312 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:34:20.616312 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:34:20.616312 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_overwrite_table"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m22:34:21.451879 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_overwrite_table"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 11, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671748460825, 'CompletedOn': 1671748461131}, 'ResponseMetadata': {'RequestId': 'a6c0492b-1b7e-4508-b899-2ac6ffca46ac', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:34:21 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '718', 'connection': 'keep-alive', 'x-amzn-requestid': 'a6c0492b-1b7e-4508-b899-2ac6ffca46ac'}, 'RetryAttempts': 0}}
[0m22:34:21.451879 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m22:34:21.452880 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m22:34:21.452880 [debug] [Thread-1 (]: SQL status: OK in 0.84 seconds
[0m22:34:21.456880 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:21.458887 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:21.999796 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table hudi_insert_overwrite_table not found.
[0m22:34:22.000796 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""

select id, cast(rand() as string) as name, current_timestamp() as ts
from sampledb.hudi_insert_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_insert_overwrite_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_insert_overwrite_table', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_overwrite_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_insert_overwrite_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_insert_overwrite_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_overwrite_table/")
        
spark.sql("""REFRESH TABLE sampledb.hudi_insert_overwrite_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_insert_overwrite_table LIMIT 1""")
        
        
[0m22:34:22.000796 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:34:22.000796 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:34:22.000796 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:34:22.001789 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""

select id, cast(rand() as string) as name, current_timestamp() as ts
from sampledb.hudi_insert_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_insert_overwrite_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_insert_overwrite_table', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_overwrite_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_insert_overwrite_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_insert_overwrite_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_overwrite_table/")

spark.sql("""REFRESH TABLE sampledb.hudi_insert_overwrite_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_insert_overwrite_table LIMIT 1""")

[0m22:34:34.421391 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 12, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\nselect id, cast(rand() as string) as name, current_timestamp() as ts\nfrom sampledb.hudi_insert_table""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'hudi_insert_overwrite_table\', \'hoodie.datasource.hive_sync.database\': \'sampledb\', \'hoodie.datasource.hive_sync.table\': \'hudi_insert_overwrite_table\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_overwrite_table/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'hudi_insert_overwrite_table\', \'hoodie.datasource.hive_sync.database\': \'sampledb\', \'hoodie.datasource.hive_sync.table\': \'hudi_insert_overwrite_table\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/sample//sampledb/hudi_insert_overwrite_table/")\n\nspark.sql("""REFRESH TABLE sampledb.hudi_insert_overwrite_table""")\nSqlWrapper2.execute("""SELECT * FROM sampledb.hudi_insert_overwrite_table LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221222223423168", "_hoodie_commit_seqno": "20221222223423168_0_2", "_hoodie_record_key": "908", "_hoodie_partition_path": "", "_hoodie_file_name": "0d53389f-dc1c-4366-822c-5b09454cc5a3-0_0-29-0_20221222223423168.parquet", "id": "908", "name": "0.6065562171113016", "ts": "2022-12-22 22:34:24.483000", "update_hudi_ts": "2022-12-22 22:34:24.483000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "id", "type": "StringType"}, {"name": "name", "type": "StringType"}, {"name": "ts", "type": "TimestampType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 12, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671748462250, 'CompletedOn': 1671748473313}, 'ResponseMetadata': {'RequestId': 'f8336daa-2ed8-402c-91d9-c7743ae2340a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:34:34 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3693', 'connection': 'keep-alive', 'x-amzn-requestid': 'f8336daa-2ed8-402c-91d9-c7743ae2340a'}, 'RetryAttempts': 0}}
[0m22:34:34.422381 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m22:34:34.422381 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m22:34:34.423377 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.hudi_insert_overwrite_table"
[0m22:34:34.424378 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.hudi_insert_overwrite_table"
[0m22:34:34.424378 [debug] [Thread-1 (]: On model.dbtglue.hudi_insert_overwrite_table: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_overwrite_table"} */
select * from sampledb.hudi_insert_overwrite_table limit 1 
[0m22:34:34.424378 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:34.424378 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:34:34.424378 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:34:34.425390 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:34:34.425390 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_overwrite_table"} */
select * from sampledb.hudi_insert_overwrite_table limit 1 ''')
[0m22:34:36.031278 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 13, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_insert_overwrite_table"} */\nselect * from sampledb.hudi_insert_overwrite_table limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221222223423168", "_hoodie_commit_seqno": "20221222223423168_0_2", "_hoodie_record_key": "908", "_hoodie_partition_path": "", "_hoodie_file_name": "0d53389f-dc1c-4366-822c-5b09454cc5a3-0_0-29-0_20221222223423168.parquet", "id": "908", "name": "0.6065562171113016", "ts": "2022-12-22 22:34:24.483000", "update_hudi_ts": "2022-12-22 22:34:24.483000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "id", "type": "StringType"}, {"name": "name", "type": "StringType"}, {"name": "ts", "type": "TimestampType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 13, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671748474639, 'CompletedOn': 1671748475649}, 'ResponseMetadata': {'RequestId': '84d1de2b-0d16-48a6-ac8b-6448dc770eca', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:34:36 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1489', 'connection': 'keep-alive', 'x-amzn-requestid': '84d1de2b-0d16-48a6-ac8b-6448dc770eca'}, 'RetryAttempts': 0}}
[0m22:34:36.031278 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m22:34:36.031278 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m22:34:36.031278 [debug] [Thread-1 (]: SQL status: OK in 1.61 seconds
[0m22:34:36.033280 [debug] [Thread-1 (]: finished collecting timing info
[0m22:34:36.033280 [debug] [Thread-1 (]: On model.dbtglue.hudi_insert_overwrite_table: ROLLBACK
[0m22:34:36.033280 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m22:34:36.033280 [debug] [Thread-1 (]: On model.dbtglue.hudi_insert_overwrite_table: Close
[0m22:34:36.034279 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m22:34:36.034279 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e1a8adc-724f-4c63-8ab3-20ac7b80975b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E6E10400>]}
[0m22:34:36.035279 [info ] [Thread-1 (]: 2 of 5 OK created sql incremental model sampledb.hudi_insert_overwrite_table ... [[32mOK[0m in 17.83s]
[0m22:34:36.036282 [debug] [Thread-1 (]: Finished running node model.dbtglue.hudi_insert_overwrite_table
[0m22:34:36.037278 [debug] [Thread-1 (]: Began running node model.dbtglue.hudi_upsert_table
[0m22:34:36.037278 [info ] [Thread-1 (]: 3 of 5 START sql incremental model sampledb.hudi_upsert_table .................. [RUN]
[0m22:34:36.038279 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.hudi_upsert_table"
[0m22:34:36.039279 [debug] [Thread-1 (]: Began compiling node model.dbtglue.hudi_upsert_table
[0m22:34:36.039279 [debug] [Thread-1 (]: Compiling model.dbtglue.hudi_upsert_table
[0m22:34:36.044279 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.hudi_upsert_table"
[0m22:34:36.055287 [debug] [Thread-1 (]: finished collecting timing info
[0m22:34:36.055287 [debug] [Thread-1 (]: Began executing node model.dbtglue.hudi_upsert_table
[0m22:34:36.058287 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:34:36.058287 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m22:34:36.536246 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m22:34:36.537244 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m22:34:36.537244 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9cd35a8c-690f-4d87-91a6-edbcea9747eb
[0m22:34:37.644894 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:38.193695 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table hudi_upsert_table not found.
[0m22:34:38.194704 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m22:34:38.194704 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.hudi_upsert_table"
[0m22:34:38.195694 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_table: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_table"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m22:34:38.195694 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:38.195694 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:34:38.195694 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:34:38.196692 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:34:38.196692 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_table"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m22:34:38.598732 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 16, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_table"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 16, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671748478416, 'CompletedOn': 1671748478573}, 'ResponseMetadata': {'RequestId': '13421e63-7179-4da5-a15e-64cce9e2620b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:34:38 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '708', 'connection': 'keep-alive', 'x-amzn-requestid': '13421e63-7179-4da5-a15e-64cce9e2620b'}, 'RetryAttempts': 0}}
[0m22:34:38.598732 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m22:34:38.599745 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m22:34:38.599745 [debug] [Thread-1 (]: SQL status: OK in 0.4 seconds
[0m22:34:38.602744 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:38.604744 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:39.141783 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table hudi_upsert_table not found.
[0m22:34:39.141783 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/*
 Example of an upsert for a non-partitioned table with incremental materialization using merge strategy.
 */


select id, name, current_timestamp() as ts
from sampledb.hudi_insert_overwrite_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_table', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_table/")
        
spark.sql("""REFRESH TABLE sampledb.hudi_upsert_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_table LIMIT 1""")
        
        
[0m22:34:39.141783 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:34:39.141783 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:34:39.142784 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:34:39.142784 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/*
 Example of an upsert for a non-partitioned table with incremental materialization using merge strategy.
 */


select id, name, current_timestamp() as ts
from sampledb.hudi_insert_overwrite_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_table', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_table/")

spark.sql("""REFRESH TABLE sampledb.hudi_upsert_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_table LIMIT 1""")

[0m22:34:50.371249 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 17, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * "License"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\n */\n\n/*\n Example of an upsert for a non-partitioned table with incremental materialization using merge strategy.\n */\n\n\nselect id, name, current_timestamp() as ts\nfrom sampledb.hudi_insert_overwrite_table""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'hudi_upsert_table\', \'hoodie.datasource.hive_sync.database\': \'sampledb\', \'hoodie.datasource.hive_sync.table\': \'hudi_upsert_table\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_table/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'hudi_upsert_table\', \'hoodie.datasource.hive_sync.database\': \'sampledb\', \'hoodie.datasource.hive_sync.table\': \'hudi_upsert_table\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_table/")\n\nspark.sql("""REFRESH TABLE sampledb.hudi_upsert_table""")\nSqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_table LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221222223440106", "_hoodie_commit_seqno": "20221222223440106_0_1", "_hoodie_record_key": "908", "_hoodie_partition_path": "", "_hoodie_file_name": "1371ce18-b3de-44fe-b7fa-34742a9efa0f-0_0-51-0_20221222223440106.parquet", "id": "908", "name": "0.6065562171113016", "ts": "2022-12-22 22:34:41.438000", "update_hudi_ts": "2022-12-22 22:34:41.438000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "id", "type": "StringType"}, {"name": "name", "type": "StringType"}, {"name": "ts", "type": "TimestampType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 17, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671748479349, 'CompletedOn': 1671748490035}, 'ResponseMetadata': {'RequestId': '44f47bad-5df5-4ca8-b069-23c2357527c8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:34:50 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4548', 'connection': 'keep-alive', 'x-amzn-requestid': '44f47bad-5df5-4ca8-b069-23c2357527c8'}, 'RetryAttempts': 0}}
[0m22:34:50.371249 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m22:34:50.371249 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m22:34:50.372249 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.hudi_upsert_table"
[0m22:34:50.381249 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.hudi_upsert_table"
[0m22:34:50.381249 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_table: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_table"} */
select * from sampledb.hudi_upsert_table limit 1 
[0m22:34:50.382242 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:50.382242 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:34:50.382242 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:34:50.382242 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:34:50.383256 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_table"} */
select * from sampledb.hudi_upsert_table limit 1 ''')
[0m22:34:51.963862 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 18, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_table"} */\nselect * from sampledb.hudi_upsert_table limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221222223440106", "_hoodie_commit_seqno": "20221222223440106_0_1", "_hoodie_record_key": "908", "_hoodie_partition_path": "", "_hoodie_file_name": "1371ce18-b3de-44fe-b7fa-34742a9efa0f-0_0-51-0_20221222223440106.parquet", "id": "908", "name": "0.6065562171113016", "ts": "2022-12-22 22:34:41.438000", "update_hudi_ts": "2022-12-22 22:34:41.438000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "id", "type": "StringType"}, {"name": "name", "type": "StringType"}, {"name": "ts", "type": "TimestampType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 18, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671748490597, 'CompletedOn': 1671748491391}, 'ResponseMetadata': {'RequestId': '11e6130e-fd84-42aa-bf57-ca98b2c847f7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:34:51 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1469', 'connection': 'keep-alive', 'x-amzn-requestid': '11e6130e-fd84-42aa-bf57-ca98b2c847f7'}, 'RetryAttempts': 0}}
[0m22:34:51.963862 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m22:34:51.964934 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m22:34:51.964934 [debug] [Thread-1 (]: SQL status: OK in 1.58 seconds
[0m22:34:51.965936 [debug] [Thread-1 (]: finished collecting timing info
[0m22:34:51.965936 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_table: ROLLBACK
[0m22:34:51.965936 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m22:34:51.966948 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_table: Close
[0m22:34:51.966948 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m22:34:51.968957 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e1a8adc-724f-4c63-8ab3-20ac7b80975b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E7F7E920>]}
[0m22:34:51.969959 [info ] [Thread-1 (]: 3 of 5 OK created sql incremental model sampledb.hudi_upsert_table ............. [[32mOK[0m in 15.93s]
[0m22:34:51.970948 [debug] [Thread-1 (]: Finished running node model.dbtglue.hudi_upsert_table
[0m22:34:51.972567 [debug] [Thread-1 (]: Began running node model.dbtglue.hudi_upsert_partitioned_cow_table
[0m22:34:51.972567 [info ] [Thread-1 (]: 4 of 5 START sql incremental model sampledb.hudi_upsert_partitioned_cow_table .. [RUN]
[0m22:34:51.973573 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.hudi_upsert_partitioned_cow_table"
[0m22:34:51.973573 [debug] [Thread-1 (]: Began compiling node model.dbtglue.hudi_upsert_partitioned_cow_table
[0m22:34:51.973573 [debug] [Thread-1 (]: Compiling model.dbtglue.hudi_upsert_partitioned_cow_table
[0m22:34:51.977579 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.hudi_upsert_partitioned_cow_table"
[0m22:34:51.977579 [debug] [Thread-1 (]: finished collecting timing info
[0m22:34:51.978582 [debug] [Thread-1 (]: Began executing node model.dbtglue.hudi_upsert_partitioned_cow_table
[0m22:34:51.979579 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:34:51.979579 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m22:34:52.490319 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m22:34:52.491311 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m22:34:52.491311 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9cd35a8c-690f-4d87-91a6-edbcea9747eb
[0m22:34:53.298475 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:53.847461 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table hudi_upsert_partitioned_cow_table not found.
[0m22:34:53.848458 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m22:34:53.848458 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.hudi_upsert_partitioned_cow_table"
[0m22:34:53.849474 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_partitioned_cow_table: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_cow_table"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m22:34:53.849474 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:53.849474 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:34:53.849474 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:34:53.850401 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:34:53.850401 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_cow_table"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m22:34:54.250502 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 21, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_cow_table"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 21, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671748494064, 'CompletedOn': 1671748494219}, 'ResponseMetadata': {'RequestId': 'fd2d65cf-f031-469a-8c99-7524e2ccf721', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:34:54 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '724', 'connection': 'keep-alive', 'x-amzn-requestid': 'fd2d65cf-f031-469a-8c99-7524e2ccf721'}, 'RetryAttempts': 0}}
[0m22:34:54.250502 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m22:34:54.251525 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m22:34:54.251525 [debug] [Thread-1 (]: SQL status: OK in 0.4 seconds
[0m22:34:54.255509 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:54.257507 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:54.803423 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table hudi_upsert_partitioned_cow_table not found.
[0m22:34:54.803423 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/*
 Example of an upsert for a partitioned copy on write table with incremental materialization using merge strategy.
 */


select id, name, current_timestamp() as ts, current_date as datestr
from sampledb.hudi_upsert_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if datestr is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.write.partitionpath.field': 'd,a,t,e,s,t,r', 'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor', 'hoodie.datasource.hive_sync.partition_fields': 'd,a,t,e,s,t,r','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")
        
spark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_cow_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_cow_table LIMIT 1""")
        
        
[0m22:34:54.803423 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:34:54.804424 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:34:54.804424 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:34:54.804424 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/*
 Example of an upsert for a partitioned copy on write table with incremental materialization using merge strategy.
 */


select id, name, current_timestamp() as ts, current_date as datestr
from sampledb.hudi_upsert_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if datestr is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.write.partitionpath.field': 'd,a,t,e,s,t,r', 'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor', 'hoodie.datasource.hive_sync.partition_fields': 'd,a,t,e,s,t,r','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")

spark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_cow_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_cow_table LIMIT 1""")

[0m22:34:56.398015 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 22, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * "License"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\n */\n\n/*\n Example of an upsert for a partitioned copy on write table with incremental materialization using merge strategy.\n */\n\n\nselect id, name, current_timestamp() as ts, current_date as datestr\nfrom sampledb.hudi_upsert_table""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if datestr is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'hudi_upsert_partitioned_cow_table\', \'hoodie.datasource.hive_sync.database\': \'sampledb\', \'hoodie.datasource.hive_sync.table\': \'hudi_upsert_partitioned_cow_table\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.write.partitionpath.field\': \'d,a,t,e,s,t,r\', \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.MultiPartKeysValueExtractor\', \'hoodie.datasource.hive_sync.partition_fields\': \'d,a,t,e,s,t,r\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'hudi_upsert_partitioned_cow_table\', \'hoodie.datasource.hive_sync.database\': \'sampledb\', \'hoodie.datasource.hive_sync.table\': \'hudi_upsert_partitioned_cow_table\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")\n\nspark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_cow_table""")\nSqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_cow_table LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 22, 'Status': 'error', 'ErrorName': 'NameError', 'ErrorValue': "name 'datestr' is not defined", 'Traceback': ['Traceback (most recent call last):\n', "NameError: name 'datestr' is not defined\n"]}, 'Progress': 1.0, 'StartedOn': 1671748495018, 'CompletedOn': 1671748495661}, 'ResponseMetadata': {'RequestId': '9e34970a-c91c-406e-ac73-dcd60812bae6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:34:56 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4145', 'connection': 'keep-alive', 'x-amzn-requestid': '9e34970a-c91c-406e-ac73-dcd60812bae6'}, 'RetryAttempts': 0}}
[0m22:34:56.398953 [debug] [Thread-1 (]: Glue adapter: status = error
[0m22:34:56.400010 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/*
 Example of an upsert for a partitioned copy on write table with incremental materialization using merge strategy.
 */


select id, name, current_timestamp() as ts, current_date as datestr
from sampledb.hudi_upsert_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if datestr is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.write.partitionpath.field': 'd,a,t,e,s,t,r', 'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor', 'hoodie.datasource.hive_sync.partition_fields': 'd,a,t,e,s,t,r','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")

spark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_cow_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_cow_table LIMIT 1""")
, NameError: name 'datestr' is not defined
[0m22:34:56.406010 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/*
 Example of an upsert for a partitioned copy on write table with incremental materialization using merge strategy.
 */


select id, name, current_timestamp() as ts, current_date as datestr
from sampledb.hudi_upsert_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if datestr is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.write.partitionpath.field': 'd,a,t,e,s,t,r', 'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor', 'hoodie.datasource.hive_sync.partition_fields': 'd,a,t,e,s,t,r','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")

spark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_cow_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_cow_table LIMIT 1""")
, NameError: name 'datestr' is not defined
[0m22:34:56.406010 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""/*
   * Licensed to the Apache Software Foundation (ASF) under one
   * or more contributor license agreements.  See the NOTICE file
   * distributed with this work for additional information
   * regarding copyright ownership.  The ASF licenses this file
   * to you under the Apache License, Version 2.0 (the
   * "License"); you may not use this file except in compliance
   * with the License.  You may obtain a copy of the License at
   *
   *   http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing,
   * software distributed under the License is distributed on an
   * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
   * KIND, either express or implied.  See the License for the
   * specific language governing permissions and limitations
   * under the License.
   *
   */
  
  /*
   Example of an upsert for a partitioned copy on write table with incremental materialization using merge strategy.
   */
  
  
  select id, name, current_timestamp() as ts, current_date as datestr
  from sampledb.hudi_upsert_table""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if datestr is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.write.partitionpath.field': 'd,a,t,e,s,t,r', 'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor', 'hoodie.datasource.hive_sync.partition_fields': 'd,a,t,e,s,t,r','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_cow_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_cow_table/")
  
  spark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_cow_table""")
  SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_cow_table LIMIT 1""")
  , NameError: name 'datestr' is not defined
[0m22:34:56.409015 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.hudi_upsert_partitioned_cow_table"
[0m22:34:56.409951 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.hudi_upsert_partitioned_cow_table"
[0m22:34:56.409951 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_partitioned_cow_table: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_cow_table"} */
select * from sampledb.hudi_upsert_partitioned_cow_table limit 1 
[0m22:34:56.409951 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:56.409951 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:34:56.409951 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:34:56.411016 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:34:56.411016 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_cow_table"} */
select * from sampledb.hudi_upsert_partitioned_cow_table limit 1 ''')
[0m22:34:57.998954 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 23, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_cow_table"} */\nselect * from sampledb.hudi_upsert_partitioned_cow_table limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 23, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: sampledb.hudi_upsert_partitioned_cow_table; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_cow_table], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: sampledb.hudi_upsert_partitioned_cow_table; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_cow_table], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671748496613, 'CompletedOn': 1671748496847}, 'ResponseMetadata': {'RequestId': '595c10d8-576a-4328-bec0-8a39b2c33d20', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:34:58 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1554', 'connection': 'keep-alive', 'x-amzn-requestid': '595c10d8-576a-4328-bec0-8a39b2c33d20'}, 'RetryAttempts': 0}}
[0m22:34:57.999935 [debug] [Thread-1 (]: Glue adapter: status = error
[0m22:34:58.000933 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_cow_table"} */
select * from sampledb.hudi_upsert_partitioned_cow_table limit 1 '''), AnalysisException: Table or view not found: sampledb.hudi_upsert_partitioned_cow_table; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_cow_table], [], false

[0m22:34:58.003867 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_cow_table"} */
select * from sampledb.hudi_upsert_partitioned_cow_table limit 1 '''), AnalysisException: Table or view not found: sampledb.hudi_upsert_partitioned_cow_table; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_cow_table], [], false

[0m22:34:58.004915 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_cow_table"} */
select * from sampledb.hudi_upsert_partitioned_cow_table limit 1 
[0m22:34:58.004915 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_partitioned_cow_table: ROLLBACK
[0m22:34:58.005932 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m22:34:58.005932 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_partitioned_cow_table: Close
[0m22:34:58.006934 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m22:34:58.007933 [debug] [Thread-1 (]: finished collecting timing info
[0m22:34:58.008926 [debug] [Thread-1 (]: Database Error in model hudi_upsert_partitioned_cow_table (models\metrics\hudi_upsert_partitioned_cow_table.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_cow_table"} */
  select * from sampledb.hudi_upsert_partitioned_cow_table limit 1 '''), AnalysisException: Table or view not found: sampledb.hudi_upsert_partitioned_cow_table; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_cow_table], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\hudi_upsert_partitioned_cow_table.sql
[0m22:34:58.008926 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e1a8adc-724f-4c63-8ab3-20ac7b80975b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E83249A0>]}
[0m22:34:58.009923 [error] [Thread-1 (]: 4 of 5 ERROR creating sql incremental model sampledb.hudi_upsert_partitioned_cow_table  [[31mERROR[0m in 6.04s]
[0m22:34:58.010862 [debug] [Thread-1 (]: Finished running node model.dbtglue.hudi_upsert_partitioned_cow_table
[0m22:34:58.010862 [debug] [Thread-1 (]: Began running node model.dbtglue.hudi_upsert_partitioned_mor_table
[0m22:34:58.011873 [info ] [Thread-1 (]: 5 of 5 START sql incremental model sampledb.hudi_upsert_partitioned_mor_table .. [RUN]
[0m22:34:58.012875 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.hudi_upsert_partitioned_mor_table"
[0m22:34:58.012875 [debug] [Thread-1 (]: Began compiling node model.dbtglue.hudi_upsert_partitioned_mor_table
[0m22:34:58.012875 [debug] [Thread-1 (]: Compiling model.dbtglue.hudi_upsert_partitioned_mor_table
[0m22:34:58.015868 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.hudi_upsert_partitioned_mor_table"
[0m22:34:58.016868 [debug] [Thread-1 (]: finished collecting timing info
[0m22:34:58.016868 [debug] [Thread-1 (]: Began executing node model.dbtglue.hudi_upsert_partitioned_mor_table
[0m22:34:58.018860 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m22:34:58.018860 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m22:34:58.492792 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m22:34:58.493805 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m22:34:58.493805 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9cd35a8c-690f-4d87-91a6-edbcea9747eb
[0m22:34:59.308569 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:59.847160 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table hudi_upsert_partitioned_mor_table not found.
[0m22:34:59.848156 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m22:34:59.848156 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.hudi_upsert_partitioned_mor_table"
[0m22:34:59.848156 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_partitioned_mor_table: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_mor_table"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m22:34:59.848156 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:34:59.849175 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:34:59.849175 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:34:59.849175 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:34:59.849175 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_mor_table"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m22:35:00.259807 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 26, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_mor_table"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 26, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671748500070, 'CompletedOn': 1671748500217}, 'ResponseMetadata': {'RequestId': '3cc2c2ee-4777-4e33-8992-00488c7a3059', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:35:00 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '724', 'connection': 'keep-alive', 'x-amzn-requestid': '3cc2c2ee-4777-4e33-8992-00488c7a3059'}, 'RetryAttempts': 0}}
[0m22:35:00.260881 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m22:35:00.260881 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m22:35:00.260881 [debug] [Thread-1 (]: SQL status: OK in 0.41 seconds
[0m22:35:00.307826 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:35:00.310826 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:35:01.580371 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table hudi_upsert_partitioned_mor_table not found.
[0m22:35:01.581393 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/*
 Example of an upsert for a partitioned merge on read table with incremental materialization using merge strategy.
 */


select id, name, current_timestamp() as ts, current_date as datestr
from sampledb.hudi_upsert_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if datestr is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.write.partitionpath.field': 'd,a,t,e,s,t,r', 'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor', 'hoodie.datasource.hive_sync.partition_fields': 'd,a,t,e,s,t,r','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")
        
spark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_mor_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_mor_table LIMIT 1""")
        
        
[0m22:35:01.581393 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:35:01.581393 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:35:01.581393 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:35:01.582372 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/*
 Example of an upsert for a partitioned merge on read table with incremental materialization using merge strategy.
 */


select id, name, current_timestamp() as ts, current_date as datestr
from sampledb.hudi_upsert_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if datestr is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.write.partitionpath.field': 'd,a,t,e,s,t,r', 'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor', 'hoodie.datasource.hive_sync.partition_fields': 'd,a,t,e,s,t,r','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")

spark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_mor_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_mor_table LIMIT 1""")

[0m22:35:03.827052 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 27, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * "License"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n *\n */\n\n/*\n Example of an upsert for a partitioned merge on read table with incremental materialization using merge strategy.\n */\n\n\nselect id, name, current_timestamp() as ts, current_date as datestr\nfrom sampledb.hudi_upsert_table""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if datestr is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'hudi_upsert_partitioned_mor_table\', \'hoodie.datasource.hive_sync.database\': \'sampledb\', \'hoodie.datasource.hive_sync.table\': \'hudi_upsert_partitioned_mor_table\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.write.partitionpath.field\': \'d,a,t,e,s,t,r\', \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.MultiPartKeysValueExtractor\', \'hoodie.datasource.hive_sync.partition_fields\': \'d,a,t,e,s,t,r\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'hudi_upsert_partitioned_mor_table\', \'hoodie.datasource.hive_sync.database\': \'sampledb\', \'hoodie.datasource.hive_sync.table\': \'hudi_upsert_partitioned_mor_table\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")\n\nspark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_mor_table""")\nSqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_mor_table LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 27, 'Status': 'error', 'ErrorName': 'NameError', 'ErrorValue': "name 'datestr' is not defined", 'Traceback': ['Traceback (most recent call last):\n', "NameError: name 'datestr' is not defined\n"]}, 'Progress': 1.0, 'StartedOn': 1671748502433, 'CompletedOn': 1671748503258}, 'ResponseMetadata': {'RequestId': 'e981201f-0871-44d2-b789-0b0b2522a2be', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:35:03 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4145', 'connection': 'keep-alive', 'x-amzn-requestid': 'e981201f-0871-44d2-b789-0b0b2522a2be'}, 'RetryAttempts': 0}}
[0m22:35:03.827052 [debug] [Thread-1 (]: Glue adapter: status = error
[0m22:35:03.827052 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/*
 Example of an upsert for a partitioned merge on read table with incremental materialization using merge strategy.
 */


select id, name, current_timestamp() as ts, current_date as datestr
from sampledb.hudi_upsert_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if datestr is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.write.partitionpath.field': 'd,a,t,e,s,t,r', 'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor', 'hoodie.datasource.hive_sync.partition_fields': 'd,a,t,e,s,t,r','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")

spark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_mor_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_mor_table LIMIT 1""")
, NameError: name 'datestr' is not defined
[0m22:35:03.830052 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 *
 */

/*
 Example of an upsert for a partitioned merge on read table with incremental materialization using merge strategy.
 */


select id, name, current_timestamp() as ts, current_date as datestr
from sampledb.hudi_upsert_table""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if datestr is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.write.partitionpath.field': 'd,a,t,e,s,t,r', 'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor', 'hoodie.datasource.hive_sync.partition_fields': 'd,a,t,e,s,t,r','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")

spark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_mor_table""")
SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_mor_table LIMIT 1""")
, NameError: name 'datestr' is not defined
[0m22:35:03.830052 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""/*
   * Licensed to the Apache Software Foundation (ASF) under one
   * or more contributor license agreements.  See the NOTICE file
   * distributed with this work for additional information
   * regarding copyright ownership.  The ASF licenses this file
   * to you under the Apache License, Version 2.0 (the
   * "License"); you may not use this file except in compliance
   * with the License.  You may obtain a copy of the License at
   *
   *   http://www.apache.org/licenses/LICENSE-2.0
   *
   * Unless required by applicable law or agreed to in writing,
   * software distributed under the License is distributed on an
   * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
   * KIND, either express or implied.  See the License for the
   * specific language governing permissions and limitations
   * under the License.
   *
   */
  
  /*
   Example of an upsert for a partitioned merge on read table with incremental materialization using merge strategy.
   */
  
  
  select id, name, current_timestamp() as ts, current_date as datestr
  from sampledb.hudi_upsert_table""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if datestr is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.write.partitionpath.field': 'd,a,t,e,s,t,r', 'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.MultiPartKeysValueExtractor', 'hoodie.datasource.hive_sync.partition_fields': 'd,a,t,e,s,t,r','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.database': 'sampledb', 'hoodie.datasource.hive_sync.table': 'hudi_upsert_partitioned_mor_table', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/sample//sampledb/hudi_upsert_partitioned_mor_table/")
  
  spark.sql("""REFRESH TABLE sampledb.hudi_upsert_partitioned_mor_table""")
  SqlWrapper2.execute("""SELECT * FROM sampledb.hudi_upsert_partitioned_mor_table LIMIT 1""")
  , NameError: name 'datestr' is not defined
[0m22:35:03.834052 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.hudi_upsert_partitioned_mor_table"
[0m22:35:03.834052 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.hudi_upsert_partitioned_mor_table"
[0m22:35:03.835054 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_partitioned_mor_table: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_mor_table"} */
select * from sampledb.hudi_upsert_partitioned_mor_table limit 1 
[0m22:35:03.835054 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m22:35:03.835054 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m22:35:03.835054 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m22:35:03.835054 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m22:35:03.836046 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_mor_table"} */
select * from sampledb.hudi_upsert_partitioned_mor_table limit 1 ''')
[0m22:35:05.435312 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 28, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_mor_table"} */\nselect * from sampledb.hudi_upsert_partitioned_mor_table limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 28, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: sampledb.hudi_upsert_partitioned_mor_table; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_mor_table], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: sampledb.hudi_upsert_partitioned_mor_table; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_mor_table], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671748504038, 'CompletedOn': 1671748504292}, 'ResponseMetadata': {'RequestId': 'ce670b9b-4ba6-4ec0-b027-e37111b8a36e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 22:35:05 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1554', 'connection': 'keep-alive', 'x-amzn-requestid': 'ce670b9b-4ba6-4ec0-b027-e37111b8a36e'}, 'RetryAttempts': 0}}
[0m22:35:05.436340 [debug] [Thread-1 (]: Glue adapter: status = error
[0m22:35:05.437336 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_mor_table"} */
select * from sampledb.hudi_upsert_partitioned_mor_table limit 1 '''), AnalysisException: Table or view not found: sampledb.hudi_upsert_partitioned_mor_table; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_mor_table], [], false

[0m22:35:05.439331 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_mor_table"} */
select * from sampledb.hudi_upsert_partitioned_mor_table limit 1 '''), AnalysisException: Table or view not found: sampledb.hudi_upsert_partitioned_mor_table; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_mor_table], [], false

[0m22:35:05.440262 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_mor_table"} */
select * from sampledb.hudi_upsert_partitioned_mor_table limit 1 
[0m22:35:05.440262 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_partitioned_mor_table: ROLLBACK
[0m22:35:05.441336 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m22:35:05.441336 [debug] [Thread-1 (]: On model.dbtglue.hudi_upsert_partitioned_mor_table: Close
[0m22:35:05.442267 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m22:35:05.443268 [debug] [Thread-1 (]: finished collecting timing info
[0m22:35:05.443268 [debug] [Thread-1 (]: Database Error in model hudi_upsert_partitioned_mor_table (models\metrics\hudi_upsert_partitioned_mor_table.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_mor_table"} */
  select * from sampledb.hudi_upsert_partitioned_mor_table limit 1 '''), AnalysisException: Table or view not found: sampledb.hudi_upsert_partitioned_mor_table; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_mor_table], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\hudi_upsert_partitioned_mor_table.sql
[0m22:35:05.443268 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0e1a8adc-724f-4c63-8ab3-20ac7b80975b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E6209C30>]}
[0m22:35:05.444260 [error] [Thread-1 (]: 5 of 5 ERROR creating sql incremental model sampledb.hudi_upsert_partitioned_mor_table  [[31mERROR[0m in 7.43s]
[0m22:35:05.445257 [debug] [Thread-1 (]: Finished running node model.dbtglue.hudi_upsert_partitioned_mor_table
[0m22:35:05.446258 [debug] [MainThread]: Acquiring new glue connection "master"
[0m22:35:05.447261 [debug] [MainThread]: On master: ROLLBACK
[0m22:35:05.447261 [debug] [MainThread]: Opening a new connection, currently in state init
[0m22:35:05.447846 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m22:35:06.246141 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m22:35:06.246141 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m22:35:06.247223 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9cd35a8c-690f-4d87-91a6-edbcea9747eb
[0m22:35:07.182245 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m22:35:07.183252 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m22:35:07.183252 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m22:35:07.183252 [debug] [MainThread]: On master: ROLLBACK
[0m22:35:07.183252 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m22:35:07.184244 [debug] [MainThread]: On master: Close
[0m22:35:07.184244 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m22:35:07.184244 [info ] [MainThread]: 
[0m22:35:07.185253 [info ] [MainThread]: Finished running 5 incremental models in 0 hours 2 minutes and 33.78 seconds (153.78s).
[0m22:35:07.185253 [debug] [MainThread]: Glue adapter: cleanup called
[0m22:35:07.456151 [info ] [MainThread]: 
[0m22:35:07.457483 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m22:35:07.458496 [info ] [MainThread]: 
[0m22:35:07.458496 [error] [MainThread]: [33mDatabase Error in model hudi_upsert_partitioned_cow_table (models\metrics\hudi_upsert_partitioned_cow_table.sql)[0m
[0m22:35:07.459494 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_cow_table"} */
[0m22:35:07.459494 [error] [MainThread]:   select * from sampledb.hudi_upsert_partitioned_cow_table limit 1 '''), AnalysisException: Table or view not found: sampledb.hudi_upsert_partitioned_cow_table; line 2 pos 14;
[0m22:35:07.460494 [error] [MainThread]:   'GlobalLimit 1
[0m22:35:07.460494 [error] [MainThread]:   +- 'LocalLimit 1
[0m22:35:07.461495 [error] [MainThread]:      +- 'Project [*]
[0m22:35:07.461495 [error] [MainThread]:         +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_cow_table], [], false
[0m22:35:07.462497 [error] [MainThread]:   
[0m22:35:07.462497 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\hudi_upsert_partitioned_cow_table.sql
[0m22:35:07.463497 [info ] [MainThread]: 
[0m22:35:07.463497 [error] [MainThread]: [33mDatabase Error in model hudi_upsert_partitioned_mor_table (models\metrics\hudi_upsert_partitioned_mor_table.sql)[0m
[0m22:35:07.464496 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.hudi_upsert_partitioned_mor_table"} */
[0m22:35:07.464496 [error] [MainThread]:   select * from sampledb.hudi_upsert_partitioned_mor_table limit 1 '''), AnalysisException: Table or view not found: sampledb.hudi_upsert_partitioned_mor_table; line 2 pos 14;
[0m22:35:07.464496 [error] [MainThread]:   'GlobalLimit 1
[0m22:35:07.465494 [error] [MainThread]:   +- 'LocalLimit 1
[0m22:35:07.465494 [error] [MainThread]:      +- 'Project [*]
[0m22:35:07.465494 [error] [MainThread]:         +- 'UnresolvedRelation [sampledb, hudi_upsert_partitioned_mor_table], [], false
[0m22:35:07.466494 [error] [MainThread]:   
[0m22:35:07.466494 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\hudi_upsert_partitioned_mor_table.sql
[0m22:35:07.466494 [info ] [MainThread]: 
[0m22:35:07.467496 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=2 SKIP=0 TOTAL=5
[0m22:35:07.467496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E6C1E6B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E84D2770>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000189E84D24D0>]}
[0m22:35:07.467496 [debug] [MainThread]: Flushing usage events
[0m22:35:07.998203 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 23:08:58.677499 | 8050224a-f478-4361-b515-f4e30258be18 ==============================
[0m23:08:58.677499 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:08:58.678502 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'clean', 'indirect_selection': 'eager'}
[0m23:08:58.679500 [debug] [MainThread]: Tracking: tracking
[0m23:08:58.703499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E87FD1570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E87FD10C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E87FD13C0>]}
[0m23:08:58.710501 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E87FD1450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E87FD1090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000013E87FD1030>]}
[0m23:08:58.711500 [debug] [MainThread]: Flushing usage events


============================== 2022-12-22 23:09:26.265256 | 4e26150e-7a8e-4a1c-856c-bb044cb6e52c ==============================
[0m23:09:26.265256 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:09:26.265256 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'config_dir': False, 'which': 'debug', 'indirect_selection': 'eager'}
[0m23:09:26.265256 [debug] [MainThread]: Tracking: tracking
[0m23:09:26.285262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1E0423BE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1E04226E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1E0423D30>]}
[0m23:09:26.442259 [debug] [MainThread]: Executing "git --help"
[0m23:09:26.504255 [debug] [MainThread]: STDOUT: "b"usage: git [-v | --version] [-h | --help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m23:09:26.505265 [debug] [MainThread]: STDERR: "b''"
[0m23:09:26.508254 [debug] [MainThread]: Acquiring new glue connection "debug"
[0m23:09:26.508254 [debug] [MainThread]: Using glue connection "debug"
[0m23:09:26.508254 [debug] [MainThread]: On debug: select 1 as id
[0m23:09:26.508254 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:09:26.509255 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m23:09:26.509255 [debug] [MainThread]: Glue adapter: No session present, starting one
[0m23:09:26.509255 [debug] [MainThread]: Glue adapter: GlueConnection _start_session called
[0m23:10:18.548564 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m23:10:18.549523 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-f125f1c7-e166-4281-b071-fae0487f5b52
[0m23:10:27.895985 [debug] [MainThread]: Glue adapter: GlueConnection cursor called
[0m23:10:27.895985 [debug] [MainThread]: Glue adapter: GlueCursor execute called
[0m23:10:27.896977 [debug] [MainThread]: Glue adapter: GlueCursor remove_comments_header called
[0m23:10:27.896977 [debug] [MainThread]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:10:27.896977 [debug] [MainThread]: Glue adapter: client : SqlWrapper2.execute('''select 1 as id''')
[0m23:10:46.259452 [debug] [MainThread]: Glue adapter: {'Statement': {'Id': 2, 'Code': "SqlWrapper2.execute('''select 1 as id''')", 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"id": 1}}], "description": [{"name": "id", "type": "IntegerType"}]}'}, 'ExecutionCount': 2, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671750628129, 'CompletedOn': 1671750645301}, 'ResponseMetadata': {'RequestId': 'e3710901-ab54-4627-b2c9-6d93c3e804ea', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:10:46 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '398', 'connection': 'keep-alive', 'x-amzn-requestid': 'e3710901-ab54-4627-b2c9-6d93c3e804ea'}, 'RetryAttempts': 0}}
[0m23:10:46.259452 [debug] [MainThread]: Glue adapter: status = ok
[0m23:10:46.260453 [debug] [MainThread]: Glue adapter: GlueCursor execute successfully
[0m23:10:46.260453 [debug] [MainThread]: SQL status: OK in 79.75 seconds
[0m23:10:46.260453 [debug] [MainThread]: On debug: Close
[0m23:10:46.261454 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m23:10:46.262454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1E09EADA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1E13DF7F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001D1E13DD810>]}
[0m23:10:46.262454 [debug] [MainThread]: Flushing usage events
[0m23:10:46.445225 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 23:11:13.799558 | d43cc0a3-a6a3-46e1-94d6-2020a6148830 ==============================
[0m23:11:13.799558 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:11:13.800552 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:11:13.800552 [debug] [MainThread]: Tracking: tracking
[0m23:11:13.823484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDD0FB4A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDCDEFBDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDD0FB4490>]}
[0m23:11:13.833541 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m23:11:13.833541 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'd43cc0a3-a6a3-46e1-94d6-2020a6148830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDD0F9F940>]}
[0m23:11:13.882484 [debug] [MainThread]: Parsing macros\adapters.sql
[0m23:11:13.903560 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m23:11:13.904555 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m23:11:13.904555 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m23:11:13.908490 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m23:11:13.919490 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m23:11:13.920494 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m23:11:13.927490 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m23:11:13.930490 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m23:11:13.932490 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m23:11:13.933490 [debug] [MainThread]: Parsing macros\adapters.sql
[0m23:11:13.964491 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m23:11:13.966492 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m23:11:13.973491 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m23:11:13.991492 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m23:11:13.995491 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m23:11:13.996494 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m23:11:13.998496 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m23:11:14.003491 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m23:11:14.009492 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m23:11:14.013493 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m23:11:14.014493 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m23:11:14.014493 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m23:11:14.015491 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m23:11:14.015491 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m23:11:14.016491 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m23:11:14.017495 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m23:11:14.017495 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m23:11:14.021492 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m23:11:14.030492 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m23:11:14.032491 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m23:11:14.033497 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m23:11:14.033497 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m23:11:14.043491 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m23:11:14.050492 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m23:11:14.052491 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m23:11:14.054491 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m23:11:14.059492 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m23:11:14.062491 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m23:11:14.073491 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m23:11:14.074491 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m23:11:14.077491 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m23:11:14.086491 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m23:11:14.090491 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m23:11:14.091492 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m23:11:14.092491 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m23:11:14.093491 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m23:11:14.093491 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m23:11:14.095491 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m23:11:14.096494 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m23:11:14.098491 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m23:11:14.099492 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m23:11:14.102491 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m23:11:14.107491 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m23:11:14.115484 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m23:11:14.116485 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m23:11:14.125484 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m23:11:14.135563 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m23:11:14.139541 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m23:11:14.141550 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m23:11:14.146547 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m23:11:14.149549 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m23:11:14.150495 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m23:11:14.151484 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m23:11:14.155485 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m23:11:14.166485 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m23:11:14.171543 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m23:11:14.181484 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m23:11:14.189493 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m23:11:14.191494 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m23:11:14.203484 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m23:11:14.204497 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m23:11:14.208499 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m23:11:14.209493 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m23:11:14.216491 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m23:11:14.217492 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m23:11:14.219492 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m23:11:14.220492 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m23:11:14.222492 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m23:11:14.223486 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m23:11:14.224484 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m23:11:14.226499 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m23:11:14.231647 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m23:11:14.232648 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m23:11:14.234648 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m23:11:14.235647 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m23:11:14.237655 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m23:11:14.238651 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m23:11:14.239641 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m23:11:14.239641 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m23:11:14.241649 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m23:11:14.243640 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m23:11:14.244649 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m23:11:14.245642 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m23:11:14.246641 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m23:11:14.247640 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m23:11:14.248640 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m23:11:14.249648 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m23:11:14.251640 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m23:11:14.565610 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m23:11:14.654469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd43cc0a3-a6a3-46e1-94d6-2020a6148830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDD10CD480>]}
[0m23:11:14.662480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd43cc0a3-a6a3-46e1-94d6-2020a6148830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDD10CF400>]}
[0m23:11:14.662480 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:11:14.663476 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd43cc0a3-a6a3-46e1-94d6-2020a6148830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDD11152D0>]}
[0m23:11:14.664467 [info ] [MainThread]: 
[0m23:11:14.665466 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:11:14.667468 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m23:11:14.667468 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:11:14.667468 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:11:14.668551 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m23:11:14.668551 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m23:11:41.489513 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:11:41.489513 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-c9619da3-b26d-4dac-9b42-cf4c677c9ac8
[0m23:11:48.483288 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:11:49.070754 [debug] [ThreadPool]: On list_hudidb: Close
[0m23:11:49.070754 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:11:49.072391 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m23:11:49.072391 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:11:49.072391 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:11:49.828710 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m23:11:49.829699 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:11:49.829699 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-c9619da3-b26d-4dac-9b42-cf4c677c9ac8
[0m23:11:51.853635 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:11:52.381032 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m23:11:52.382033 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:11:52.384035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd43cc0a3-a6a3-46e1-94d6-2020a6148830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDD0FB6710>]}
[0m23:11:52.384035 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:11:52.384035 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:11:52.385028 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:11:52.386045 [info ] [MainThread]: 
[0m23:11:52.390044 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m23:11:52.391118 [info ] [Thread-1 (]: 1 of 1 START sql table model hudidb.cleansed_order ............................. [RUN]
[0m23:11:52.391118 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m23:11:52.392111 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m23:11:52.392111 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m23:11:52.395063 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m23:11:52.395658 [debug] [Thread-1 (]: finished collecting timing info
[0m23:11:52.396663 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m23:11:52.403666 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:11:52.403666 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m23:11:52.936676 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m23:11:52.937675 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m23:11:52.938675 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-c9619da3-b26d-4dac-9b42-cf4c677c9ac8
[0m23:11:54.051613 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:11:54.565674 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:11:54.601670 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:11:54.604600 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m23:11:54.606601 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m23:11:54.606601 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:11:54.607609 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid as invoice_id,
        category as category,
        destinationstate as state
        itemid as item_id
    FROM 

)

select
    *
from source_data
where invoiceid is not null
  
[0m23:11:54.607609 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:11:54.607609 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:11:54.607609 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:11:54.607609 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:11:54.608608 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid as invoice_id,
        category as category,
        destinationstate as state
        itemid as item_id
    FROM 

)

select
    *
from source_data
where invoiceid is not null
  ''')
[0m23:11:54.995808 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n  \n    \n    \tcreate table hudidb.cleansed_order\n    \n    \n    using hudi\n    \n    \n    \n    LOCATION \'s3://soumil-dms-learn/hudi//hudidb/cleansed_order/\'\n    \n\tas\n\t\n\n\nwith source_data as (\n    SELECT\n        invoiceid as invoice_id,\n        category as category,\n        destinationstate as state\n        itemid as item_id\n    FROM \n\n)\n\nselect\n    *\nfrom source_data\nwhere invoiceid is not null\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 6, 'Status': 'error', 'ErrorName': 'ParseException', 'ErrorValue': '\nmismatched input \'itemid\' expecting {\')\', \',\', \'CLUSTER\', \'DISTRIBUTE\', \'EXCEPT\', \'FROM\', \'GROUP\', \'HAVING\', \'INTERSECT\', \'LATERAL\', \'LIMIT\', \'ORDER\', \'MINUS\', \'SORT\', \'UNION\', \'WHERE\', \'WINDOW\', \'-\'}(line 23, pos 8)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n  \n    \n    \tcreate table hudidb.cleansed_order\n    \n    \n    using hudi\n    \n    \n    \n    LOCATION \'s3://soumil-dms-learn/hudi//hudidb/cleansed_order/\'\n    \n\tas\n\t\n\n\nwith source_data as (\n    SELECT\n        invoiceid as invoice_id,\n        category as category,\n        destinationstate as state\n        itemid as item_id\n--------^^^\n    FROM \n\n)\n\nselect\n    *\nfrom source_data\nwhere invoiceid is not null\n  \n', 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', 'pyspark.sql.utils.ParseException: \nmismatched input \'itemid\' expecting {\')\', \',\', \'CLUSTER\', \'DISTRIBUTE\', \'EXCEPT\', \'FROM\', \'GROUP\', \'HAVING\', \'INTERSECT\', \'LATERAL\', \'LIMIT\', \'ORDER\', \'MINUS\', \'SORT\', \'UNION\', \'WHERE\', \'WINDOW\', \'-\'}(line 23, pos 8)\n\n== SQL ==\n/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n  \n    \n    \tcreate table hudidb.cleansed_order\n    \n    \n    using hudi\n    \n    \n    \n    LOCATION \'s3://soumil-dms-learn/hudi//hudidb/cleansed_order/\'\n    \n\tas\n\t\n\n\nwith source_data as (\n    SELECT\n        invoiceid as invoice_id,\n        category as category,\n        destinationstate as state\n        itemid as item_id\n--------^^^\n    FROM \n\n)\n\nselect\n    *\nfrom source_data\nwhere invoiceid is not null\n  \n\n']}, 'Progress': 1.0, 'StartedOn': 1671750714828, 'CompletedOn': 1671750714919}, 'ResponseMetadata': {'RequestId': 'a4615b40-6fc5-4f26-9312-ab160533b79d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:11:55 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3115', 'connection': 'keep-alive', 'x-amzn-requestid': 'a4615b40-6fc5-4f26-9312-ab160533b79d'}, 'RetryAttempts': 0}}
[0m23:11:54.996878 [debug] [Thread-1 (]: Glue adapter: status = error
[0m23:11:54.996878 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid as invoice_id,
        category as category,
        destinationstate as state
        itemid as item_id
    FROM 

)

select
    *
from source_data
where invoiceid is not null
  '''), ParseException: 
mismatched input 'itemid' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 23, pos 8)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid as invoice_id,
        category as category,
        destinationstate as state
        itemid as item_id
--------^^^
    FROM 

)

select
    *
from source_data
where invoiceid is not null
  

[0m23:11:55.000807 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid as invoice_id,
        category as category,
        destinationstate as state
        itemid as item_id
    FROM 

)

select
    *
from source_data
where invoiceid is not null
  '''), ParseException: 
mismatched input 'itemid' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 23, pos 8)

== SQL ==
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid as invoice_id,
        category as category,
        destinationstate as state
        itemid as item_id
--------^^^
    FROM 

)

select
    *
from source_data
where invoiceid is not null
  

[0m23:11:55.000807 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid as invoice_id,
        category as category,
        destinationstate as state
        itemid as item_id
    FROM 

)

select
    *
from source_data
where invoiceid is not null
  
[0m23:11:55.001873 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m23:11:55.001873 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m23:11:55.001873 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m23:11:55.001873 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m23:11:55.001873 [debug] [Thread-1 (]: finished collecting timing info
[0m23:11:55.002875 [debug] [Thread-1 (]: Database Error in model cleansed_order (models\metrics\cleansed_order.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
  
    
      
      	create table hudidb.cleansed_order
      
      
      using hudi
      
      
      
      LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
      
  	as
  	
  
  
  with source_data as (
      SELECT
          invoiceid as invoice_id,
          category as category,
          destinationstate as state
          itemid as item_id
      FROM 
  
  )
  
  select
      *
  from source_data
  where invoiceid is not null
    '''), ParseException: 
  mismatched input 'itemid' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 23, pos 8)
  
  == SQL ==
  /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
  
    
      
      	create table hudidb.cleansed_order
      
      
      using hudi
      
      
      
      LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
      
  	as
  	
  
  
  with source_data as (
      SELECT
          invoiceid as invoice_id,
          category as category,
          destinationstate as state
          itemid as item_id
  --------^^^
      FROM 
  
  )
  
  select
      *
  from source_data
  where invoiceid is not null
    
  
  compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:11:55.002875 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd43cc0a3-a6a3-46e1-94d6-2020a6148830', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDD1D07D30>]}
[0m23:11:55.003871 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model hudidb.cleansed_order .................... [[31mERROR[0m in 2.61s]
[0m23:11:55.004807 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m23:11:55.005807 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:11:55.005807 [debug] [MainThread]: On master: ROLLBACK
[0m23:11:55.005807 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:11:55.006808 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m23:11:55.491290 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m23:11:55.491290 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m23:11:55.491290 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-c9619da3-b26d-4dac-9b42-cf4c677c9ac8
[0m23:11:56.285820 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:11:56.285820 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:11:56.286818 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:11:56.286818 [debug] [MainThread]: On master: ROLLBACK
[0m23:11:56.286818 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:11:56.286818 [debug] [MainThread]: On master: Close
[0m23:11:56.286818 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m23:11:56.287908 [info ] [MainThread]: 
[0m23:11:56.287908 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 41.62 seconds (41.62s).
[0m23:11:56.288819 [debug] [MainThread]: Glue adapter: cleanup called
[0m23:11:56.551310 [info ] [MainThread]: 
[0m23:11:56.552310 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:11:56.553310 [info ] [MainThread]: 
[0m23:11:56.554310 [error] [MainThread]: [33mDatabase Error in model cleansed_order (models\metrics\cleansed_order.sql)[0m
[0m23:11:56.554310 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
[0m23:11:56.555309 [error] [MainThread]:   
[0m23:11:56.555309 [error] [MainThread]:     
[0m23:11:56.556316 [error] [MainThread]:       
[0m23:11:56.557309 [error] [MainThread]:       	create table hudidb.cleansed_order
[0m23:11:56.557309 [error] [MainThread]:       
[0m23:11:56.557309 [error] [MainThread]:       
[0m23:11:56.558319 [error] [MainThread]:       using hudi
[0m23:11:56.558319 [error] [MainThread]:       
[0m23:11:56.559309 [error] [MainThread]:       
[0m23:11:56.559309 [error] [MainThread]:       
[0m23:11:56.559309 [error] [MainThread]:       LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
[0m23:11:56.560309 [error] [MainThread]:       
[0m23:11:56.560309 [error] [MainThread]:   	as
[0m23:11:56.561309 [error] [MainThread]:   	
[0m23:11:56.561309 [error] [MainThread]:   
[0m23:11:56.562309 [error] [MainThread]:   
[0m23:11:56.562309 [error] [MainThread]:   with source_data as (
[0m23:11:56.562309 [error] [MainThread]:       SELECT
[0m23:11:56.563309 [error] [MainThread]:           invoiceid as invoice_id,
[0m23:11:56.563309 [error] [MainThread]:           category as category,
[0m23:11:56.564309 [error] [MainThread]:           destinationstate as state
[0m23:11:56.564309 [error] [MainThread]:           itemid as item_id
[0m23:11:56.565313 [error] [MainThread]:       FROM 
[0m23:11:56.565313 [error] [MainThread]:   
[0m23:11:56.566311 [error] [MainThread]:   )
[0m23:11:56.566311 [error] [MainThread]:   
[0m23:11:56.567310 [error] [MainThread]:   select
[0m23:11:56.567310 [error] [MainThread]:       *
[0m23:11:56.568310 [error] [MainThread]:   from source_data
[0m23:11:56.568310 [error] [MainThread]:   where invoiceid is not null
[0m23:11:56.569312 [error] [MainThread]:     '''), ParseException: 
[0m23:11:56.569312 [error] [MainThread]:   mismatched input 'itemid' expecting {')', ',', 'CLUSTER', 'DISTRIBUTE', 'EXCEPT', 'FROM', 'GROUP', 'HAVING', 'INTERSECT', 'LATERAL', 'LIMIT', 'ORDER', 'MINUS', 'SORT', 'UNION', 'WHERE', 'WINDOW', '-'}(line 23, pos 8)
[0m23:11:56.570310 [error] [MainThread]:   
[0m23:11:56.570310 [error] [MainThread]:   == SQL ==
[0m23:11:56.570310 [error] [MainThread]:   /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
[0m23:11:56.571311 [error] [MainThread]:   
[0m23:11:56.571311 [error] [MainThread]:     
[0m23:11:56.572311 [error] [MainThread]:       
[0m23:11:56.572311 [error] [MainThread]:       	create table hudidb.cleansed_order
[0m23:11:56.573311 [error] [MainThread]:       
[0m23:11:56.573311 [error] [MainThread]:       
[0m23:11:56.574311 [error] [MainThread]:       using hudi
[0m23:11:56.574311 [error] [MainThread]:       
[0m23:11:56.574311 [error] [MainThread]:       
[0m23:11:56.575310 [error] [MainThread]:       
[0m23:11:56.575310 [error] [MainThread]:       LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
[0m23:11:56.576309 [error] [MainThread]:       
[0m23:11:56.576309 [error] [MainThread]:   	as
[0m23:11:56.576309 [error] [MainThread]:   	
[0m23:11:56.577311 [error] [MainThread]:   
[0m23:11:56.577311 [error] [MainThread]:   
[0m23:11:56.577311 [error] [MainThread]:   with source_data as (
[0m23:11:56.578309 [error] [MainThread]:       SELECT
[0m23:11:56.578309 [error] [MainThread]:           invoiceid as invoice_id,
[0m23:11:56.579309 [error] [MainThread]:           category as category,
[0m23:11:56.579309 [error] [MainThread]:           destinationstate as state
[0m23:11:56.579309 [error] [MainThread]:           itemid as item_id
[0m23:11:56.580310 [error] [MainThread]:   --------^^^
[0m23:11:56.580310 [error] [MainThread]:       FROM 
[0m23:11:56.581310 [error] [MainThread]:   
[0m23:11:56.581310 [error] [MainThread]:   )
[0m23:11:56.582309 [error] [MainThread]:   
[0m23:11:56.582309 [error] [MainThread]:   select
[0m23:11:56.582309 [error] [MainThread]:       *
[0m23:11:56.583309 [error] [MainThread]:   from source_data
[0m23:11:56.583309 [error] [MainThread]:   where invoiceid is not null
[0m23:11:56.584309 [error] [MainThread]:     
[0m23:11:56.584309 [error] [MainThread]:   
[0m23:11:56.585311 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:11:56.585311 [info ] [MainThread]: 
[0m23:11:56.586310 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:11:56.586310 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDD0FB5060>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDD1CAA110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001BDD1CCAD70>]}
[0m23:11:56.586310 [debug] [MainThread]: Flushing usage events
[0m23:11:56.754988 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 23:22:34.495622 | 2545239a-af1d-4895-b101-2d3c63966ddb ==============================
[0m23:22:34.495622 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:22:34.497615 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:22:34.497615 [debug] [MainThread]: Tracking: tracking
[0m23:22:34.525614 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801E9BD540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801E9BD8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801E9BD690>]}
[0m23:22:34.628776 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:22:34.629768 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m23:22:34.643774 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m23:22:34.686760 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2545239a-af1d-4895-b101-2d3c63966ddb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801BA4DCF0>]}
[0m23:22:34.693771 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2545239a-af1d-4895-b101-2d3c63966ddb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801EB7FFD0>]}
[0m23:22:34.693771 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:22:34.694768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2545239a-af1d-4895-b101-2d3c63966ddb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801EB66830>]}
[0m23:22:34.695707 [info ] [MainThread]: 
[0m23:22:34.697699 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:22:34.698742 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m23:22:34.698742 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:22:34.699734 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:22:34.699734 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m23:22:34.699734 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m23:23:14.279040 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:23:14.280037 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-bb54cffa-9513-4efc-9fed-d71284823b37
[0m23:23:21.257675 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:23:22.071422 [debug] [ThreadPool]: On list_hudidb: Close
[0m23:23:22.072422 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:23:22.074422 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m23:23:22.074422 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:23:22.074422 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:23:22.558417 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m23:23:22.558417 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:23:22.558417 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-bb54cffa-9513-4efc-9fed-d71284823b37
[0m23:23:23.363113 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:23:23.903801 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m23:23:23.903801 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:23:23.904757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2545239a-af1d-4895-b101-2d3c63966ddb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801EBA77C0>]}
[0m23:23:23.904757 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:23:23.904757 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:23:23.905811 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:23:23.906482 [info ] [MainThread]: 
[0m23:23:23.911051 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m23:23:23.912062 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m23:23:23.912062 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m23:23:23.913051 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m23:23:23.913051 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m23:23:23.915057 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m23:23:23.916057 [debug] [Thread-1 (]: finished collecting timing info
[0m23:23:23.916057 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m23:23:23.936057 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:23:23.937050 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m23:23:24.397296 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m23:23:24.398303 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m23:23:24.398303 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-bb54cffa-9513-4efc-9fed-d71284823b37
[0m23:23:25.258880 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:23:26.100063 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:23:26.107063 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m23:23:26.107063 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:23:26.107063 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m23:23:26.108064 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:23:26.108064 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:23:26.108064 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:23:26.108064 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:23:26.108064 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m23:23:46.923206 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671751406354, 'CompletedOn': 1671751426335}, 'ResponseMetadata': {'RequestId': '2d9a983f-cbd5-49a3-a7aa-1f2bdb7ca4a2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:23:46 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': '2d9a983f-cbd5-49a3-a7aa-1f2bdb7ca4a2'}, 'RetryAttempts': 0}}
[0m23:23:46.924145 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m23:23:46.924145 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m23:23:46.925270 [debug] [Thread-1 (]: SQL status: OK in 20.82 seconds
[0m23:23:46.929193 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:23:46.931192 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:23:47.506685 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:23:47.507685 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    Where invoice_id is not NULL
)

select
    *
from source_data
where invoiceid is not null""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m23:23:47.507685 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:23:47.507685 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:23:47.507685 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:23:47.508685 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    Where invoice_id is not NULL
)

select
    *
from source_data
where invoiceid is not null""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m23:23:49.045273 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    Where invoice_id is not NULL\n)\n\nselect\n    *\nfrom source_data\nwhere invoiceid is not null""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 7, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "cannot resolve '`invoice_id`' given input columns: []; line 10 pos 10;\n'Project [*]\n+- 'Filter isnotnull('invoiceid)\n   +- 'SubqueryAlias source_data\n      +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]\n         +- 'Filter isnotnull('invoice_id)\n            +- OneRowRelation\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: cannot resolve '`invoice_id`' given input columns: []; line 10 pos 10;\n'Project [*]\n+- 'Filter isnotnull('invoiceid)\n   +- 'SubqueryAlias source_data\n      +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]\n         +- 'Filter isnotnull('invoice_id)\n            +- OneRowRelation\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671751427719, 'CompletedOn': 1671751427964}, 'ResponseMetadata': {'RequestId': 'f40b85e4-3eb3-4777-9466-98f49b407d1e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:23:49 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3847', 'connection': 'keep-alive', 'x-amzn-requestid': 'f40b85e4-3eb3-4777-9466-98f49b407d1e'}, 'RetryAttempts': 0}}
[0m23:23:49.046269 [debug] [Thread-1 (]: Glue adapter: status = error
[0m23:23:49.046269 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    Where invoice_id is not NULL
)

select
    *
from source_data
where invoiceid is not null""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
, AnalysisException: cannot resolve '`invoice_id`' given input columns: []; line 10 pos 10;
'Project [*]
+- 'Filter isnotnull('invoiceid)
   +- 'SubqueryAlias source_data
      +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
         +- 'Filter isnotnull('invoice_id)
            +- OneRowRelation

[0m23:23:49.049260 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    Where invoice_id is not NULL
)

select
    *
from source_data
where invoiceid is not null""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
, AnalysisException: cannot resolve '`invoice_id`' given input columns: []; line 10 pos 10;
'Project [*]
+- 'Filter isnotnull('invoiceid)
   +- 'SubqueryAlias source_data
      +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
         +- 'Filter isnotnull('invoice_id)
            +- OneRowRelation

[0m23:23:49.049260 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""
  
  
  with source_data as (
      SELECT
          invoiceid ,
          category ,
          destinationstate,
          itemid
      Where invoice_id is not NULL
  )
  
  select
      *
  from source_data
  where invoiceid is not null""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if None is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
  
  spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
  SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
  , AnalysisException: cannot resolve '`invoice_id`' given input columns: []; line 10 pos 10;
  'Project [*]
  +- 'Filter isnotnull('invoiceid)
     +- 'SubqueryAlias source_data
        +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
           +- 'Filter isnotnull('invoice_id)
              +- OneRowRelation
  
[0m23:23:49.052260 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m23:23:49.052260 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:23:49.053269 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m23:23:49.053269 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:23:49.053269 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:23:49.053269 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:23:49.053269 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:23:49.054267 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m23:23:50.659597 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 8, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: hudidb.cleansed_order; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671751429296, 'CompletedOn': 1671751429746}, 'ResponseMetadata': {'RequestId': '089311e1-a9a5-4cf6-a48b-cf2bb15a4cd4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:23:50 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1428', 'connection': 'keep-alive', 'x-amzn-requestid': '089311e1-a9a5-4cf6-a48b-cf2bb15a4cd4'}, 'RetryAttempts': 0}}
[0m23:23:50.659597 [debug] [Thread-1 (]: Glue adapter: status = error
[0m23:23:50.660598 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false

[0m23:23:50.660598 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false

[0m23:23:50.661597 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m23:23:50.661597 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m23:23:50.661597 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m23:23:50.661597 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m23:23:50.661597 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m23:23:50.662598 [debug] [Thread-1 (]: finished collecting timing info
[0m23:23:50.662598 [debug] [Thread-1 (]: Database Error in model cleansed_order (models\metrics\cleansed_order.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
  select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [hudidb, cleansed_order], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:23:50.662598 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2545239a-af1d-4895-b101-2d3c63966ddb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801F762560>]}
[0m23:23:50.663598 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model hudidb.cleansed_order .............. [[31mERROR[0m in 26.75s]
[0m23:23:50.664596 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m23:23:50.665596 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:23:50.665596 [debug] [MainThread]: On master: ROLLBACK
[0m23:23:50.665596 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:23:50.666594 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m23:23:51.151355 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m23:23:51.151355 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m23:23:51.152433 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-bb54cffa-9513-4efc-9fed-d71284823b37
[0m23:23:51.987505 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:23:51.987505 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:23:51.988505 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:23:51.988505 [debug] [MainThread]: On master: ROLLBACK
[0m23:23:51.988505 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:23:51.988505 [debug] [MainThread]: On master: Close
[0m23:23:51.988505 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m23:23:51.989505 [info ] [MainThread]: 
[0m23:23:51.989505 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 17.29 seconds (77.29s).
[0m23:23:51.990505 [debug] [MainThread]: Glue adapter: cleanup called
[0m23:23:52.263721 [info ] [MainThread]: 
[0m23:23:52.264721 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:23:52.264721 [info ] [MainThread]: 
[0m23:23:52.265720 [error] [MainThread]: [33mDatabase Error in model cleansed_order (models\metrics\cleansed_order.sql)[0m
[0m23:23:52.265720 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
[0m23:23:52.266720 [error] [MainThread]:   select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
[0m23:23:52.266720 [error] [MainThread]:   'GlobalLimit 1
[0m23:23:52.267720 [error] [MainThread]:   +- 'LocalLimit 1
[0m23:23:52.267720 [error] [MainThread]:      +- 'Project [*]
[0m23:23:52.268730 [error] [MainThread]:         +- 'UnresolvedRelation [hudidb, cleansed_order], [], false
[0m23:23:52.268730 [error] [MainThread]:   
[0m23:23:52.268730 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:23:52.269720 [info ] [MainThread]: 
[0m23:23:52.269720 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:23:52.270719 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801EBA7550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801F7B2C50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002801F7E0880>]}
[0m23:23:52.270719 [debug] [MainThread]: Flushing usage events
[0m23:23:52.458267 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 23:25:41.685020 | 21cb2495-3844-4bbd-95e9-af025025731a ==============================
[0m23:25:41.685020 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:25:41.686028 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:25:41.686028 [debug] [MainThread]: Tracking: tracking
[0m23:25:41.709029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012E834ED540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012E834ED8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012E834ED690>]}
[0m23:25:41.787025 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:25:41.788021 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m23:25:41.800014 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m23:25:41.839020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '21cb2495-3844-4bbd-95e9-af025025731a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012E8064DCF0>]}
[0m23:25:41.846025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '21cb2495-3844-4bbd-95e9-af025025731a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012E836AFFD0>]}
[0m23:25:41.846025 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:25:41.847020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '21cb2495-3844-4bbd-95e9-af025025731a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012E83696830>]}
[0m23:25:41.848029 [info ] [MainThread]: 
[0m23:25:41.849021 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:25:41.851015 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m23:25:41.851015 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:25:41.851015 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:25:41.852027 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m23:25:41.852027 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called


============================== 2022-12-22 23:26:09.527315 | d13334a2-18a5-4988-863a-6b6a72dc74f8 ==============================
[0m23:26:09.527315 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:26:09.528314 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:26:09.529307 [debug] [MainThread]: Tracking: tracking
[0m23:26:09.554316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3EFD4A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3BF1BDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3EFD4490>]}
[0m23:26:09.642940 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:26:09.642940 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m23:26:09.654941 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m23:26:09.696941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd13334a2-18a5-4988-863a-6b6a72dc74f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3C02E3E0>]}
[0m23:26:09.703940 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd13334a2-18a5-4988-863a-6b6a72dc74f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3F17FF70>]}
[0m23:26:09.703940 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:26:09.704941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd13334a2-18a5-4988-863a-6b6a72dc74f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3BF1BDF0>]}
[0m23:26:09.705936 [info ] [MainThread]: 
[0m23:26:09.706935 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:26:09.708934 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m23:26:09.708934 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:26:09.708934 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:26:09.708934 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m23:26:09.709942 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m23:26:49.489322 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:26:49.489322 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-f4dfb597-bfbb-4d23-9c5a-fdf979933904
[0m23:26:57.601472 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:26:58.180882 [debug] [ThreadPool]: On list_hudidb: Close
[0m23:26:58.181882 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:26:58.183878 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m23:26:58.183878 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:26:58.183878 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:26:58.662403 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m23:26:58.663487 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:26:58.663487 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-f4dfb597-bfbb-4d23-9c5a-fdf979933904
[0m23:26:59.800575 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:27:00.342333 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m23:27:00.342333 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:27:00.343458 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd13334a2-18a5-4988-863a-6b6a72dc74f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3F1AB7C0>]}
[0m23:27:00.343458 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:27:00.343458 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:27:00.344465 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:27:00.344465 [info ] [MainThread]: 
[0m23:27:00.350468 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m23:27:00.350468 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m23:27:00.351465 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m23:27:00.351465 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m23:27:00.351465 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m23:27:00.355466 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m23:27:00.355466 [debug] [Thread-1 (]: finished collecting timing info
[0m23:27:00.356466 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m23:27:00.375465 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:27:00.376466 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m23:27:00.846531 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m23:27:00.847544 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m23:27:00.847544 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-f4dfb597-bfbb-4d23-9c5a-fdf979933904
[0m23:27:01.634049 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:27:02.305148 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:27:02.311143 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m23:27:02.312139 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:27:02.312139 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m23:27:02.312139 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:27:02.312139 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:27:02.312139 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:27:02.313154 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:27:02.313154 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m23:27:24.582819 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671751622545, 'CompletedOn': 1671751643773}, 'ResponseMetadata': {'RequestId': 'b219945a-1ba3-4c78-9227-2e5761b313ea', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:27:24 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': 'b219945a-1ba3-4c78-9227-2e5761b313ea'}, 'RetryAttempts': 0}}
[0m23:27:24.582819 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m23:27:24.583820 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m23:27:24.583820 [debug] [Thread-1 (]: SQL status: OK in 22.27 seconds
[0m23:27:24.590820 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:27:24.595820 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:27:25.119452 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:27:25.120447 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid

)

select * from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m23:27:25.120447 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:27:25.121448 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:27:25.121448 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:27:25.121448 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid

)

select * from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m23:27:25.924671 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n\n)\n\nselect * from source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 7, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "cannot resolve '`invoiceid`' given input columns: []; line 6 pos 8;\n'Project [*]\n+- 'SubqueryAlias source_data\n   +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]\n      +- OneRowRelation\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 6 pos 8;\n'Project [*]\n+- 'SubqueryAlias source_data\n   +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]\n      +- OneRowRelation\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671751645761, 'CompletedOn': 1671751645915}, 'ResponseMetadata': {'RequestId': 'd9f1cc6e-a633-4090-9a6d-703d064eb6ef', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:27:25 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3608', 'connection': 'keep-alive', 'x-amzn-requestid': 'd9f1cc6e-a633-4090-9a6d-703d064eb6ef'}, 'RetryAttempts': 0}}
[0m23:27:25.925652 [debug] [Thread-1 (]: Glue adapter: status = error
[0m23:27:25.925652 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid

)

select * from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
, AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 6 pos 8;
'Project [*]
+- 'SubqueryAlias source_data
   +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
      +- OneRowRelation

[0m23:27:25.927592 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid

)

select * from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
, AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 6 pos 8;
'Project [*]
+- 'SubqueryAlias source_data
   +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
      +- OneRowRelation

[0m23:27:25.927592 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""
  
  
  with source_data as (
      SELECT
          invoiceid ,
          category ,
          destinationstate,
          itemid
  
  )
  
  select * from source_data""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if None is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
  
  spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
  SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
  , AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 6 pos 8;
  'Project [*]
  +- 'SubqueryAlias source_data
     +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
        +- OneRowRelation
  
[0m23:27:25.930661 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m23:27:25.931676 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:27:25.931676 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m23:27:25.932687 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:27:25.932687 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:27:25.932687 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:27:25.932687 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:27:25.932687 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m23:27:27.837271 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 8, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: hudidb.cleansed_order; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671751646179, 'CompletedOn': 1671751646735}, 'ResponseMetadata': {'RequestId': 'e31848bb-533b-4430-aa4f-3122c73f74e7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:27:27 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1428', 'connection': 'keep-alive', 'x-amzn-requestid': 'e31848bb-533b-4430-aa4f-3122c73f74e7'}, 'RetryAttempts': 0}}
[0m23:27:27.837271 [debug] [Thread-1 (]: Glue adapter: status = error
[0m23:27:27.838277 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false

[0m23:27:27.839270 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false

[0m23:27:27.839270 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m23:27:27.839270 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m23:27:27.840278 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m23:27:27.840278 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m23:27:27.840278 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m23:27:27.840278 [debug] [Thread-1 (]: finished collecting timing info
[0m23:27:27.841270 [debug] [Thread-1 (]: Database Error in model cleansed_order (models\metrics\cleansed_order.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
  select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [hudidb, cleansed_order], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:27:27.841270 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd13334a2-18a5-4988-863a-6b6a72dc74f8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3FDABE20>]}
[0m23:27:27.842277 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model hudidb.cleansed_order .............. [[31mERROR[0m in 27.49s]
[0m23:27:27.843271 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m23:27:27.844270 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:27:27.845270 [debug] [MainThread]: On master: ROLLBACK
[0m23:27:27.845270 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:27:27.846269 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m23:27:28.303519 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m23:27:28.303519 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m23:27:28.304514 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-f4dfb597-bfbb-4d23-9c5a-fdf979933904
[0m23:27:29.490445 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:27:29.491445 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:27:29.491445 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:27:29.492446 [debug] [MainThread]: On master: ROLLBACK
[0m23:27:29.492446 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:27:29.492446 [debug] [MainThread]: On master: Close
[0m23:27:29.492446 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m23:27:29.493446 [info ] [MainThread]: 
[0m23:27:29.494445 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 19.79 seconds (79.79s).
[0m23:27:29.495445 [debug] [MainThread]: Glue adapter: cleanup called
[0m23:27:29.763611 [info ] [MainThread]: 
[0m23:27:29.764611 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:27:29.766604 [info ] [MainThread]: 
[0m23:27:29.766604 [error] [MainThread]: [33mDatabase Error in model cleansed_order (models\metrics\cleansed_order.sql)[0m
[0m23:27:29.767604 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
[0m23:27:29.768604 [error] [MainThread]:   select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
[0m23:27:29.769606 [error] [MainThread]:   'GlobalLimit 1
[0m23:27:29.769606 [error] [MainThread]:   +- 'LocalLimit 1
[0m23:27:29.770613 [error] [MainThread]:      +- 'Project [*]
[0m23:27:29.770613 [error] [MainThread]:         +- 'UnresolvedRelation [hudidb, cleansed_order], [], false
[0m23:27:29.771613 [error] [MainThread]:   
[0m23:27:29.771613 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:27:29.772604 [info ] [MainThread]: 
[0m23:27:29.772604 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:27:29.773604 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3F1AAEC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3FE3A200>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001EF3FE3A1D0>]}
[0m23:27:29.773604 [debug] [MainThread]: Flushing usage events
[0m23:27:30.531502 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 23:28:53.263953 | cbd5ab5e-ae05-494a-a510-0aa193e38c90 ==============================
[0m23:28:53.263953 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:28:53.264964 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:28:53.264964 [debug] [MainThread]: Tracking: tracking
[0m23:28:53.287953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E7D25D540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E7D25D9F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E7D25D630>]}
[0m23:28:53.369955 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:28:53.370957 [debug] [MainThread]: Partial parsing: update schema file: dbtglue://models\metrics\schema.yml
[0m23:28:53.383954 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m23:28:53.419960 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cbd5ab5e-ae05-494a-a510-0aa193e38c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E7D43D600>]}
[0m23:28:53.425963 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cbd5ab5e-ae05-494a-a510-0aa193e38c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E7D382AD0>]}
[0m23:28:53.425963 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:28:53.426956 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cbd5ab5e-ae05-494a-a510-0aa193e38c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E7D382E90>]}
[0m23:28:53.427960 [info ] [MainThread]: 
[0m23:28:53.428953 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:28:53.429961 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m23:28:53.429961 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:28:53.429961 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:28:53.430960 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m23:28:53.430960 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m23:29:20.276395 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:29:20.276395 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-c253788c-4ad8-4963-bea0-bc84c7f67041
[0m23:29:28.395945 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:29:28.952102 [debug] [ThreadPool]: On list_hudidb: Close
[0m23:29:28.952102 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:29:28.954085 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m23:29:28.954085 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:29:28.955037 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:29:29.425522 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m23:29:29.426530 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:29:29.426530 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-c253788c-4ad8-4963-bea0-bc84c7f67041
[0m23:29:30.218357 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:29:30.790539 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m23:29:30.790539 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:29:30.791538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cbd5ab5e-ae05-494a-a510-0aa193e38c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E7D4E69E0>]}
[0m23:29:30.791538 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:29:30.792535 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:29:30.792535 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:29:30.793340 [info ] [MainThread]: 
[0m23:29:30.797658 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m23:29:30.797658 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m23:29:30.798664 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m23:29:30.798664 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m23:29:30.798664 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m23:29:30.801670 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m23:29:30.802670 [debug] [Thread-1 (]: finished collecting timing info
[0m23:29:30.802670 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m23:29:30.821670 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:29:30.821670 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m23:29:31.300277 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m23:29:31.301271 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m23:29:31.302272 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-c253788c-4ad8-4963-bea0-bc84c7f67041
[0m23:29:32.113819 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:29:32.655521 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:29:32.662513 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m23:29:32.662513 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:29:32.662513 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m23:29:32.662513 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:29:32.663454 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:29:32.663454 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:29:32.663454 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:29:32.663454 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m23:29:53.379147 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671751772889, 'CompletedOn': 1671751792875}, 'ResponseMetadata': {'RequestId': 'd1984fbb-c270-4a60-80d9-259c6bf26bec', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:29:53 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': 'd1984fbb-c270-4a60-80d9-259c6bf26bec'}, 'RetryAttempts': 0}}
[0m23:29:53.379147 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m23:29:53.380119 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m23:29:53.380119 [debug] [Thread-1 (]: SQL status: OK in 20.72 seconds
[0m23:29:53.384163 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:29:53.386091 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:29:53.928632 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:29:53.929638 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid

)

select * from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m23:29:53.929638 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:29:53.929638 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:29:53.929638 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:29:53.930636 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid

)

select * from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m23:29:54.303312 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n\n)\n\nselect * from source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 7, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "cannot resolve '`invoiceid`' given input columns: []; line 6 pos 8;\n'Project [*]\n+- 'SubqueryAlias source_data\n   +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]\n      +- OneRowRelation\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 6 pos 8;\n'Project [*]\n+- 'SubqueryAlias source_data\n   +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]\n      +- OneRowRelation\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671751794140, 'CompletedOn': 1671751794303}, 'ResponseMetadata': {'RequestId': 'd8fcb0c9-32a3-42a3-8628-0a519fc7a6d7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:29:54 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3608', 'connection': 'keep-alive', 'x-amzn-requestid': 'd8fcb0c9-32a3-42a3-8628-0a519fc7a6d7'}, 'RetryAttempts': 0}}
[0m23:29:54.303312 [debug] [Thread-1 (]: Glue adapter: status = error
[0m23:29:54.303312 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid

)

select * from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
, AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 6 pos 8;
'Project [*]
+- 'SubqueryAlias source_data
   +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
      +- OneRowRelation

[0m23:29:54.305312 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid

)

select * from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
, AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 6 pos 8;
'Project [*]
+- 'SubqueryAlias source_data
   +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
      +- OneRowRelation

[0m23:29:54.306314 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""
  
  
  with source_data as (
      SELECT
          invoiceid ,
          category ,
          destinationstate,
          itemid
  
  )
  
  select * from source_data""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if None is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi//hudidb/cleansed_order/")
  
  spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
  SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
  , AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 6 pos 8;
  'Project [*]
  +- 'SubqueryAlias source_data
     +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
        +- OneRowRelation
  
[0m23:29:54.308314 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m23:29:54.309314 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:29:54.309314 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m23:29:54.309314 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:29:54.310371 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:29:54.310371 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:29:54.310371 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:29:54.310371 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m23:29:56.224194 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 8, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: hudidb.cleansed_order; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671751794860, 'CompletedOn': 1671751795343}, 'ResponseMetadata': {'RequestId': '5f271970-bc6a-4616-8320-76b6be0dec9a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:29:56 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1428', 'connection': 'keep-alive', 'x-amzn-requestid': '5f271970-bc6a-4616-8320-76b6be0dec9a'}, 'RetryAttempts': 0}}
[0m23:29:56.224194 [debug] [Thread-1 (]: Glue adapter: status = error
[0m23:29:56.225194 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false

[0m23:29:56.226195 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false

[0m23:29:56.226195 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m23:29:56.227194 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m23:29:56.227194 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m23:29:56.227194 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m23:29:56.227194 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m23:29:56.227194 [debug] [Thread-1 (]: finished collecting timing info
[0m23:29:56.228199 [debug] [Thread-1 (]: Database Error in model cleansed_order (models\metrics\cleansed_order.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
  select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [hudidb, cleansed_order], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:29:56.229194 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cbd5ab5e-ae05-494a-a510-0aa193e38c90', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E7E002620>]}
[0m23:29:56.229194 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model hudidb.cleansed_order .............. [[31mERROR[0m in 25.43s]
[0m23:29:56.230194 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m23:29:56.232194 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:29:56.233195 [debug] [MainThread]: On master: ROLLBACK
[0m23:29:56.233195 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:29:56.233195 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m23:29:57.064403 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m23:29:57.064403 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m23:29:57.065481 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-c253788c-4ad8-4963-bea0-bc84c7f67041
[0m23:29:57.840314 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:29:57.841313 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:29:57.842324 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:29:57.842324 [debug] [MainThread]: On master: ROLLBACK
[0m23:29:57.843316 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:29:57.844323 [debug] [MainThread]: On master: Close
[0m23:29:57.844323 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m23:29:57.845329 [info ] [MainThread]: 
[0m23:29:57.846299 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 4.42 seconds (64.42s).
[0m23:29:57.848237 [debug] [MainThread]: Glue adapter: cleanup called
[0m23:29:58.164734 [info ] [MainThread]: 
[0m23:29:58.165741 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:29:58.166734 [info ] [MainThread]: 
[0m23:29:58.166734 [error] [MainThread]: [33mDatabase Error in model cleansed_order (models\metrics\cleansed_order.sql)[0m
[0m23:29:58.167740 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
[0m23:29:58.168737 [error] [MainThread]:   select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
[0m23:29:58.168737 [error] [MainThread]:   'GlobalLimit 1
[0m23:29:58.169734 [error] [MainThread]:   +- 'LocalLimit 1
[0m23:29:58.169734 [error] [MainThread]:      +- 'Project [*]
[0m23:29:58.170733 [error] [MainThread]:         +- 'UnresolvedRelation [hudidb, cleansed_order], [], false
[0m23:29:58.170733 [error] [MainThread]:   
[0m23:29:58.170733 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:29:58.171740 [info ] [MainThread]: 
[0m23:29:58.171740 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:29:58.172742 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E7D4E57E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E7E053DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021E7E0807F0>]}
[0m23:29:58.172742 [debug] [MainThread]: Flushing usage events
[0m23:29:58.795428 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 23:32:07.012158 | ffbdf2d2-0efb-4f15-a361-3733e53510dc ==============================
[0m23:32:07.012158 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:32:07.014158 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'clean', 'indirect_selection': 'eager'}
[0m23:32:07.014158 [debug] [MainThread]: Tracking: tracking
[0m23:32:07.044165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B52B2115A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B52B2110F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B52B2113F0>]}
[0m23:32:07.051157 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B52B211480>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B52B2110C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B52B211060>]}
[0m23:32:07.051157 [debug] [MainThread]: Flushing usage events


============================== 2022-12-22 23:32:13.561237 | 962a8def-0709-47b8-8155-ec9170f957cc ==============================
[0m23:32:13.561237 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:32:13.562246 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:32:13.562246 [debug] [MainThread]: Tracking: tracking
[0m23:32:13.582246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002543418D540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002543418D900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002543418D6C0>]}
[0m23:32:13.593237 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m23:32:13.593237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '962a8def-0709-47b8-8155-ec9170f957cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002543418FB80>]}
[0m23:32:13.644237 [debug] [MainThread]: Parsing macros\adapters.sql
[0m23:32:13.664237 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m23:32:13.665246 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m23:32:13.666245 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m23:32:13.669244 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m23:32:13.680237 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m23:32:13.681238 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m23:32:13.688237 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m23:32:13.691247 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m23:32:13.693244 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m23:32:13.694244 [debug] [MainThread]: Parsing macros\adapters.sql
[0m23:32:13.722237 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m23:32:13.724246 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m23:32:13.731244 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m23:32:13.749244 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m23:32:13.754237 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m23:32:13.755246 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m23:32:13.757244 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m23:32:13.762244 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m23:32:13.769239 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m23:32:13.772764 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m23:32:13.773754 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m23:32:13.773754 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m23:32:13.774770 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m23:32:13.775764 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m23:32:13.776761 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m23:32:13.776761 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m23:32:13.777768 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m23:32:13.781299 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m23:32:13.791293 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m23:32:13.792299 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m23:32:13.793300 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m23:32:13.794304 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m23:32:13.804300 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m23:32:13.810300 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m23:32:13.812300 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m23:32:13.814294 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m23:32:13.820299 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m23:32:13.823301 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m23:32:13.834294 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m23:32:13.836295 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m23:32:13.838293 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m23:32:13.845300 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m23:32:13.849305 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m23:32:13.850301 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m23:32:13.851293 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m23:32:13.852292 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m23:32:13.852292 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m23:32:13.854302 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m23:32:13.855293 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m23:32:13.857300 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m23:32:13.859300 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m23:32:13.862299 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m23:32:13.868292 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m23:32:13.876292 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m23:32:13.877302 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m23:32:13.888300 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m23:32:13.900299 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m23:32:13.905292 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m23:32:13.908295 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m23:32:13.913294 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m23:32:13.915300 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m23:32:13.917293 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m23:32:13.918300 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m23:32:13.922309 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m23:32:13.936301 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m23:32:13.941293 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m23:32:13.951302 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m23:32:13.960300 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m23:32:13.961303 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m23:32:13.973300 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m23:32:13.974293 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m23:32:13.980300 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m23:32:13.982300 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m23:32:13.990300 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m23:32:13.991300 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m23:32:13.992294 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m23:32:13.993300 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m23:32:13.995300 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m23:32:13.995300 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m23:32:13.996300 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m23:32:13.997300 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m23:32:14.002300 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m23:32:14.003300 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m23:32:14.004300 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m23:32:14.005300 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m23:32:14.006300 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m23:32:14.007307 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m23:32:14.008304 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m23:32:14.009300 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m23:32:14.011302 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m23:32:14.012301 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m23:32:14.014300 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m23:32:14.015300 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m23:32:14.016300 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m23:32:14.017299 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m23:32:14.018300 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m23:32:14.019293 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m23:32:14.021300 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m23:32:14.277565 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m23:32:14.370839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '962a8def-0709-47b8-8155-ec9170f957cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254342B9180>]}
[0m23:32:14.378831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '962a8def-0709-47b8-8155-ec9170f957cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254342BB7F0>]}
[0m23:32:14.379831 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 0 sources, 0 exposures, 0 metrics
[0m23:32:14.380831 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '962a8def-0709-47b8-8155-ec9170f957cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254343056C0>]}
[0m23:32:14.381831 [info ] [MainThread]: 
[0m23:32:14.382838 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:32:14.383831 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m23:32:14.383831 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:32:14.384839 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:32:14.384839 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m23:32:14.384839 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m23:32:54.277422 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:32:54.277962 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-2352aa03-bfb4-4086-a347-614d3b3ff738
[0m23:33:02.432429 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:33:02.979623 [debug] [ThreadPool]: On list_hudidb: Close
[0m23:33:02.980624 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:33:02.981617 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m23:33:02.982617 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:33:02.982617 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:33:03.456614 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m23:33:03.456614 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:33:03.457611 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-2352aa03-bfb4-4086-a347-614d3b3ff738
[0m23:33:04.262471 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:33:04.807895 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m23:33:04.807895 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:33:04.809460 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '962a8def-0709-47b8-8155-ec9170f957cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254341A5930>]}
[0m23:33:04.809460 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:33:04.809460 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:33:04.810534 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:33:04.811079 [info ] [MainThread]: 
[0m23:33:04.816084 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m23:33:04.816084 [info ] [Thread-1 (]: 1 of 1 START sql table model hudidb.cleansed_order ............................. [RUN]
[0m23:33:04.817084 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m23:33:04.817084 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m23:33:04.817084 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m23:33:04.820091 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m23:33:04.822091 [debug] [Thread-1 (]: finished collecting timing info
[0m23:33:04.823091 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m23:33:04.830091 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:33:04.830091 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m23:33:05.304741 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m23:33:05.305740 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m23:33:05.305740 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-2352aa03-bfb4-4086-a347-614d3b3ff738
[0m23:33:06.821327 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:33:07.704748 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:33:07.731748 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:33:07.735755 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m23:33:07.736748 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m23:33:07.737748 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:33:07.737748 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
)

select * from source_data
  
[0m23:33:07.737748 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:33:07.737748 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:33:07.737748 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:33:07.737748 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:33:07.738756 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
)

select * from source_data
  ''')
[0m23:33:09.330762 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n  \n    \n    \tcreate table hudidb.cleansed_order\n    \n    \n    using hudi\n    \n    \n    \n    LOCATION \'s3://soumil-dms-learn/hudi//hudidb/cleansed_order/\'\n    \n\tas\n\t\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n)\n\nselect * from source_data\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 6, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "cannot resolve '`invoiceid`' given input columns: []; line 20 pos 8;\n'CreateTable `hudidb`.`cleansed_order`, ErrorIfExists\n+- 'Project [*]\n   +- 'SubqueryAlias source_data\n      +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]\n         +- OneRowRelation\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 20 pos 8;\n'CreateTable `hudidb`.`cleansed_order`, ErrorIfExists\n+- 'Project [*]\n   +- 'SubqueryAlias source_data\n      +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]\n         +- OneRowRelation\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671751987972, 'CompletedOn': 1671751988484}, 'ResponseMetadata': {'RequestId': '0b47adc1-ba7f-4f26-a006-e98f1f37b566', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:33:09 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1906', 'connection': 'keep-alive', 'x-amzn-requestid': '0b47adc1-ba7f-4f26-a006-e98f1f37b566'}, 'RetryAttempts': 0}}
[0m23:33:09.330762 [debug] [Thread-1 (]: Glue adapter: status = error
[0m23:33:09.330762 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
)

select * from source_data
  '''), AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 20 pos 8;
'CreateTable `hudidb`.`cleansed_order`, ErrorIfExists
+- 'Project [*]
   +- 'SubqueryAlias source_data
      +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
         +- OneRowRelation

[0m23:33:09.332762 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
)

select * from source_data
  '''), AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 20 pos 8;
'CreateTable `hudidb`.`cleansed_order`, ErrorIfExists
+- 'Project [*]
   +- 'SubqueryAlias source_data
      +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
         +- OneRowRelation

[0m23:33:09.332762 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
)

select * from source_data
  
[0m23:33:09.333762 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m23:33:09.333762 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m23:33:09.333762 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m23:33:09.333762 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m23:33:09.333762 [debug] [Thread-1 (]: finished collecting timing info
[0m23:33:09.334773 [debug] [Thread-1 (]: Database Error in model cleansed_order (models\metrics\cleansed_order.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
  
    
      
      	create table hudidb.cleansed_order
      
      
      using hudi
      
      
      
      LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
      
  	as
  	
  
  
  with source_data as (
      SELECT
          invoiceid ,
          category ,
          destinationstate,
          itemid
  )
  
  select * from source_data
    '''), AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 20 pos 8;
  'CreateTable `hudidb`.`cleansed_order`, ErrorIfExists
  +- 'Project [*]
     +- 'SubqueryAlias source_data
        +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
           +- OneRowRelation
  
  compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:33:09.334773 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '962a8def-0709-47b8-8155-ec9170f957cc', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025434EFCC40>]}
[0m23:33:09.335762 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model hudidb.cleansed_order .................... [[31mERROR[0m in 4.52s]
[0m23:33:09.335762 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m23:33:09.337772 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:33:09.337772 [debug] [MainThread]: On master: ROLLBACK
[0m23:33:09.337772 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:33:09.338762 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m23:33:10.136504 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m23:33:10.137446 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m23:33:10.137446 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-2352aa03-bfb4-4086-a347-614d3b3ff738
[0m23:33:10.930662 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:33:10.930662 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:33:10.931672 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:33:10.931672 [debug] [MainThread]: On master: ROLLBACK
[0m23:33:10.931672 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:33:10.931672 [debug] [MainThread]: On master: Close
[0m23:33:10.931672 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m23:33:10.932671 [info ] [MainThread]: 
[0m23:33:10.933412 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 56.55 seconds (56.55s).
[0m23:33:10.933412 [debug] [MainThread]: Glue adapter: cleanup called
[0m23:33:11.204502 [info ] [MainThread]: 
[0m23:33:11.205481 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:33:11.206482 [info ] [MainThread]: 
[0m23:33:11.207650 [error] [MainThread]: [33mDatabase Error in model cleansed_order (models\metrics\cleansed_order.sql)[0m
[0m23:33:11.207650 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
[0m23:33:11.208657 [error] [MainThread]:   
[0m23:33:11.209457 [error] [MainThread]:     
[0m23:33:11.209839 [error] [MainThread]:       
[0m23:33:11.209839 [error] [MainThread]:       	create table hudidb.cleansed_order
[0m23:33:11.209839 [error] [MainThread]:       
[0m23:33:11.210849 [error] [MainThread]:       
[0m23:33:11.210849 [error] [MainThread]:       using hudi
[0m23:33:11.210849 [error] [MainThread]:       
[0m23:33:11.211848 [error] [MainThread]:       
[0m23:33:11.211848 [error] [MainThread]:       
[0m23:33:11.211848 [error] [MainThread]:       LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
[0m23:33:11.212848 [error] [MainThread]:       
[0m23:33:11.212848 [error] [MainThread]:   	as
[0m23:33:11.212848 [error] [MainThread]:   	
[0m23:33:11.212848 [error] [MainThread]:   
[0m23:33:11.213848 [error] [MainThread]:   
[0m23:33:11.213848 [error] [MainThread]:   with source_data as (
[0m23:33:11.213848 [error] [MainThread]:       SELECT
[0m23:33:11.214848 [error] [MainThread]:           invoiceid ,
[0m23:33:11.214848 [error] [MainThread]:           category ,
[0m23:33:11.214848 [error] [MainThread]:           destinationstate,
[0m23:33:11.215848 [error] [MainThread]:           itemid
[0m23:33:11.215848 [error] [MainThread]:   )
[0m23:33:11.215848 [error] [MainThread]:   
[0m23:33:11.216848 [error] [MainThread]:   select * from source_data
[0m23:33:11.216848 [error] [MainThread]:     '''), AnalysisException: cannot resolve '`invoiceid`' given input columns: []; line 20 pos 8;
[0m23:33:11.216848 [error] [MainThread]:   'CreateTable `hudidb`.`cleansed_order`, ErrorIfExists
[0m23:33:11.217848 [error] [MainThread]:   +- 'Project [*]
[0m23:33:11.217848 [error] [MainThread]:      +- 'SubqueryAlias source_data
[0m23:33:11.217848 [error] [MainThread]:         +- 'Project ['invoiceid, 'category, 'destinationstate, 'itemid]
[0m23:33:11.218848 [error] [MainThread]:            +- OneRowRelation
[0m23:33:11.218848 [error] [MainThread]:   
[0m23:33:11.218848 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:33:11.219848 [info ] [MainThread]: 
[0m23:33:11.219848 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:33:11.220849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254341A49A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025434E9A1D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025434E99F00>]}
[0m23:33:11.220849 [debug] [MainThread]: Flushing usage events
[0m23:33:11.384253 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 23:38:46.419897 | 92bff63b-b9b5-456c-b7b8-f7ed06f0839d ==============================
[0m23:38:46.419897 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:38:46.420911 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:38:46.421897 [debug] [MainThread]: Tracking: tracking
[0m23:38:46.446903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002960B23D540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002960B23D900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002960B23D6C0>]}
[0m23:38:46.542897 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m23:38:46.543897 [debug] [MainThread]: Partial parsing: added file: dbtglue://models\source_tables.yml
[0m23:38:46.543897 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m23:38:46.557902 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m23:38:46.600901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '92bff63b-b9b5-456c-b7b8-f7ed06f0839d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002960B48EB90>]}
[0m23:38:46.606899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '92bff63b-b9b5-456c-b7b8-f7ed06f0839d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002960B3FFF70>]}
[0m23:38:46.606899 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m23:38:46.608279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '92bff63b-b9b5-456c-b7b8-f7ed06f0839d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000296081D39D0>]}
[0m23:38:46.608279 [info ] [MainThread]: 
[0m23:38:46.609715 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:38:46.610721 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m23:38:46.610721 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:38:46.611720 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:38:46.611720 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m23:38:46.611720 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m23:39:43.559742 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:39:43.560713 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-63594819-a53f-4a81-bb23-24117b200669
[0m23:39:51.725800 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:39:52.291812 [debug] [ThreadPool]: On list_hudidb: Close
[0m23:39:52.291812 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:39:52.293811 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m23:39:52.293811 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:39:52.293811 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:39:52.762352 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m23:39:52.763427 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:39:52.763427 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-63594819-a53f-4a81-bb23-24117b200669
[0m23:39:53.626690 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:39:54.161008 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m23:39:54.161008 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:39:54.161942 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '92bff63b-b9b5-456c-b7b8-f7ed06f0839d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002960B479030>]}
[0m23:39:54.161942 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:39:54.163006 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:39:54.163006 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:39:54.164075 [info ] [MainThread]: 
[0m23:39:54.168742 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m23:39:54.168742 [info ] [Thread-1 (]: 1 of 1 START sql table model hudidb.cleansed_order ............................. [RUN]
[0m23:39:54.169751 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m23:39:54.169751 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m23:39:54.169751 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m23:39:54.173749 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m23:39:54.174741 [debug] [Thread-1 (]: finished collecting timing info
[0m23:39:54.174741 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m23:39:54.180749 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:39:54.180749 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m23:39:54.670197 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m23:39:54.671259 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m23:39:54.671259 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-63594819-a53f-4a81-bb23-24117b200669
[0m23:39:55.453903 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:39:55.973567 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:39:56.000569 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:39:56.003569 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m23:39:56.004562 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m23:39:56.004562 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:39:56.004562 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data
  
[0m23:39:56.004562 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:39:56.005562 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:39:56.005562 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:39:56.005562 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:39:56.005562 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data
  ''')
[0m23:39:59.997671 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n  \n    \n    \tcreate table hudidb.cleansed_order\n    \n    \n    using hudi\n    \n    \n    \n    LOCATION \'s3://soumil-dms-learn/hudi//hudidb/cleansed_order/\'\n    \n\tas\n\t\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect * from source_data\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 6, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': "An error occurred while calling o75.sql.\n: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.\n\tat org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', "py4j.protocol.Py4JJavaError: An error occurred while calling o75.sql.\n: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.\n\tat org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671752396243, 'CompletedOn': 1671752399545}, 'ResponseMetadata': {'RequestId': '6234da59-c550-4505-b38e-af7256cf8f8d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:40:00 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '8560', 'connection': 'keep-alive', 'x-amzn-requestid': '6234da59-c550-4505-b38e-af7256cf8f8d'}, 'RetryAttempts': 0}}
[0m23:39:59.998678 [debug] [Thread-1 (]: Glue adapter: status = error
[0m23:39:59.998678 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data
  '''), Py4JJavaError: An error occurred while calling o75.sql.
: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

[0m23:40:00.001670 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data
  '''), Py4JJavaError: An error occurred while calling o75.sql.
: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

[0m23:40:00.002678 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data
  
[0m23:40:00.002678 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m23:40:00.002678 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m23:40:00.003669 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m23:40:00.003669 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m23:40:00.003669 [debug] [Thread-1 (]: finished collecting timing info
[0m23:40:00.004670 [debug] [Thread-1 (]: Database Error in model cleansed_order (models\metrics\cleansed_order.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
  
    
      
      	create table hudidb.cleansed_order
      
      
      using hudi
      
      
      
      LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
      
  	as
  	
  
  
  with source_data as (
      SELECT
          invoiceid ,
          category ,
          destinationstate,
          itemid
      FROM hudidb.order
  )
  
  select * from source_data
    '''), Py4JJavaError: An error occurred while calling o75.sql.
  : org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
  	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
  	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
  	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
  	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
  	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
  	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  
  compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:40:00.004670 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '92bff63b-b9b5-456c-b7b8-f7ed06f0839d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002960C006740>]}
[0m23:40:00.005669 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model hudidb.cleansed_order .................... [[31mERROR[0m in 5.83s]
[0m23:40:00.006671 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m23:40:00.008670 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:40:00.008670 [debug] [MainThread]: On master: ROLLBACK
[0m23:40:00.008670 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:40:00.009671 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m23:40:00.491518 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m23:40:00.492518 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m23:40:00.492518 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-63594819-a53f-4a81-bb23-24117b200669
[0m23:40:01.335529 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:40:01.335529 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:40:01.336536 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:40:01.336536 [debug] [MainThread]: On master: ROLLBACK
[0m23:40:01.336536 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:40:01.336536 [debug] [MainThread]: On master: Close
[0m23:40:01.336536 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m23:40:01.337536 [info ] [MainThread]: 
[0m23:40:01.337536 [info ] [MainThread]: Finished running 1 table model in 0 hours 1 minutes and 14.73 seconds (74.73s).
[0m23:40:01.338537 [debug] [MainThread]: Glue adapter: cleanup called
[0m23:40:01.641188 [info ] [MainThread]: 
[0m23:40:01.641847 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:40:01.642852 [info ] [MainThread]: 
[0m23:40:01.643852 [error] [MainThread]: [33mDatabase Error in model cleansed_order (models\metrics\cleansed_order.sql)[0m
[0m23:40:01.643852 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
[0m23:40:01.644860 [error] [MainThread]:   
[0m23:40:01.644860 [error] [MainThread]:     
[0m23:40:01.645851 [error] [MainThread]:       
[0m23:40:01.645851 [error] [MainThread]:       	create table hudidb.cleansed_order
[0m23:40:01.645851 [error] [MainThread]:       
[0m23:40:01.645851 [error] [MainThread]:       
[0m23:40:01.646851 [error] [MainThread]:       using hudi
[0m23:40:01.646851 [error] [MainThread]:       
[0m23:40:01.646851 [error] [MainThread]:       
[0m23:40:01.647851 [error] [MainThread]:       
[0m23:40:01.647851 [error] [MainThread]:       LOCATION 's3://soumil-dms-learn/hudi//hudidb/cleansed_order/'
[0m23:40:01.647851 [error] [MainThread]:       
[0m23:40:01.648851 [error] [MainThread]:   	as
[0m23:40:01.648851 [error] [MainThread]:   	
[0m23:40:01.648851 [error] [MainThread]:   
[0m23:40:01.649851 [error] [MainThread]:   
[0m23:40:01.649851 [error] [MainThread]:   with source_data as (
[0m23:40:01.649851 [error] [MainThread]:       SELECT
[0m23:40:01.649851 [error] [MainThread]:           invoiceid ,
[0m23:40:01.650851 [error] [MainThread]:           category ,
[0m23:40:01.650851 [error] [MainThread]:           destinationstate,
[0m23:40:01.650851 [error] [MainThread]:           itemid
[0m23:40:01.651851 [error] [MainThread]:       FROM hudidb.order
[0m23:40:01.651851 [error] [MainThread]:   )
[0m23:40:01.652855 [error] [MainThread]:   
[0m23:40:01.652855 [error] [MainThread]:   select * from source_data
[0m23:40:01.652855 [error] [MainThread]:     '''), Py4JJavaError: An error occurred while calling o75.sql.
[0m23:40:01.653851 [error] [MainThread]:   : org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
[0m23:40:01.653851 [error] [MainThread]:   	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
[0m23:40:01.653851 [error] [MainThread]:   	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
[0m23:40:01.654852 [error] [MainThread]:   	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
[0m23:40:01.654852 [error] [MainThread]:   	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
[0m23:40:01.654852 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
[0m23:40:01.655851 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
[0m23:40:01.655851 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[0m23:40:01.656850 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[0m23:40:01.656850 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
[0m23:40:01.656850 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
[0m23:40:01.657850 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
[0m23:40:01.657850 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
[0m23:40:01.657850 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
[0m23:40:01.658851 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
[0m23:40:01.658851 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
[0m23:40:01.658851 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
[0m23:40:01.659850 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
[0m23:40:01.659850 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
[0m23:40:01.659850 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
[0m23:40:01.660850 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
[0m23:40:01.660850 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m23:40:01.660850 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
[0m23:40:01.661851 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
[0m23:40:01.661851 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
[0m23:40:01.661851 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
[0m23:40:01.661851 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m23:40:01.662851 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
[0m23:40:01.662851 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
[0m23:40:01.662851 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m23:40:01.663851 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
[0m23:40:01.663851 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[0m23:40:01.663851 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[0m23:40:01.664851 [error] [MainThread]:   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[0m23:40:01.664851 [error] [MainThread]:   	at java.lang.reflect.Method.invoke(Method.java:498)
[0m23:40:01.664851 [error] [MainThread]:   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[0m23:40:01.665851 [error] [MainThread]:   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[0m23:40:01.665851 [error] [MainThread]:   	at py4j.Gateway.invoke(Gateway.java:282)
[0m23:40:01.665851 [error] [MainThread]:   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[0m23:40:01.666851 [error] [MainThread]:   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[0m23:40:01.666851 [error] [MainThread]:   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[0m23:40:01.666851 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:750)
[0m23:40:01.667851 [error] [MainThread]:   
[0m23:40:01.667851 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:40:01.668851 [info ] [MainThread]: 
[0m23:40:01.668851 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:40:01.669851 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002960C032F50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002960C0336A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002960BF845B0>]}
[0m23:40:01.669851 [debug] [MainThread]: Flushing usage events
[0m23:40:01.853947 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 23:45:18.079366 | 66f27ee7-97a6-40bb-b780-b8bd15895728 ==============================
[0m23:45:18.079366 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:45:18.080367 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'clean', 'indirect_selection': 'eager'}
[0m23:45:18.080367 [debug] [MainThread]: Tracking: tracking
[0m23:45:18.102663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012CB32C1570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012CB32C10C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012CB32C13C0>]}
[0m23:45:18.107648 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012CB32C1450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012CB32C1090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012CB32C1030>]}
[0m23:45:18.107648 [debug] [MainThread]: Flushing usage events


============================== 2022-12-22 23:45:52.614034 | 2c4e2e01-9bb5-4024-87c1-a6e1fa5e7971 ==============================
[0m23:45:52.614034 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:45:52.614966 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:45:52.614966 [debug] [MainThread]: Tracking: tracking
[0m23:45:52.634536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A0789D540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A0789D900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A0789D6C0>]}
[0m23:45:52.644543 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m23:45:52.645535 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '2c4e2e01-9bb5-4024-87c1-a6e1fa5e7971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A0789FB80>]}
[0m23:45:52.692543 [debug] [MainThread]: Parsing macros\adapters.sql
[0m23:45:52.711543 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m23:45:52.712536 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m23:45:52.713545 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m23:45:52.716543 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m23:45:52.728536 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m23:45:52.729547 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m23:45:52.736039 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m23:45:52.738628 [debug] [MainThread]: Parsing macros\materializations\table\iceberg_table_replace.sql
[0m23:45:52.741549 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m23:45:52.742549 [debug] [MainThread]: Parsing macros\adapters.sql
[0m23:45:52.771624 [debug] [MainThread]: Parsing macros\apply_grants.sql
[0m23:45:52.773605 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m23:45:52.779555 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m23:45:52.795549 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m23:45:52.799549 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m23:45:52.800550 [debug] [MainThread]: Parsing macros\materializations\incremental\column_helpers.sql
[0m23:45:52.801551 [debug] [MainThread]: Parsing macros\materializations\incremental\incremental.sql
[0m23:45:52.806549 [debug] [MainThread]: Parsing macros\materializations\incremental\strategies.sql
[0m23:45:52.812549 [debug] [MainThread]: Parsing macros\materializations\incremental\validate.sql
[0m23:45:52.815550 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m23:45:52.816551 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m23:45:52.816551 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m23:45:52.817550 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m23:45:52.817550 [debug] [MainThread]: Parsing macros\utils\assert_not_null.sql
[0m23:45:52.818552 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m23:45:52.819552 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m23:45:52.819552 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m23:45:52.823550 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m23:45:52.833550 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m23:45:52.834552 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m23:45:52.835549 [debug] [MainThread]: Parsing macros\utils\timestamps.sql
[0m23:45:52.836553 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m23:45:52.846550 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m23:45:52.853550 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m23:45:52.855552 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m23:45:52.857552 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m23:45:52.863551 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m23:45:52.866551 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m23:45:52.877550 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m23:45:52.879552 [debug] [MainThread]: Parsing macros\adapters\timestamps.sql
[0m23:45:52.881552 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m23:45:52.887551 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m23:45:52.892551 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m23:45:52.893553 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m23:45:52.894553 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m23:45:52.894553 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m23:45:52.895553 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m23:45:52.896551 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m23:45:52.898553 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m23:45:52.900552 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m23:45:52.901553 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m23:45:52.904551 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m23:45:52.910560 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m23:45:52.918550 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m23:45:52.919552 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m23:45:52.930551 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m23:45:52.943551 [debug] [MainThread]: Parsing macros\materializations\models\incremental\strategies.sql
[0m23:45:52.948551 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m23:45:52.951551 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m23:45:52.955552 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m23:45:52.958550 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m23:45:52.960552 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m23:45:52.961552 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m23:45:52.965550 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m23:45:52.979548 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m23:45:52.985548 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m23:45:52.995548 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m23:45:53.003555 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m23:45:53.004548 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m23:45:53.016559 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m23:45:53.018042 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m23:45:53.021547 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m23:45:53.023549 [debug] [MainThread]: Parsing macros\python_model\python.sql
[0m23:45:53.028579 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m23:45:53.029586 [debug] [MainThread]: Parsing macros\utils\array_append.sql
[0m23:45:53.030586 [debug] [MainThread]: Parsing macros\utils\array_concat.sql
[0m23:45:53.031586 [debug] [MainThread]: Parsing macros\utils\array_construct.sql
[0m23:45:53.032589 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m23:45:53.033590 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m23:45:53.034587 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m23:45:53.035591 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m23:45:53.040580 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m23:45:53.041591 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m23:45:53.042587 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m23:45:53.043586 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m23:45:53.044593 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m23:45:53.045590 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m23:45:53.046586 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m23:45:53.047586 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m23:45:53.049579 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m23:45:53.050588 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m23:45:53.052586 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m23:45:53.052586 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m23:45:53.053586 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m23:45:53.054588 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m23:45:53.055586 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m23:45:53.056579 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m23:45:53.058586 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m23:45:53.300639 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m23:45:53.390640 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2c4e2e01-9bb5-4024-87c1-a6e1fa5e7971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A079DD1E0>]}
[0m23:45:53.398639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2c4e2e01-9bb5-4024-87c1-a6e1fa5e7971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A0789D900>]}
[0m23:45:53.398639 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m23:45:53.399641 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c4e2e01-9bb5-4024-87c1-a6e1fa5e7971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A0789C7F0>]}
[0m23:45:53.400679 [info ] [MainThread]: 
[0m23:45:53.401733 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:45:53.402641 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m23:45:53.402641 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:45:53.403720 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:45:53.403720 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m23:45:53.403720 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m23:46:49.567257 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:46:49.567257 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-010dfb28-29bf-4bc1-ac9a-03c37693489d
[0m23:46:57.707559 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:46:58.288634 [debug] [ThreadPool]: On list_hudidb: Close
[0m23:46:58.288634 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:46:58.289634 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m23:46:58.290637 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:46:58.290637 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:46:58.785488 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m23:46:58.785488 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:46:58.785488 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-010dfb28-29bf-4bc1-ac9a-03c37693489d
[0m23:46:59.604403 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:47:00.150179 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m23:47:00.151178 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:47:00.151178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2c4e2e01-9bb5-4024-87c1-a6e1fa5e7971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A078A0A90>]}
[0m23:47:00.152171 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:47:00.152171 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:47:00.152171 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:47:00.153571 [info ] [MainThread]: 
[0m23:47:00.158149 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m23:47:00.158149 [info ] [Thread-1 (]: 1 of 1 START sql table model hudidb.cleansed_order ............................. [RUN]
[0m23:47:00.159159 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m23:47:00.160160 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m23:47:00.160160 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m23:47:00.163156 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m23:47:00.173276 [debug] [Thread-1 (]: finished collecting timing info
[0m23:47:00.174290 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m23:47:00.180288 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:47:00.180288 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m23:47:00.663645 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m23:47:00.663645 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m23:47:00.664646 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-010dfb28-29bf-4bc1-ac9a-03c37693489d
[0m23:47:01.487909 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:47:02.034944 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:47:02.062944 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:47:02.065944 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m23:47:02.067937 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m23:47:02.067937 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:47:02.067937 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data
  
[0m23:47:02.067937 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:47:02.067937 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:47:02.068938 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:47:02.068938 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:47:02.068938 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data
  ''')
[0m23:47:06.356271 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n  \n    \n    \tcreate table hudidb.cleansed_order\n    \n    \n    using hudi\n    \n    \n    \n    LOCATION \'s3://soumil-dms-learn/hudi/hudidb/cleansed_order/\'\n    \n\tas\n\t\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect * from source_data\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 6, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': "An error occurred while calling o75.sql.\n: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.\n\tat org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', "py4j.protocol.Py4JJavaError: An error occurred while calling o75.sql.\n: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.\n\tat org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671752822611, 'CompletedOn': 1671752826172}, 'ResponseMetadata': {'RequestId': 'd5cabd01-9d5b-474a-bbfd-905c487ae7a9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:47:06 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '8559', 'connection': 'keep-alive', 'x-amzn-requestid': 'd5cabd01-9d5b-474a-bbfd-905c487ae7a9'}, 'RetryAttempts': 0}}
[0m23:47:06.356271 [debug] [Thread-1 (]: Glue adapter: status = error
[0m23:47:06.357273 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data
  '''), Py4JJavaError: An error occurred while calling o75.sql.
: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

[0m23:47:06.359979 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data
  '''), Py4JJavaError: An error occurred while calling o75.sql.
: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

[0m23:47:06.361011 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

  
    
    	create table hudidb.cleansed_order
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/cleansed_order/'
    
	as
	


with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data
  
[0m23:47:06.361011 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m23:47:06.361011 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m23:47:06.361011 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m23:47:06.361981 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m23:47:06.361981 [debug] [Thread-1 (]: finished collecting timing info
[0m23:47:06.361981 [debug] [Thread-1 (]: Database Error in model cleansed_order (models\metrics\cleansed_order.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
  
    
      
      	create table hudidb.cleansed_order
      
      
      using hudi
      
      
      
      LOCATION 's3://soumil-dms-learn/hudi/hudidb/cleansed_order/'
      
  	as
  	
  
  
  with source_data as (
      SELECT
          invoiceid ,
          category ,
          destinationstate,
          itemid
      FROM hudidb.order
  )
  
  select * from source_data
    '''), Py4JJavaError: An error occurred while calling o75.sql.
  : org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
  	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
  	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
  	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
  	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
  	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
  	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  
  compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:47:06.363051 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2c4e2e01-9bb5-4024-87c1-a6e1fa5e7971', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A0868E7A0>]}
[0m23:47:06.363051 [error] [Thread-1 (]: 1 of 1 ERROR creating sql table model hudidb.cleansed_order .................... [[31mERROR[0m in 6.20s]
[0m23:47:06.363980 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m23:47:06.364981 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:47:06.364981 [debug] [MainThread]: On master: ROLLBACK
[0m23:47:06.365989 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:47:06.365989 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m23:47:06.853174 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m23:47:06.854169 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m23:47:06.854169 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-010dfb28-29bf-4bc1-ac9a-03c37693489d
[0m23:47:07.954800 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:47:07.955807 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:47:07.956819 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:47:07.956819 [debug] [MainThread]: On master: ROLLBACK
[0m23:47:07.957746 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:47:07.957746 [debug] [MainThread]: On master: Close
[0m23:47:07.958745 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m23:47:07.958745 [info ] [MainThread]: 
[0m23:47:07.959745 [info ] [MainThread]: Finished running 1 table model in 0 hours 1 minutes and 14.56 seconds (74.56s).
[0m23:47:07.960733 [debug] [MainThread]: Glue adapter: cleanup called
[0m23:47:08.290702 [info ] [MainThread]: 
[0m23:47:08.290702 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m23:47:08.291952 [info ] [MainThread]: 
[0m23:47:08.291952 [error] [MainThread]: [33mDatabase Error in model cleansed_order (models\metrics\cleansed_order.sql)[0m
[0m23:47:08.292958 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
[0m23:47:08.292958 [error] [MainThread]:   
[0m23:47:08.293957 [error] [MainThread]:     
[0m23:47:08.293957 [error] [MainThread]:       
[0m23:47:08.293957 [error] [MainThread]:       	create table hudidb.cleansed_order
[0m23:47:08.293957 [error] [MainThread]:       
[0m23:47:08.294957 [error] [MainThread]:       
[0m23:47:08.294957 [error] [MainThread]:       using hudi
[0m23:47:08.294957 [error] [MainThread]:       
[0m23:47:08.295956 [error] [MainThread]:       
[0m23:47:08.295956 [error] [MainThread]:       
[0m23:47:08.295956 [error] [MainThread]:       LOCATION 's3://soumil-dms-learn/hudi/hudidb/cleansed_order/'
[0m23:47:08.296957 [error] [MainThread]:       
[0m23:47:08.296957 [error] [MainThread]:   	as
[0m23:47:08.297958 [error] [MainThread]:   	
[0m23:47:08.297958 [error] [MainThread]:   
[0m23:47:08.298957 [error] [MainThread]:   
[0m23:47:08.298957 [error] [MainThread]:   with source_data as (
[0m23:47:08.299969 [error] [MainThread]:       SELECT
[0m23:47:08.299969 [error] [MainThread]:           invoiceid ,
[0m23:47:08.299969 [error] [MainThread]:           category ,
[0m23:47:08.300957 [error] [MainThread]:           destinationstate,
[0m23:47:08.300957 [error] [MainThread]:           itemid
[0m23:47:08.300957 [error] [MainThread]:       FROM hudidb.order
[0m23:47:08.301956 [error] [MainThread]:   )
[0m23:47:08.301956 [error] [MainThread]:   
[0m23:47:08.301956 [error] [MainThread]:   select * from source_data
[0m23:47:08.301956 [error] [MainThread]:     '''), Py4JJavaError: An error occurred while calling o75.sql.
[0m23:47:08.302957 [error] [MainThread]:   : org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
[0m23:47:08.302957 [error] [MainThread]:   	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
[0m23:47:08.302957 [error] [MainThread]:   	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
[0m23:47:08.303957 [error] [MainThread]:   	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
[0m23:47:08.303957 [error] [MainThread]:   	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
[0m23:47:08.303957 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
[0m23:47:08.304957 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
[0m23:47:08.304957 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[0m23:47:08.304957 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[0m23:47:08.305957 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
[0m23:47:08.305957 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
[0m23:47:08.305957 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
[0m23:47:08.306957 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
[0m23:47:08.306957 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
[0m23:47:08.306957 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
[0m23:47:08.307957 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
[0m23:47:08.307957 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
[0m23:47:08.307957 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
[0m23:47:08.309075 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
[0m23:47:08.309075 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
[0m23:47:08.309075 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
[0m23:47:08.310079 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m23:47:08.310079 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
[0m23:47:08.310079 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
[0m23:47:08.311079 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
[0m23:47:08.311079 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
[0m23:47:08.311079 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m23:47:08.311079 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
[0m23:47:08.312080 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
[0m23:47:08.312080 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m23:47:08.312080 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
[0m23:47:08.313080 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[0m23:47:08.314079 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[0m23:47:08.314079 [error] [MainThread]:   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[0m23:47:08.315084 [error] [MainThread]:   	at java.lang.reflect.Method.invoke(Method.java:498)
[0m23:47:08.315084 [error] [MainThread]:   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[0m23:47:08.315084 [error] [MainThread]:   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[0m23:47:08.316080 [error] [MainThread]:   	at py4j.Gateway.invoke(Gateway.java:282)
[0m23:47:08.316080 [error] [MainThread]:   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[0m23:47:08.316080 [error] [MainThread]:   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[0m23:47:08.317080 [error] [MainThread]:   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[0m23:47:08.317080 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:750)
[0m23:47:08.317080 [error] [MainThread]:   
[0m23:47:08.318079 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m23:47:08.318079 [info ] [MainThread]: 
[0m23:47:08.318079 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m23:47:08.319079 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A0807F7F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A08671900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025A086110C0>]}
[0m23:47:08.319079 [debug] [MainThread]: Flushing usage events
[0m23:47:08.510653 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-22 23:52:37.838976 | aa6c5b02-d283-4381-b011-b139c3a7bc58 ==============================
[0m23:52:37.838976 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:52:37.839970 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:52:37.839970 [debug] [MainThread]: Tracking: tracking
[0m23:52:37.862982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4C574D6C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4C574DB70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4C574D7B0>]}
[0m23:52:37.942028 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:52:37.942028 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m23:52:37.954021 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m23:52:37.991028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'aa6c5b02-d283-4381-b011-b139c3a7bc58', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4C5924790>]}
[0m23:52:37.998028 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'aa6c5b02-d283-4381-b011-b139c3a7bc58', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4C590FF10>]}
[0m23:52:37.999032 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m23:52:37.999032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aa6c5b02-d283-4381-b011-b139c3a7bc58', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4C58F6D40>]}
[0m23:52:38.000030 [info ] [MainThread]: 
[0m23:52:38.001032 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:52:38.002029 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m23:52:38.003023 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:52:38.003023 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:52:38.003023 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m23:52:38.003023 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m23:53:39.564561 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:53:39.564561 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-0333030a-de38-439f-afd6-3a36fb472b27
[0m23:53:48.262249 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:53:48.831615 [debug] [ThreadPool]: On list_hudidb: Close
[0m23:53:48.831615 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:53:48.833611 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m23:53:48.833611 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:53:48.833611 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:53:49.295948 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m23:53:49.295948 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:53:49.296949 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-0333030a-de38-439f-afd6-3a36fb472b27
[0m23:53:50.453598 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:53:50.968189 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m23:53:50.968189 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:53:50.969188 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'aa6c5b02-d283-4381-b011-b139c3a7bc58', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D4C5959FC0>]}
[0m23:53:50.969188 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:53:50.969188 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:53:50.970189 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:53:50.971189 [info ] [MainThread]: 
[0m23:53:50.976205 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m23:53:50.977191 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m23:53:50.978190 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m23:53:50.978190 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m23:53:50.978190 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m23:53:50.982192 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m23:53:50.983190 [debug] [Thread-1 (]: finished collecting timing info
[0m23:53:50.983190 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m23:53:51.004198 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:53:51.004198 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m23:53:51.466648 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m23:53:51.467661 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m23:53:51.467661 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-0333030a-de38-439f-afd6-3a36fb472b27
[0m23:53:52.294201 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:53:52.832015 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:53:52.839008 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m23:53:52.839008 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:53:52.839008 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m23:53:52.840015 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:53:52.840015 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:53:52.840015 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:53:52.840015 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:53:52.841022 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')


============================== 2022-12-22 23:54:28.563227 | 802732e7-2f5f-47fc-a971-a5182570e9ce ==============================
[0m23:54:28.563227 [info ] [MainThread]: Running with dbt=1.3.1
[0m23:54:28.564229 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m23:54:28.564229 [debug] [MainThread]: Tracking: tracking
[0m23:54:28.588220 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291AFBFD540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291AFBFD900>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291AFBFD6C0>]}
[0m23:54:28.677223 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m23:54:28.678222 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m23:54:28.692230 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m23:54:28.731223 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '802732e7-2f5f-47fc-a971-a5182570e9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291AFDD4820>]}
[0m23:54:28.738225 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '802732e7-2f5f-47fc-a971-a5182570e9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291AFDBFFD0>]}
[0m23:54:28.739221 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m23:54:28.740221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '802732e7-2f5f-47fc-a971-a5182570e9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291AFDA6DD0>]}
[0m23:54:28.741220 [info ] [MainThread]: 
[0m23:54:28.742222 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:54:28.743222 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m23:54:28.744227 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:54:28.744227 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:54:28.744227 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m23:54:28.744227 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m23:55:12.945516 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:55:12.946509 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-0113ac77-43f0-4a83-8dce-b07669332bb2
[0m23:55:21.069668 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:55:21.620222 [debug] [ThreadPool]: On list_hudidb: Close
[0m23:55:21.621222 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:55:21.622222 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m23:55:21.623222 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m23:55:21.623222 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m23:55:22.094575 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m23:55:22.094575 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m23:55:22.095641 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-0113ac77-43f0-4a83-8dce-b07669332bb2
[0m23:55:22.857377 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m23:55:23.347435 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m23:55:23.348436 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m23:55:23.348436 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '802732e7-2f5f-47fc-a971-a5182570e9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291AFDE5A20>]}
[0m23:55:23.349437 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:55:23.349437 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:55:23.350444 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m23:55:23.352435 [info ] [MainThread]: 
[0m23:55:23.355877 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m23:55:23.356882 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m23:55:23.356882 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m23:55:23.357881 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m23:55:23.357881 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m23:55:23.361882 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m23:55:23.362882 [debug] [Thread-1 (]: finished collecting timing info
[0m23:55:23.362882 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m23:55:23.382882 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m23:55:23.382882 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m23:55:23.853642 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m23:55:23.853642 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m23:55:23.854635 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-0113ac77-43f0-4a83-8dce-b07669332bb2
[0m23:55:24.662834 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:55:25.238983 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:55:25.245982 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m23:55:25.245982 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:55:25.245982 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m23:55:25.246976 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:55:25.246976 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:55:25.246976 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:55:25.246976 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:55:25.246976 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m23:55:40.262487 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671753325498, 'CompletedOn': 1671753340226}, 'ResponseMetadata': {'RequestId': '8e872c10-f498-4333-912c-f9dc1d0e808e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:55:40 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': '8e872c10-f498-4333-912c-f9dc1d0e808e'}, 'RetryAttempts': 0}}
[0m23:55:40.263497 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m23:55:40.263497 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m23:55:40.263497 [debug] [Thread-1 (]: SQL status: OK in 15.02 seconds
[0m23:55:40.267494 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:55:40.270494 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:55:40.824986 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m23:55:40.825979 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m23:55:40.825979 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:55:40.825979 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:55:40.826988 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:55:40.826988 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select * from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m23:56:06.305568 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect * from source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221222235548198", "_hoodie_commit_seqno": "20221222235548198_5_1", "_hoodie_record_key": "3", "_hoodie_partition_path": "", "_hoodie_file_name": "4f257f49-1972-4def-9867-7cabb3514720-0_5-16-0_20221222235548198.parquet", "invoiceid": 3, "category": "Kitchen", "destinationstate": "GA", "itemid": 60, "update_hudi_ts": "2022-12-22 23:55:50.183000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoiceid", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "destinationstate", "type": "StringType"}, {"name": "itemid", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671753341056, 'CompletedOn': 1671753366198}, 'ResponseMetadata': {'RequestId': 'da18b4b9-9a55-49d5-8a60-a1f830bb4c3e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:56:06 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3726', 'connection': 'keep-alive', 'x-amzn-requestid': 'da18b4b9-9a55-49d5-8a60-a1f830bb4c3e'}, 'RetryAttempts': 0}}
[0m23:56:06.305568 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m23:56:06.306576 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m23:56:06.306576 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m23:56:06.307576 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m23:56:06.307576 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m23:56:06.307576 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m23:56:06.307576 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m23:56:06.308576 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m23:56:06.308576 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m23:56:06.308576 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m23:56:07.882605 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221222235548198", "_hoodie_commit_seqno": "20221222235548198_6_16", "_hoodie_record_key": "16184", "_hoodie_partition_path": "", "_hoodie_file_name": "b1bc78c9-bb8f-4aff-ab7d-09e0c5e2ac80-0_6-13-0_20221222235548198.parquet", "invoiceid": 16184, "category": "Household", "destinationstate": "OH", "itemid": 47, "update_hudi_ts": "2022-12-22 23:55:50.183000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoiceid", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "destinationstate", "type": "StringType"}, {"name": "itemid", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671753366545, 'CompletedOn': 1671753367661}, 'ResponseMetadata': {'RequestId': 'adf7e28a-ec38-42ac-acbb-29081a0baa70', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Thu, 22 Dec 2022 23:56:07 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1536', 'connection': 'keep-alive', 'x-amzn-requestid': 'adf7e28a-ec38-42ac-acbb-29081a0baa70'}, 'RetryAttempts': 0}}
[0m23:56:07.883658 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m23:56:07.883658 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m23:56:07.884655 [debug] [Thread-1 (]: SQL status: OK in 1.58 seconds
[0m23:56:07.886656 [debug] [Thread-1 (]: finished collecting timing info
[0m23:56:07.887657 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m23:56:07.888648 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m23:56:07.888648 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m23:56:07.888648 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m23:56:07.889588 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '802732e7-2f5f-47fc-a971-a5182570e9ce', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291B03B3AC0>]}
[0m23:56:07.889588 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 44.53s]
[0m23:56:07.890605 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m23:56:07.891592 [debug] [MainThread]: Acquiring new glue connection "master"
[0m23:56:07.891592 [debug] [MainThread]: On master: ROLLBACK
[0m23:56:07.892592 [debug] [MainThread]: Opening a new connection, currently in state init
[0m23:56:07.892592 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m23:56:08.361201 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m23:56:08.361201 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m23:56:08.362187 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-0113ac77-43f0-4a83-8dce-b07669332bb2
[0m23:56:09.194181 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:56:09.195181 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m23:56:09.195181 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m23:56:09.195181 [debug] [MainThread]: On master: ROLLBACK
[0m23:56:09.195181 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m23:56:09.196181 [debug] [MainThread]: On master: Close
[0m23:56:09.196181 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m23:56:09.196181 [info ] [MainThread]: 
[0m23:56:09.197182 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 40.45 seconds (100.45s).
[0m23:56:09.197182 [debug] [MainThread]: Glue adapter: cleanup called
[0m23:56:09.485439 [info ] [MainThread]: 
[0m23:56:09.486451 [info ] [MainThread]: [32mCompleted successfully[0m
[0m23:56:09.486451 [info ] [MainThread]: 
[0m23:56:09.487446 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m23:56:09.488445 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291AFDE56F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291B0A728C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000291B0A71600>]}
[0m23:56:09.488445 [debug] [MainThread]: Flushing usage events
[0m23:56:09.652593 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 13:45:19.643058 | bf385f37-c47b-4bc0-af16-4c06cb9edc73 ==============================
[0m13:45:19.643058 [info ] [MainThread]: Running with dbt=1.3.1
[0m13:45:19.644057 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:45:19.644057 [debug] [MainThread]: Tracking: tracking
[0m13:45:19.666057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB3F2D6C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB3F2DB70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB3F2D7B0>]}
[0m13:45:19.768059 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:45:19.769059 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m13:45:19.782059 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m13:45:19.824058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bf385f37-c47b-4bc0-af16-4c06cb9edc73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB4108790>]}
[0m13:45:19.831059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bf385f37-c47b-4bc0-af16-4c06cb9edc73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB40EFF10>]}
[0m13:45:19.831059 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m13:45:19.832063 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bf385f37-c47b-4bc0-af16-4c06cb9edc73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB40D6D40>]}
[0m13:45:19.833059 [info ] [MainThread]: 
[0m13:45:19.834060 [debug] [MainThread]: Acquiring new glue connection "master"
[0m13:45:19.836058 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m13:45:19.836058 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:45:19.836058 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m13:45:19.836058 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m13:45:19.837058 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m13:46:04.377824 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m13:46:04.377824 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-fb466a12-9b9a-424b-8187-bc49ec2405b3
[0m13:46:12.508418 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m13:46:13.049418 [debug] [ThreadPool]: On list_hudidb: Close
[0m13:46:13.049418 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m13:46:13.050418 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m13:46:13.051418 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:46:13.051418 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m13:46:13.537925 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m13:46:13.538925 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m13:46:13.538925 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-fb466a12-9b9a-424b-8187-bc49ec2405b3
[0m13:46:14.590103 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m13:46:15.136103 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m13:46:15.136103 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m13:46:15.137102 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bf385f37-c47b-4bc0-af16-4c06cb9edc73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB41184C0>]}
[0m13:46:15.137102 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m13:46:15.137102 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m13:46:15.138102 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:46:15.138102 [info ] [MainThread]: 
[0m13:46:15.144102 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m13:46:15.144102 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m13:46:15.145102 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m13:46:15.145102 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m13:46:15.145102 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m13:46:15.149102 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m13:46:15.150102 [debug] [Thread-1 (]: finished collecting timing info
[0m13:46:15.150102 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m13:46:15.171103 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:46:15.171103 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m13:46:15.666967 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m13:46:15.666967 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m13:46:15.666967 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-fb466a12-9b9a-424b-8187-bc49ec2405b3
[0m13:46:16.478467 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:46:17.014985 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m13:46:17.021986 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m13:46:17.022984 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m13:46:17.022984 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m13:46:17.022984 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:46:17.023984 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m13:46:17.023984 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m13:46:17.023984 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m13:46:17.023984 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m13:46:39.024883 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671803177664, 'CompletedOn': 1671803199307}, 'ResponseMetadata': {'RequestId': '0ed4438d-8b1f-4ad6-bce3-385eece9c9b2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 13:46:39 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': '0ed4438d-8b1f-4ad6-bce3-385eece9c9b2'}, 'RetryAttempts': 0}}
[0m13:46:39.025810 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m13:46:39.025810 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m13:46:39.026810 [debug] [Thread-1 (]: SQL status: OK in 22.0 seconds
[0m13:46:39.030810 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:46:39.034810 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:46:39.904191 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m13:46:39.905182 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    source_data.invoiceid as invoice_id,
    source_data.category as category
    source_data.destinationstate as state
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m13:46:39.905182 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m13:46:39.905182 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m13:46:39.906181 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m13:46:39.906181 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    source_data.invoiceid as invoice_id,
    source_data.category as category
    source_data.destinationstate as state
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m13:46:40.731142 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    source_data.invoiceid as invoice_id,\n    source_data.category as category\n    source_data.destinationstate as state\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 7, 'Status': 'error', 'ErrorName': 'ParseException', 'ErrorValue': "\nmismatched input 'source_data' expecting {<EOF>, ';'}(line 18, pos 4)\n\n== SQL ==\n\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    source_data.invoiceid as invoice_id,\n    source_data.category as category\n    source_data.destinationstate as state\n----^^^\n    source_data.itemid as item_id\nfrom source_data\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.ParseException: \nmismatched input 'source_data' expecting {<EOF>, ';'}(line 18, pos 4)\n\n== SQL ==\n\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    source_data.invoiceid as invoice_id,\n    source_data.category as category\n    source_data.destinationstate as state\n----^^^\n    source_data.itemid as item_id\nfrom source_data\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671803200614, 'CompletedOn': 1671803200782}, 'ResponseMetadata': {'RequestId': 'a09deaf8-3eb2-4784-9e79-cd9fd8dd0e25', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 13:46:41 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4240', 'connection': 'keep-alive', 'x-amzn-requestid': 'a09deaf8-3eb2-4784-9e79-cd9fd8dd0e25'}, 'RetryAttempts': 0}}
[0m13:46:40.732066 [debug] [Thread-1 (]: Glue adapter: status = error
[0m13:46:40.732066 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    source_data.invoiceid as invoice_id,
    source_data.category as category
    source_data.destinationstate as state
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
, ParseException: 
mismatched input 'source_data' expecting {<EOF>, ';'}(line 18, pos 4)

== SQL ==





with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    source_data.invoiceid as invoice_id,
    source_data.category as category
    source_data.destinationstate as state
----^^^
    source_data.itemid as item_id
from source_data

[0m13:46:40.735143 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    source_data.invoiceid as invoice_id,
    source_data.category as category
    source_data.destinationstate as state
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
, ParseException: 
mismatched input 'source_data' expecting {<EOF>, ';'}(line 18, pos 4)

== SQL ==





with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    source_data.invoiceid as invoice_id,
    source_data.category as category
    source_data.destinationstate as state
----^^^
    source_data.itemid as item_id
from source_data

[0m13:46:40.735143 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""
  
  
  
  
  with source_data as (
      SELECT
          invoiceid ,
          category ,
          destinationstate,
          itemid
      FROM hudidb.order
  )
  
  select
      source_data.invoiceid as invoice_id,
      source_data.category as category
      source_data.destinationstate as state
      source_data.itemid as item_id
  from source_data""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if None is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
  
  spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
  SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
  , ParseException: 
  mismatched input 'source_data' expecting {<EOF>, ';'}(line 18, pos 4)
  
  == SQL ==
  
  
  
  
  
  with source_data as (
      SELECT
          invoiceid ,
          category ,
          destinationstate,
          itemid
      FROM hudidb.order
  )
  
  select
      source_data.invoiceid as invoice_id,
      source_data.category as category
      source_data.destinationstate as state
  ----^^^
      source_data.itemid as item_id
  from source_data
  
[0m13:46:40.739066 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m13:46:40.740066 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m13:46:40.740066 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m13:46:40.741066 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:46:40.741066 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m13:46:40.741066 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m13:46:40.741066 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m13:46:40.742066 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m13:46:42.353079 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 8, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: hudidb.cleansed_order; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671803201360, 'CompletedOn': 1671803202070}, 'ResponseMetadata': {'RequestId': '4f54d35f-4a0b-40a3-b735-c261a6eed3bf', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 13:46:42 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1428', 'connection': 'keep-alive', 'x-amzn-requestid': '4f54d35f-4a0b-40a3-b735-c261a6eed3bf'}, 'RetryAttempts': 0}}
[0m13:46:42.354079 [debug] [Thread-1 (]: Glue adapter: status = error
[0m13:46:42.354079 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false

[0m13:46:42.355083 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false

[0m13:46:42.355083 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m13:46:42.355083 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m13:46:42.355083 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m13:46:42.355083 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m13:46:42.355083 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m13:46:42.356079 [debug] [Thread-1 (]: finished collecting timing info
[0m13:46:42.356079 [debug] [Thread-1 (]: Database Error in model cleansed_order (models\metrics\cleansed_order.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
  select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [hudidb, cleansed_order], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m13:46:42.357079 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bf385f37-c47b-4bc0-af16-4c06cb9edc73', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB4C43040>]}
[0m13:46:42.357079 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model hudidb.cleansed_order .............. [[31mERROR[0m in 27.21s]
[0m13:46:42.357079 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m13:46:42.359079 [debug] [MainThread]: Acquiring new glue connection "master"
[0m13:46:42.359079 [debug] [MainThread]: On master: ROLLBACK
[0m13:46:42.359079 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:46:42.359079 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m13:46:42.831589 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m13:46:42.831589 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m13:46:42.831589 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-fb466a12-9b9a-424b-8187-bc49ec2405b3
[0m13:46:43.633630 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m13:46:43.634631 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m13:46:43.634631 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m13:46:43.634631 [debug] [MainThread]: On master: ROLLBACK
[0m13:46:43.635631 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m13:46:43.635631 [debug] [MainThread]: On master: Close
[0m13:46:43.635631 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m13:46:43.636631 [info ] [MainThread]: 
[0m13:46:43.636631 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 23.80 seconds (83.80s).
[0m13:46:43.637630 [debug] [MainThread]: Glue adapter: cleanup called
[0m13:46:43.885629 [info ] [MainThread]: 
[0m13:46:43.886631 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:46:43.887630 [info ] [MainThread]: 
[0m13:46:43.887630 [error] [MainThread]: [33mDatabase Error in model cleansed_order (models\metrics\cleansed_order.sql)[0m
[0m13:46:43.888632 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
[0m13:46:43.888632 [error] [MainThread]:   select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
[0m13:46:43.889630 [error] [MainThread]:   'GlobalLimit 1
[0m13:46:43.889630 [error] [MainThread]:   +- 'LocalLimit 1
[0m13:46:43.890630 [error] [MainThread]:      +- 'Project [*]
[0m13:46:43.891632 [error] [MainThread]:         +- 'UnresolvedRelation [hudidb, cleansed_order], [], false
[0m13:46:43.891632 [error] [MainThread]:   
[0m13:46:43.892631 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m13:46:43.892631 [info ] [MainThread]: 
[0m13:46:43.893630 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:46:43.893630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB4118E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB4DA6500>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017BB4DA64D0>]}
[0m13:46:43.894635 [debug] [MainThread]: Flushing usage events
[0m13:46:44.078311 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 13:47:24.985126 | ca1cad93-c141-4b90-9823-3fcc900f53de ==============================
[0m13:47:24.985126 [info ] [MainThread]: Running with dbt=1.3.1
[0m13:47:24.986126 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:47:24.986126 [debug] [MainThread]: Tracking: tracking
[0m13:47:25.011125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A565C20A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A562B1BDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A565C20490>]}
[0m13:47:25.099125 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:47:25.100125 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m13:47:25.111124 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m13:47:25.151126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ca1cad93-c141-4b90-9823-3fcc900f53de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A565DF47F0>]}
[0m13:47:25.158126 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ca1cad93-c141-4b90-9823-3fcc900f53de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A565DDFF10>]}
[0m13:47:25.159126 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m13:47:25.160130 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca1cad93-c141-4b90-9823-3fcc900f53de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002A565DC6DA0>]}
[0m13:47:25.161126 [info ] [MainThread]: 
[0m13:47:25.162126 [debug] [MainThread]: Acquiring new glue connection "master"
[0m13:47:25.164126 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m13:47:25.164126 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:47:25.165126 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m13:47:25.165126 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m13:47:25.165126 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m13:47:51.944441 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m13:47:51.944441 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-c8a8753d-448e-4b5c-8e32-ae91eaa1dc8e
[0m13:47:58.838579 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m13:47:59.393260 [debug] [ThreadPool]: On list_hudidb: Close
[0m13:47:59.393260 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m13:47:59.395260 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m13:47:59.395260 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:47:59.396260 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m13:47:59.895039 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m13:47:59.896038 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m13:47:59.896038 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-c8a8753d-448e-4b5c-8e32-ae91eaa1dc8e


============================== 2022-12-23 13:48:12.773931 | 1ecf61b1-9264-48f2-8354-2d94d3f6d144 ==============================
[0m13:48:12.773931 [info ] [MainThread]: Running with dbt=1.3.1
[0m13:48:12.774931 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:48:12.774931 [debug] [MainThread]: Tracking: tracking
[0m13:48:12.799928 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAE61D540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAE61D8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAE61D690>]}
[0m13:48:12.886931 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:48:12.887933 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m13:48:12.900931 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m13:48:12.944933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1ecf61b1-9264-48f2-8354-2d94d3f6d144', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAE7F87C0>]}
[0m13:48:12.952933 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1ecf61b1-9264-48f2-8354-2d94d3f6d144', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAE7DFFD0>]}
[0m13:48:12.952933 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m13:48:12.953937 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ecf61b1-9264-48f2-8354-2d94d3f6d144', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAE7C6D70>]}
[0m13:48:12.954930 [info ] [MainThread]: 
[0m13:48:12.955940 [debug] [MainThread]: Acquiring new glue connection "master"
[0m13:48:12.956929 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m13:48:12.957939 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:48:12.957939 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m13:48:12.957939 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m13:48:12.958929 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m13:49:14.313399 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m13:49:14.314400 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-ffb2a5b3-b665-43fa-9b85-bb759bba7de6
[0m13:49:22.421912 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m13:49:22.964914 [debug] [ThreadPool]: On list_hudidb: Close
[0m13:49:22.964914 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m13:49:22.966913 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m13:49:22.966913 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:49:22.966913 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m13:49:23.437912 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m13:49:23.438912 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m13:49:23.438912 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-ffb2a5b3-b665-43fa-9b85-bb759bba7de6
[0m13:49:24.245926 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m13:49:24.789876 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m13:49:24.789876 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m13:49:24.790881 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1ecf61b1-9264-48f2-8354-2d94d3f6d144', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAE809300>]}
[0m13:49:24.791876 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m13:49:24.791876 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m13:49:24.791876 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:49:24.792875 [info ] [MainThread]: 
[0m13:49:24.797875 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m13:49:24.797875 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m13:49:24.798875 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m13:49:24.798875 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m13:49:24.798875 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m13:49:24.802875 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m13:49:24.803875 [debug] [Thread-1 (]: finished collecting timing info
[0m13:49:24.803875 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m13:49:24.824876 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:49:24.825876 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m13:49:25.310875 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m13:49:25.310875 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m13:49:25.310875 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-ffb2a5b3-b665-43fa-9b85-bb759bba7de6
[0m13:49:26.112553 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:49:26.675084 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m13:49:26.682084 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m13:49:26.683084 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m13:49:26.683084 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m13:49:26.683084 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:49:26.683084 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m13:49:26.684084 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m13:49:26.684084 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m13:49:26.684084 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m13:49:47.365378 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671803367302, 'CompletedOn': 1671803387040}, 'ResponseMetadata': {'RequestId': '0971503f-ce87-4bc8-a48e-ade3fa3e2ad4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 13:49:47 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': '0971503f-ce87-4bc8-a48e-ade3fa3e2ad4'}, 'RetryAttempts': 0}}
[0m13:49:47.365378 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m13:49:47.366379 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m13:49:47.366379 [debug] [Thread-1 (]: SQL status: OK in 20.68 seconds
[0m13:49:47.369379 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:49:47.373379 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:49:47.921378 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m13:49:47.921378 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m13:49:47.921378 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m13:49:47.921378 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m13:49:47.922378 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m13:49:47.922378 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m13:49:59.340710 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoiceid\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 7, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': 'An error occurred while calling o118.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 4 times, most recent failure: Lost task 1.3 in stage 7.0 (TID 12) (172.36.162.80 executor 1): org.apache.spark.SparkException: Failed to execute user defined function(UDFRegistration$$Lambda$3017/1630448292: (struct<invoice_id:bigint,category:string,state:string,item_id:bigint,update_hudi_ts:timestamp>) => string)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hudi.exception.HoodieKeyException: recordKey value: "null" for field: "invoiceid" cannot be null or empty.\n\tat org.apache.hudi.keygen.KeyGenUtils.getRecordKey(KeyGenUtils.java:141)\n\tat org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator.getRecordKey(NonpartitionedAvroKeyGenerator.java:60)\n\tat org.apache.hudi.keygen.NonpartitionedKeyGenerator.getRecordKey(NonpartitionedKeyGenerator.java:50)\n\tat org.apache.hudi.keygen.BaseKeyGenerator.getKey(BaseKeyGenerator.java:65)\n\tat org.apache.hudi.keygen.BuiltinKeyGenerator.getRecordKey(BuiltinKeyGenerator.java:75)\n\tat org.apache.spark.sql.UDFRegistration.$anonfun$register$352(UDFRegistration.scala:777)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2465)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2414)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2413)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2413)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2679)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2621)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2610)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:309)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:197)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:191)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:167)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$2(ShuffleExchangeExec.scala:100)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$1(ShuffleExchangeExec.scala:100)\n\tat org.apache.spark.sql.util.LazyValue.getOrInit(LazyValue.scala:41)\n\tat org.apache.spark.sql.execution.exchange.Exchange.getOrInitMaterializeFuture(Exchange.scala:68)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materializeFuture(ShuffleExchangeExec.scala:96)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize(ShuffleExchangeExec.scala:84)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize$(ShuffleExchangeExec.scala:83)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.materialize(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:161)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:74)\n\tat org.apache.spark.sql.execution.adaptive.MaterializeExecutable.tryStart(AdaptiveExecutable.scala:396)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.startChild(AdaptiveExecutor.scala:225)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper.start(ExecutionHelper.scala:47)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExecutable$$anon$2.$anonfun$new$1(AdaptiveExecutable.scala:251)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2(ExecutionHelper.scala:55)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2$adapted(ExecutionHelper.scala:54)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1(ExecutionHelper.scala:54)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1$adapted(ExecutionHelper.scala:53)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.onChildSuccess(ExecutionHelper.scala:53)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2(AdaptiveExecutor.scala:314)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2$adapted(AdaptiveExecutor.scala:314)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onActiveChildSuccess(AdaptiveExecutor.scala:314)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onChildSuccess(AdaptiveExecutor.scala:284)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1(AdaptiveExecutor.scala:92)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1$adapted(AdaptiveExecutor.scala:91)\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:145)\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:235)\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:228)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:145)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:434)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:338)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:218)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:370)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:482)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:162)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(UDFRegistration$$Lambda$3017/1630448292: (struct<invoice_id:bigint,category:string,state:string,item_id:bigint,update_hudi_ts:timestamp>) => string)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hudi.exception.HoodieKeyException: recordKey value: "null" for field: "invoiceid" cannot be null or empty.\n\tat org.apache.hudi.keygen.KeyGenUtils.getRecordKey(KeyGenUtils.java:141)\n\tat org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator.getRecordKey(NonpartitionedAvroKeyGenerator.java:60)\n\tat org.apache.hudi.keygen.NonpartitionedKeyGenerator.getRecordKey(NonpartitionedKeyGenerator.java:50)\n\tat org.apache.hudi.keygen.BaseKeyGenerator.getKey(BaseKeyGenerator.java:65)\n\tat org.apache.hudi.keygen.BuiltinKeyGenerator.getRecordKey(BuiltinKeyGenerator.java:75)\n\tat org.apache.spark.sql.UDFRegistration.$anonfun$register$352(UDFRegistration.scala:777)\n\t... 22 more\n', 'Traceback': ['Traceback (most recent call last):\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1109, in save\n    self._jwrite.save(path)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', 'py4j.protocol.Py4JJavaError: An error occurred while calling o118.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 4 times, most recent failure: Lost task 1.3 in stage 7.0 (TID 12) (172.36.162.80 executor 1): org.apache.spark.SparkException: Failed to execute user defined function(UDFRegistration$$Lambda$3017/1630448292: (struct<invoice_id:bigint,category:string,state:string,item_id:bigint,update_hudi_ts:timestamp>) => string)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hudi.exception.HoodieKeyException: recordKey value: "null" for field: "invoiceid" cannot be null or empty.\n\tat org.apache.hudi.keygen.KeyGenUtils.getRecordKey(KeyGenUtils.java:141)\n\tat org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator.getRecordKey(NonpartitionedAvroKeyGenerator.java:60)\n\tat org.apache.hudi.keygen.NonpartitionedKeyGenerator.getRecordKey(NonpartitionedKeyGenerator.java:50)\n\tat org.apache.hudi.keygen.BaseKeyGenerator.getKey(BaseKeyGenerator.java:65)\n\tat org.apache.hudi.keygen.BuiltinKeyGenerator.getRecordKey(BuiltinKeyGenerator.java:75)\n\tat org.apache.spark.sql.UDFRegistration.$anonfun$register$352(UDFRegistration.scala:777)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2465)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2414)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2413)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2413)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2679)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2621)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2610)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:309)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:197)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:191)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:167)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$2(ShuffleExchangeExec.scala:100)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$1(ShuffleExchangeExec.scala:100)\n\tat org.apache.spark.sql.util.LazyValue.getOrInit(LazyValue.scala:41)\n\tat org.apache.spark.sql.execution.exchange.Exchange.getOrInitMaterializeFuture(Exchange.scala:68)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materializeFuture(ShuffleExchangeExec.scala:96)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize(ShuffleExchangeExec.scala:84)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize$(ShuffleExchangeExec.scala:83)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.materialize(ShuffleExchangeExec.scala:128)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:161)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:74)\n\tat org.apache.spark.sql.execution.adaptive.MaterializeExecutable.tryStart(AdaptiveExecutable.scala:396)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.startChild(AdaptiveExecutor.scala:225)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper.start(ExecutionHelper.scala:47)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExecutable$$anon$2.$anonfun$new$1(AdaptiveExecutable.scala:251)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2(ExecutionHelper.scala:55)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2$adapted(ExecutionHelper.scala:54)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1(ExecutionHelper.scala:54)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1$adapted(ExecutionHelper.scala:53)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.onChildSuccess(ExecutionHelper.scala:53)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2(AdaptiveExecutor.scala:314)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2$adapted(AdaptiveExecutor.scala:314)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onActiveChildSuccess(AdaptiveExecutor.scala:314)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onChildSuccess(AdaptiveExecutor.scala:284)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1(AdaptiveExecutor.scala:92)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1$adapted(AdaptiveExecutor.scala:91)\n\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:145)\n\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:235)\n\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:228)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:145)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:91)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:434)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:338)\n\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:218)\n\tat org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:370)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:482)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:162)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function(UDFRegistration$$Lambda$3017/1630448292: (struct<invoice_id:bigint,category:string,state:string,item_id:bigint,update_hudi_ts:timestamp>) => string)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.hudi.exception.HoodieKeyException: recordKey value: "null" for field: "invoiceid" cannot be null or empty.\n\tat org.apache.hudi.keygen.KeyGenUtils.getRecordKey(KeyGenUtils.java:141)\n\tat org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator.getRecordKey(NonpartitionedAvroKeyGenerator.java:60)\n\tat org.apache.hudi.keygen.NonpartitionedKeyGenerator.getRecordKey(NonpartitionedKeyGenerator.java:50)\n\tat org.apache.hudi.keygen.BaseKeyGenerator.getKey(BaseKeyGenerator.java:65)\n\tat org.apache.hudi.keygen.BuiltinKeyGenerator.getRecordKey(BuiltinKeyGenerator.java:75)\n\tat org.apache.spark.sql.UDFRegistration.$anonfun$register$352(UDFRegistration.scala:777)\n\t... 22 more\n\n']}, 'Progress': 1.0, 'StartedOn': 1671803388523, 'CompletedOn': 1671803399502}, 'ResponseMetadata': {'RequestId': '0142b614-8dcc-4210-8f1e-3037f1663cac', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 13:49:59 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '43549', 'connection': 'keep-alive', 'x-amzn-requestid': '0142b614-8dcc-4210-8f1e-3037f1663cac'}, 'RetryAttempts': 0}}
[0m13:49:59.341710 [debug] [Thread-1 (]: Glue adapter: status = error
[0m13:49:59.341710 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
, Py4JJavaError: An error occurred while calling o118.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 4 times, most recent failure: Lost task 1.3 in stage 7.0 (TID 12) (172.36.162.80 executor 1): org.apache.spark.SparkException: Failed to execute user defined function(UDFRegistration$$Lambda$3017/1630448292: (struct<invoice_id:bigint,category:string,state:string,item_id:bigint,update_hudi_ts:timestamp>) => string)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)
	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.hudi.exception.HoodieKeyException: recordKey value: "null" for field: "invoiceid" cannot be null or empty.
	at org.apache.hudi.keygen.KeyGenUtils.getRecordKey(KeyGenUtils.java:141)
	at org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator.getRecordKey(NonpartitionedAvroKeyGenerator.java:60)
	at org.apache.hudi.keygen.NonpartitionedKeyGenerator.getRecordKey(NonpartitionedKeyGenerator.java:50)
	at org.apache.hudi.keygen.BaseKeyGenerator.getKey(BaseKeyGenerator.java:65)
	at org.apache.hudi.keygen.BuiltinKeyGenerator.getRecordKey(BuiltinKeyGenerator.java:75)
	at org.apache.spark.sql.UDFRegistration.$anonfun$register$352(UDFRegistration.scala:777)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2465)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2414)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2413)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2413)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2679)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2621)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2610)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)
	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:309)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:197)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:191)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:167)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:163)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$2(ShuffleExchangeExec.scala:100)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$1(ShuffleExchangeExec.scala:100)
	at org.apache.spark.sql.util.LazyValue.getOrInit(LazyValue.scala:41)
	at org.apache.spark.sql.execution.exchange.Exchange.getOrInitMaterializeFuture(Exchange.scala:68)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materializeFuture(ShuffleExchangeExec.scala:96)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize(ShuffleExchangeExec.scala:84)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize$(ShuffleExchangeExec.scala:83)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.materialize(ShuffleExchangeExec.scala:128)
	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:161)
	at org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:74)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:74)
	at org.apache.spark.sql.execution.adaptive.MaterializeExecutable.tryStart(AdaptiveExecutable.scala:396)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.startChild(AdaptiveExecutor.scala:225)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper.start(ExecutionHelper.scala:47)
	at org.apache.spark.sql.execution.adaptive.QueryStageExecutable$$anon$2.$anonfun$new$1(AdaptiveExecutable.scala:251)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2(ExecutionHelper.scala:55)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2$adapted(ExecutionHelper.scala:54)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1(ExecutionHelper.scala:54)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1$adapted(ExecutionHelper.scala:53)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.onChildSuccess(ExecutionHelper.scala:53)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2(AdaptiveExecutor.scala:314)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2$adapted(AdaptiveExecutor.scala:314)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onActiveChildSuccess(AdaptiveExecutor.scala:314)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onChildSuccess(AdaptiveExecutor.scala:284)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1(AdaptiveExecutor.scala:92)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1$adapted(AdaptiveExecutor.scala:91)
	at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:145)
	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:235)
	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:228)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:145)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:91)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:434)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:338)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:218)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:370)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:482)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:162)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Failed to execute user defined function(UDFRegistration$$Lambda$3017/1630448292: (struct<invoice_id:bigint,category:string,state:string,item_id:bigint,update_hudi_ts:timestamp>) => string)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)
	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.hudi.exception.HoodieKeyException: recordKey value: "null" for field: "invoiceid" cannot be null or empty.
	at org.apache.hudi.keygen.KeyGenUtils.getRecordKey(KeyGenUtils.java:141)
	at org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator.getRecordKey(NonpartitionedAvroKeyGenerator.java:60)
	at org.apache.hudi.keygen.NonpartitionedKeyGenerator.getRecordKey(NonpartitionedKeyGenerator.java:50)
	at org.apache.hudi.keygen.BaseKeyGenerator.getKey(BaseKeyGenerator.java:65)
	at org.apache.hudi.keygen.BuiltinKeyGenerator.getRecordKey(BuiltinKeyGenerator.java:75)
	at org.apache.spark.sql.UDFRegistration.$anonfun$register$352(UDFRegistration.scala:777)
	... 22 more

[0m13:49:59.460277 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
, Py4JJavaError: An error occurred while calling o118.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 4 times, most recent failure: Lost task 1.3 in stage 7.0 (TID 12) (172.36.162.80 executor 1): org.apache.spark.SparkException: Failed to execute user defined function(UDFRegistration$$Lambda$3017/1630448292: (struct<invoice_id:bigint,category:string,state:string,item_id:bigint,update_hudi_ts:timestamp>) => string)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)
	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.hudi.exception.HoodieKeyException: recordKey value: "null" for field: "invoiceid" cannot be null or empty.
	at org.apache.hudi.keygen.KeyGenUtils.getRecordKey(KeyGenUtils.java:141)
	at org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator.getRecordKey(NonpartitionedAvroKeyGenerator.java:60)
	at org.apache.hudi.keygen.NonpartitionedKeyGenerator.getRecordKey(NonpartitionedKeyGenerator.java:50)
	at org.apache.hudi.keygen.BaseKeyGenerator.getKey(BaseKeyGenerator.java:65)
	at org.apache.hudi.keygen.BuiltinKeyGenerator.getRecordKey(BuiltinKeyGenerator.java:75)
	at org.apache.spark.sql.UDFRegistration.$anonfun$register$352(UDFRegistration.scala:777)
	... 22 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2465)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2414)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2413)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2413)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2679)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2621)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2610)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)
	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:309)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:197)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:191)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:167)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:163)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$2(ShuffleExchangeExec.scala:100)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$1(ShuffleExchangeExec.scala:100)
	at org.apache.spark.sql.util.LazyValue.getOrInit(LazyValue.scala:41)
	at org.apache.spark.sql.execution.exchange.Exchange.getOrInitMaterializeFuture(Exchange.scala:68)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materializeFuture(ShuffleExchangeExec.scala:96)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize(ShuffleExchangeExec.scala:84)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize$(ShuffleExchangeExec.scala:83)
	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.materialize(ShuffleExchangeExec.scala:128)
	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:161)
	at org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:74)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:74)
	at org.apache.spark.sql.execution.adaptive.MaterializeExecutable.tryStart(AdaptiveExecutable.scala:396)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.startChild(AdaptiveExecutor.scala:225)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper.start(ExecutionHelper.scala:47)
	at org.apache.spark.sql.execution.adaptive.QueryStageExecutable$$anon$2.$anonfun$new$1(AdaptiveExecutable.scala:251)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2(ExecutionHelper.scala:55)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2$adapted(ExecutionHelper.scala:54)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1(ExecutionHelper.scala:54)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1$adapted(ExecutionHelper.scala:53)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.onChildSuccess(ExecutionHelper.scala:53)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2(AdaptiveExecutor.scala:314)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2$adapted(AdaptiveExecutor.scala:314)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onActiveChildSuccess(AdaptiveExecutor.scala:314)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onChildSuccess(AdaptiveExecutor.scala:284)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1(AdaptiveExecutor.scala:92)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1$adapted(AdaptiveExecutor.scala:91)
	at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:145)
	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:235)
	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:228)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:145)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:91)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)
	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)
	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:434)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:338)
	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:218)
	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:370)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:482)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:162)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)
Caused by: org.apache.spark.SparkException: Failed to execute user defined function(UDFRegistration$$Lambda$3017/1630448292: (struct<invoice_id:bigint,category:string,state:string,item_id:bigint,update_hudi_ts:timestamp>) => string)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)
	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more
Caused by: org.apache.hudi.exception.HoodieKeyException: recordKey value: "null" for field: "invoiceid" cannot be null or empty.
	at org.apache.hudi.keygen.KeyGenUtils.getRecordKey(KeyGenUtils.java:141)
	at org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator.getRecordKey(NonpartitionedAvroKeyGenerator.java:60)
	at org.apache.hudi.keygen.NonpartitionedKeyGenerator.getRecordKey(NonpartitionedKeyGenerator.java:50)
	at org.apache.hudi.keygen.BaseKeyGenerator.getKey(BaseKeyGenerator.java:65)
	at org.apache.hudi.keygen.BuiltinKeyGenerator.getRecordKey(BuiltinKeyGenerator.java:75)
	at org.apache.spark.sql.UDFRegistration.$anonfun$register$352(UDFRegistration.scala:777)
	... 22 more

[0m13:49:59.461274 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""
  
  
  
  
  with source_data as (
      SELECT
          invoiceid ,
          category ,
          destinationstate,
          itemid
      FROM hudidb.order
  )
  
  select
      invoiceid as invoice_id,
      source_data.category as category,
      source_data.destinationstate as state,
      source_data.itemid as item_id
  from source_data""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if None is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoiceid', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
  
  spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
  SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
  , Py4JJavaError: An error occurred while calling o118.save.
  : org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 7.0 failed 4 times, most recent failure: Lost task 1.3 in stage 7.0 (TID 12) (172.36.162.80 executor 1): org.apache.spark.SparkException: Failed to execute user defined function(UDFRegistration$$Lambda$3017/1630448292: (struct<invoice_id:bigint,category:string,state:string,item_id:bigint,update_hudi_ts:timestamp>) => string)
  	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
  	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
  	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
  	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
  	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
  	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)
  	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)
  	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)
  	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)
  	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
  	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
  	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  	at org.apache.spark.scheduler.Task.run(Task.scala:131)
  	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
  	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
  	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	at java.lang.Thread.run(Thread.java:750)
  Caused by: org.apache.hudi.exception.HoodieKeyException: recordKey value: "null" for field: "invoiceid" cannot be null or empty.
  	at org.apache.hudi.keygen.KeyGenUtils.getRecordKey(KeyGenUtils.java:141)
  	at org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator.getRecordKey(NonpartitionedAvroKeyGenerator.java:60)
  	at org.apache.hudi.keygen.NonpartitionedKeyGenerator.getRecordKey(NonpartitionedKeyGenerator.java:50)
  	at org.apache.hudi.keygen.BaseKeyGenerator.getKey(BaseKeyGenerator.java:65)
  	at org.apache.hudi.keygen.BuiltinKeyGenerator.getRecordKey(BuiltinKeyGenerator.java:75)
  	at org.apache.spark.sql.UDFRegistration.$anonfun$register$352(UDFRegistration.scala:777)
  	... 22 more
  
  Driver stacktrace:
  	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2465)
  	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2414)
  	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2413)
  	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
  	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2413)
  	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)
  	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)
  	at scala.Option.foreach(Option.scala:257)
  	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)
  	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2679)
  	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2621)
  	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2610)
  	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
  	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:914)
  	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)
  	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)
  	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)
  	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
  	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)
  	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  	at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
  	at org.apache.spark.rdd.RDD.collect(RDD.scala:1029)
  	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)
  	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:171)
  	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:309)
  	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:197)
  	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:191)
  	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:167)
  	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:163)
  	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$2(ShuffleExchangeExec.scala:100)
  	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
  	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$1(ShuffleExchangeExec.scala:100)
  	at org.apache.spark.sql.util.LazyValue.getOrInit(LazyValue.scala:41)
  	at org.apache.spark.sql.execution.exchange.Exchange.getOrInitMaterializeFuture(Exchange.scala:68)
  	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materializeFuture(ShuffleExchangeExec.scala:96)
  	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize(ShuffleExchangeExec.scala:84)
  	at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize$(ShuffleExchangeExec.scala:83)
  	at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.materialize(ShuffleExchangeExec.scala:128)
  	at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:161)
  	at org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:74)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
  	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
  	at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:74)
  	at org.apache.spark.sql.execution.adaptive.MaterializeExecutable.tryStart(AdaptiveExecutable.scala:396)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.startChild(AdaptiveExecutor.scala:225)
  	at org.apache.spark.sql.execution.adaptive.ExecutionHelper.start(ExecutionHelper.scala:47)
  	at org.apache.spark.sql.execution.adaptive.QueryStageExecutable$$anon$2.$anonfun$new$1(AdaptiveExecutable.scala:251)
  	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2(ExecutionHelper.scala:55)
  	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2$adapted(ExecutionHelper.scala:54)
  	at scala.Option.foreach(Option.scala:257)
  	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1(ExecutionHelper.scala:54)
  	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1$adapted(ExecutionHelper.scala:53)
  	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
  	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  	at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.onChildSuccess(ExecutionHelper.scala:53)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2(AdaptiveExecutor.scala:314)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2$adapted(AdaptiveExecutor.scala:314)
  	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:58)
  	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:51)
  	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onActiveChildSuccess(AdaptiveExecutor.scala:314)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onChildSuccess(AdaptiveExecutor.scala:284)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1(AdaptiveExecutor.scala:92)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1$adapted(AdaptiveExecutor.scala:91)
  	at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:145)
  	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:235)
  	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:228)
  	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
  	at scala.collection.mutable.HashMap.foreach(HashMap.scala:145)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:91)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)
  	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.doExecute(AdaptiveSparkPlanExec.scala:434)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
  	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
  	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
  	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:338)
  	at org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:336)
  	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.writeWithV2(WriteToDataSourceV2Exec.scala:218)
  	at org.apache.spark.sql.execution.datasources.v2.AppendDataExec.run(WriteToDataSourceV2Exec.scala:225)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:40)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:40)
  	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.doExecute(V2CommandExec.scala:55)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
  	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
  	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
  	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
  	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:370)
  	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:301)
  	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:482)
  	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:162)
  	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
  	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
  	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
  	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
  	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
  	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
  	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
  	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  Caused by: org.apache.spark.SparkException: Failed to execute user defined function(UDFRegistration$$Lambda$3017/1630448292: (struct<invoice_id:bigint,category:string,state:string,item_id:bigint,update_hudi_ts:timestamp>) => string)
  	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.project_doConsume_0$(Unknown Source)
  	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
  	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
  	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:454)
  	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)
  	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)
  	at org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)
  	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:915)
  	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:915)
  	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
  	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
  	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  	at org.apache.spark.scheduler.Task.run(Task.scala:131)
  	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
  	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
  	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
  	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  	... 1 more
  Caused by: org.apache.hudi.exception.HoodieKeyException: recordKey value: "null" for field: "invoiceid" cannot be null or empty.
  	at org.apache.hudi.keygen.KeyGenUtils.getRecordKey(KeyGenUtils.java:141)
  	at org.apache.hudi.keygen.NonpartitionedAvroKeyGenerator.getRecordKey(NonpartitionedAvroKeyGenerator.java:60)
  	at org.apache.hudi.keygen.NonpartitionedKeyGenerator.getRecordKey(NonpartitionedKeyGenerator.java:50)
  	at org.apache.hudi.keygen.BaseKeyGenerator.getKey(BaseKeyGenerator.java:65)
  	at org.apache.hudi.keygen.BuiltinKeyGenerator.getRecordKey(BuiltinKeyGenerator.java:75)
  	at org.apache.spark.sql.UDFRegistration.$anonfun$register$352(UDFRegistration.scala:777)
  	... 22 more
  
[0m13:49:59.492860 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m13:49:59.493860 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m13:49:59.494860 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m13:49:59.494860 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:49:59.494860 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m13:49:59.494860 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m13:49:59.495860 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m13:49:59.495860 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m13:50:01.111026 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 8, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: hudidb.cleansed_order; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671803400119, 'CompletedOn': 1671803400476}, 'ResponseMetadata': {'RequestId': '2aa406c1-3eb1-49e4-b9fb-317ab1a2ec40', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 13:50:01 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1428', 'connection': 'keep-alive', 'x-amzn-requestid': '2aa406c1-3eb1-49e4-b9fb-317ab1a2ec40'}, 'RetryAttempts': 0}}
[0m13:50:01.112027 [debug] [Thread-1 (]: Glue adapter: status = error
[0m13:50:01.112027 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false

[0m13:50:01.113027 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, cleansed_order], [], false

[0m13:50:01.113027 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m13:50:01.114027 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m13:50:01.114027 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m13:50:01.114027 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m13:50:01.114027 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m13:50:01.115027 [debug] [Thread-1 (]: finished collecting timing info
[0m13:50:01.115027 [debug] [Thread-1 (]: Database Error in model cleansed_order (models\metrics\cleansed_order.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
  select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [hudidb, cleansed_order], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m13:50:01.116027 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1ecf61b1-9264-48f2-8354-2d94d3f6d144', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAF32EA70>]}
[0m13:50:01.116027 [error] [Thread-1 (]: 1 of 1 ERROR creating sql incremental model hudidb.cleansed_order .............. [[31mERROR[0m in 36.32s]
[0m13:50:01.117027 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m13:50:01.119026 [debug] [MainThread]: Acquiring new glue connection "master"
[0m13:50:01.119026 [debug] [MainThread]: On master: ROLLBACK
[0m13:50:01.119026 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:50:01.120027 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m13:50:01.611026 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m13:50:01.612029 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m13:50:01.612029 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-ffb2a5b3-b665-43fa-9b85-bb759bba7de6
[0m13:50:02.432614 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m13:50:02.432614 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m13:50:02.432614 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m13:50:02.433614 [debug] [MainThread]: On master: ROLLBACK
[0m13:50:02.433614 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m13:50:02.433614 [debug] [MainThread]: On master: Close
[0m13:50:02.433614 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m13:50:02.434614 [info ] [MainThread]: 
[0m13:50:02.434614 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 49.48 seconds (109.48s).
[0m13:50:02.435614 [debug] [MainThread]: Glue adapter: cleanup called
[0m13:50:02.760613 [info ] [MainThread]: 
[0m13:50:02.761613 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m13:50:02.761613 [info ] [MainThread]: 
[0m13:50:02.762613 [error] [MainThread]: [33mDatabase Error in model cleansed_order (models\metrics\cleansed_order.sql)[0m
[0m13:50:02.762613 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
[0m13:50:02.763613 [error] [MainThread]:   select * from hudidb.cleansed_order limit 1 '''), AnalysisException: Table or view not found: hudidb.cleansed_order; line 2 pos 14;
[0m13:50:02.763613 [error] [MainThread]:   'GlobalLimit 1
[0m13:50:02.763613 [error] [MainThread]:   +- 'LocalLimit 1
[0m13:50:02.764613 [error] [MainThread]:      +- 'Project [*]
[0m13:50:02.764613 [error] [MainThread]:         +- 'UnresolvedRelation [hudidb, cleansed_order], [], false
[0m13:50:02.765613 [error] [MainThread]:   
[0m13:50:02.765613 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\cleansed_order.sql
[0m13:50:02.765613 [info ] [MainThread]: 
[0m13:50:02.766613 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
[0m13:50:02.766613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAE809270>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAF492050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000022BAF492020>]}
[0m13:50:02.766613 [debug] [MainThread]: Flushing usage events
[0m13:50:02.935137 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 13:51:55.264805 | 707b54c8-66e0-494d-9413-1fb47f301386 ==============================
[0m13:51:55.264805 [info ] [MainThread]: Running with dbt=1.3.1
[0m13:51:55.265821 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:51:55.265821 [debug] [MainThread]: Tracking: tracking
[0m13:51:55.291802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A92E14D540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A92E14D8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A92E14D690>]}
[0m13:51:55.375799 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:51:55.376800 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m13:51:55.389813 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m13:51:55.439801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '707b54c8-66e0-494d-9413-1fb47f301386', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A92E3247C0>]}
[0m13:51:55.447804 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '707b54c8-66e0-494d-9413-1fb47f301386', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A92E30FFD0>]}
[0m13:51:55.448802 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m13:51:55.448802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '707b54c8-66e0-494d-9413-1fb47f301386', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A92E2F6D70>]}
[0m13:51:55.450810 [info ] [MainThread]: 
[0m13:51:55.450810 [debug] [MainThread]: Acquiring new glue connection "master"
[0m13:51:55.452808 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m13:51:55.452808 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:51:55.452808 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m13:51:55.452808 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m13:51:55.453800 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m13:52:40.401772 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m13:52:40.401772 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-d901d63f-1d0a-4dfb-80d1-69379875ad95
[0m13:52:48.554920 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m13:52:49.138918 [debug] [ThreadPool]: On list_hudidb: Close
[0m13:52:49.138918 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m13:52:49.140918 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m13:52:49.140918 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:52:49.140918 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m13:52:49.614137 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m13:52:49.614137 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m13:52:49.614137 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-d901d63f-1d0a-4dfb-80d1-69379875ad95
[0m13:52:50.431137 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m13:52:50.970987 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m13:52:50.971987 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m13:52:50.971987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '707b54c8-66e0-494d-9413-1fb47f301386', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A92E339240>]}
[0m13:52:50.972987 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m13:52:50.972987 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m13:52:50.973987 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m13:52:50.973987 [info ] [MainThread]: 
[0m13:52:50.978988 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m13:52:50.978988 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m13:52:50.979988 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m13:52:50.979988 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m13:52:50.980988 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m13:52:50.983987 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m13:52:50.984987 [debug] [Thread-1 (]: finished collecting timing info
[0m13:52:50.984987 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m13:52:51.010988 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m13:52:51.011989 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m13:52:51.495510 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m13:52:51.495510 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m13:52:51.496510 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-d901d63f-1d0a-4dfb-80d1-69379875ad95
[0m13:52:52.295510 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:52:52.813523 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m13:52:52.821523 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m13:52:52.821523 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m13:52:52.821523 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m13:52:52.822530 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:52:52.822530 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m13:52:52.822530 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m13:52:52.822530 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m13:52:52.823523 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m13:53:28.383134 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671803573430, 'CompletedOn': 1671803608677}, 'ResponseMetadata': {'RequestId': '44e7ea9e-7f5b-452d-ac60-cad2f55bd57f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 13:53:28 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': '44e7ea9e-7f5b-452d-ac60-cad2f55bd57f'}, 'RetryAttempts': 0}}
[0m13:53:28.383134 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m13:53:28.384134 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m13:53:28.384134 [debug] [Thread-1 (]: SQL status: OK in 35.56 seconds
[0m13:53:28.387133 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:53:28.390133 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:53:28.935161 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m13:53:28.935161 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m13:53:28.936161 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m13:53:28.936161 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m13:53:28.936161 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m13:53:28.937162 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m13:53:53.352066 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223135335118", "_hoodie_commit_seqno": "20221223135335118_6_22", "_hoodie_record_key": "16184", "_hoodie_partition_path": "", "_hoodie_file_name": "0528e5d1-9534-494b-b6fe-b857790d10f4-0_6-12-0_20221223135335118.parquet", "invoice_id": 16184, "category": "Kitchen", "state": "CO", "item_id": 100, "update_hudi_ts": "2022-12-23 13:53:37.388000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671803609584, 'CompletedOn': 1671803632817}, 'ResponseMetadata': {'RequestId': '00eb93c2-7423-47ef-9b29-df2c17e388c6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 13:53:53 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3867', 'connection': 'keep-alive', 'x-amzn-requestid': '00eb93c2-7423-47ef-9b29-df2c17e388c6'}, 'RetryAttempts': 0}}
[0m13:53:53.352066 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m13:53:53.352066 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m13:53:53.353066 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m13:53:53.354065 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m13:53:53.354065 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m13:53:53.354065 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m13:53:53.354065 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m13:53:53.355066 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m13:53:53.355066 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m13:53:53.355066 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m13:53:54.956290 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223135335118", "_hoodie_commit_seqno": "20221223135335118_6_22", "_hoodie_record_key": "16184", "_hoodie_partition_path": "", "_hoodie_file_name": "0528e5d1-9534-494b-b6fe-b857790d10f4-0_6-12-0_20221223135335118.parquet", "invoice_id": 16184, "category": "Kitchen", "state": "CO", "item_id": 100, "update_hudi_ts": "2022-12-23 13:53:37.388000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671803633972, 'CompletedOn': 1671803635076}, 'ResponseMetadata': {'RequestId': 'f15484d9-8dd4-4be1-ab1e-56e790de3a0a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 13:53:55 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1517', 'connection': 'keep-alive', 'x-amzn-requestid': 'f15484d9-8dd4-4be1-ab1e-56e790de3a0a'}, 'RetryAttempts': 0}}
[0m13:53:54.957290 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m13:53:54.957290 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m13:53:54.957290 [debug] [Thread-1 (]: SQL status: OK in 1.6 seconds
[0m13:53:54.958290 [debug] [Thread-1 (]: finished collecting timing info
[0m13:53:54.958290 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m13:53:54.958290 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m13:53:54.959291 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m13:53:54.959291 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m13:53:54.959291 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '707b54c8-66e0-494d-9413-1fb47f301386', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A92E903460>]}
[0m13:53:54.960290 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 63.98s]
[0m13:53:54.961290 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m13:53:54.962304 [debug] [MainThread]: Acquiring new glue connection "master"
[0m13:53:54.962304 [debug] [MainThread]: On master: ROLLBACK
[0m13:53:54.962304 [debug] [MainThread]: Opening a new connection, currently in state init
[0m13:53:54.963291 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m13:53:55.469311 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m13:53:55.470311 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m13:53:55.470311 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-d901d63f-1d0a-4dfb-80d1-69379875ad95
[0m13:53:56.242411 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m13:53:56.243412 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m13:53:56.243412 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m13:53:56.243412 [debug] [MainThread]: On master: ROLLBACK
[0m13:53:56.243412 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m13:53:56.244426 [debug] [MainThread]: On master: Close
[0m13:53:56.244426 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m13:53:56.244426 [info ] [MainThread]: 
[0m13:53:56.245411 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 2 minutes and 0.79 seconds (120.79s).
[0m13:53:56.245411 [debug] [MainThread]: Glue adapter: cleanup called
[0m13:53:56.492413 [info ] [MainThread]: 
[0m13:53:56.493413 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:53:56.494413 [info ] [MainThread]: 
[0m13:53:56.495413 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m13:53:56.495413 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A92E338E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A92EFC2260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A92EFC0FA0>]}
[0m13:53:56.496413 [debug] [MainThread]: Flushing usage events
[0m13:53:56.667439 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 14:08:46.419653 | bf740333-bb6f-4770-87ed-d41143092078 ==============================
[0m14:08:46.419653 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:08:46.419653 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:08:46.420653 [debug] [MainThread]: Tracking: tracking
[0m14:08:46.442654 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000255536AD6C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000255536ADB70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000255536AD7B0>]}
[0m14:08:46.525653 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:08:46.525653 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m14:08:46.537652 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m14:08:46.574659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'bf740333-bb6f-4770-87ed-d41143092078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025553884790>]}
[0m14:08:46.581663 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'bf740333-bb6f-4770-87ed-d41143092078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002555386FF10>]}
[0m14:08:46.581663 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m14:08:46.582664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bf740333-bb6f-4770-87ed-d41143092078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025553856D40>]}
[0m14:08:46.583655 [info ] [MainThread]: 
[0m14:08:46.584669 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:08:46.586664 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m14:08:46.586664 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:08:46.586664 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:08:46.587654 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m14:08:46.587654 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m14:09:25.742523 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:09:25.742523 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-780912d7-ef99-46cc-acd2-114a7f2c461f
[0m14:09:33.784789 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:09:34.283172 [debug] [ThreadPool]: On list_hudidb: Close
[0m14:09:34.283697 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:09:34.285280 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m14:09:34.285280 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:09:34.285280 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:09:34.768907 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m14:09:34.768907 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:09:34.768907 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-780912d7-ef99-46cc-acd2-114a7f2c461f
[0m14:09:35.611480 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:09:36.161497 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m14:09:36.162498 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:09:36.163498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'bf740333-bb6f-4770-87ed-d41143092078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025553894550>]}
[0m14:09:36.163498 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:09:36.163498 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:09:36.164497 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:09:36.164497 [info ] [MainThread]: 
[0m14:09:36.170498 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m14:09:36.170498 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m14:09:36.171498 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m14:09:36.171498 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m14:09:36.172498 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m14:09:36.175498 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m14:09:36.176498 [debug] [Thread-1 (]: finished collecting timing info
[0m14:09:36.176498 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m14:09:36.200498 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:09:36.200498 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:09:36.679498 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:09:36.680498 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:09:36.680498 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-780912d7-ef99-46cc-acd2-114a7f2c461f
[0m14:09:37.469087 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:09:38.008085 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m14:09:38.016086 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:09:38.016086 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:09:38.016086 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m14:09:38.016086 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:09:38.017086 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:09:38.017086 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:09:38.017086 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:09:38.017086 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m14:10:01.125135 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671804578694, 'CompletedOn': 1671804600778}, 'ResponseMetadata': {'RequestId': '989d6da0-5d47-4973-9e48-aa7b272f29a1', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:10:01 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': '989d6da0-5d47-4973-9e48-aa7b272f29a1'}, 'RetryAttempts': 0}}
[0m14:10:01.125135 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:10:01.125135 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:10:01.126135 [debug] [Thread-1 (]: SQL status: OK in 23.11 seconds
[0m14:10:01.129147 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:10:01.132144 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:10:01.781145 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m14:10:01.781145 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select

    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m14:10:01.782145 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:10:01.782145 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:10:01.782145 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:10:01.782145 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select

    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m14:10:28.640222 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223141008718", "_hoodie_commit_seqno": "20221223141008718_2_3", "_hoodie_record_key": "15385", "_hoodie_partition_path": "", "_hoodie_file_name": "73408d9a-54b1-4c93-960f-0d1a7fee537f-0_2-9-0_20221223141008718.parquet", "invoice_id": 15385, "category": "Household", "state": "NJ", "item_id": 39, "update_hudi_ts": "2022-12-23 14:10:10.846000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671804602440, 'CompletedOn': 1671804628052}, 'ResponseMetadata': {'RequestId': '29cca07e-fd49-419d-bd15-10b126640e32', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:10:29 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3868', 'connection': 'keep-alive', 'x-amzn-requestid': '29cca07e-fd49-419d-bd15-10b126640e32'}, 'RetryAttempts': 0}}
[0m14:10:28.641222 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:10:28.641222 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:10:28.642222 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m14:10:28.643222 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:10:28.643222 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m14:10:28.643222 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:10:28.643222 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:10:28.643222 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:10:28.644221 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:10:28.644221 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m14:10:30.234098 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223141008718", "_hoodie_commit_seqno": "20221223141008718_2_3", "_hoodie_record_key": "15385", "_hoodie_partition_path": "", "_hoodie_file_name": "73408d9a-54b1-4c93-960f-0d1a7fee537f-0_2-9-0_20221223141008718.parquet", "invoice_id": 15385, "category": "Household", "state": "NJ", "item_id": 39, "update_hudi_ts": "2022-12-23 14:10:10.846000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671804629293, 'CompletedOn': 1671804630356}, 'ResponseMetadata': {'RequestId': '5b2dc0cf-dad9-4783-a3f6-eb3a3317673f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:10:30 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1516', 'connection': 'keep-alive', 'x-amzn-requestid': '5b2dc0cf-dad9-4783-a3f6-eb3a3317673f'}, 'RetryAttempts': 0}}
[0m14:10:30.235098 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:10:30.235098 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:10:30.235098 [debug] [Thread-1 (]: SQL status: OK in 1.59 seconds
[0m14:10:30.236098 [debug] [Thread-1 (]: finished collecting timing info
[0m14:10:30.237098 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m14:10:30.237098 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:10:30.237098 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m14:10:30.237098 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:10:30.238097 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bf740333-bb6f-4770-87ed-d41143092078', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025553E63A30>]}
[0m14:10:30.238097 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 54.07s]
[0m14:10:30.239097 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m14:10:30.241097 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:10:30.241097 [debug] [MainThread]: On master: ROLLBACK
[0m14:10:30.241097 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:10:30.241097 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m14:10:30.729934 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m14:10:30.729934 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m14:10:30.729934 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-780912d7-ef99-46cc-acd2-114a7f2c461f
[0m14:10:31.525451 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:10:31.526451 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:10:31.526451 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:10:31.526451 [debug] [MainThread]: On master: ROLLBACK
[0m14:10:31.526451 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:10:31.526451 [debug] [MainThread]: On master: Close
[0m14:10:31.527451 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m14:10:31.527451 [info ] [MainThread]: 
[0m14:10:31.528451 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 44.94 seconds (104.94s).
[0m14:10:31.528451 [debug] [MainThread]: Glue adapter: cleanup called
[0m14:10:31.798453 [info ] [MainThread]: 
[0m14:10:31.799452 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:10:31.800452 [info ] [MainThread]: 
[0m14:10:31.800452 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:10:31.801452 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000255538951E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025554532830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025554531570>]}
[0m14:10:31.801452 [debug] [MainThread]: Flushing usage events
[0m14:10:31.992450 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 14:10:45.243839 | a0ac1db7-5c55-47cd-8c88-688974fb22b4 ==============================
[0m14:10:45.243839 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:10:45.244839 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:10:45.245843 [debug] [MainThread]: Tracking: tracking
[0m14:10:45.269839 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175D1324A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175CE24BDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175D1324490>]}
[0m14:10:45.371840 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:10:45.371840 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\cleansed_order.sql
[0m14:10:45.386350 [debug] [MainThread]: 1699: static parser successfully parsed metrics\cleansed_order.sql
[0m14:10:45.427352 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a0ac1db7-5c55-47cd-8c88-688974fb22b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175D14E47F0>]}
[0m14:10:45.434359 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a0ac1db7-5c55-47cd-8c88-688974fb22b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175D14CFF10>]}
[0m14:10:45.434359 [info ] [MainThread]: Found 1 model, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m14:10:45.435353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a0ac1db7-5c55-47cd-8c88-688974fb22b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175D14B6DA0>]}
[0m14:10:45.436352 [info ] [MainThread]: 
[0m14:10:45.438351 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:10:45.439358 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m14:10:45.439358 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:10:45.440351 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:10:45.440351 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m14:10:45.440351 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m14:11:30.348514 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:11:30.349514 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-cb82a65f-fd7c-4ba5-991a-13bab9986d91
[0m14:11:38.478420 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:11:39.020027 [debug] [ThreadPool]: On list_hudidb: Close
[0m14:11:39.021035 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:11:39.022027 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m14:11:39.023027 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:11:39.023027 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:11:39.809559 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m14:11:39.809559 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:11:39.809559 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-cb82a65f-fd7c-4ba5-991a-13bab9986d91
[0m14:11:40.577064 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:11:41.117244 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m14:11:41.117244 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:11:41.118245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a0ac1db7-5c55-47cd-8c88-688974fb22b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175D14F8F40>]}
[0m14:11:41.119244 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:11:41.119244 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:11:41.120244 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:11:41.120244 [info ] [MainThread]: 
[0m14:11:41.127245 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m14:11:41.127245 [info ] [Thread-1 (]: 1 of 1 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m14:11:41.128246 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m14:11:41.129246 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m14:11:41.129246 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m14:11:41.135246 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m14:11:41.137246 [debug] [Thread-1 (]: finished collecting timing info
[0m14:11:41.137246 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m14:11:41.166246 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:11:41.167248 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:11:41.659306 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:11:41.659306 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:11:41.659306 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-cb82a65f-fd7c-4ba5-991a-13bab9986d91
[0m14:11:42.459838 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:11:43.047890 [debug] [Thread-1 (]: Glue adapter: table_name : cleansed_order
[0m14:11:43.047890 [debug] [Thread-1 (]: Glue adapter: table type : table
[0m14:11:43.055889 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:11:43.055889 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:11:43.055889 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m14:11:43.056890 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:11:43.056890 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:11:43.056890 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:11:43.056890 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:11:43.056890 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m14:11:48.254613 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671804703689, 'CompletedOn': 1671804708468}, 'ResponseMetadata': {'RequestId': 'a8d49156-10f7-4eff-bdd8-5cec9b027faa', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:11:48 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': 'a8d49156-10f7-4eff-bdd8-5cec9b027faa'}, 'RetryAttempts': 0}}
[0m14:11:48.255613 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:11:48.255613 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:11:48.255613 [debug] [Thread-1 (]: SQL status: OK in 5.2 seconds
[0m14:11:48.259613 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:11:48.262613 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:11:48.839612 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : cleansed_order
                             type : table
                        
[0m14:11:48.840612 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m14:11:48.840612 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:11:48.840612 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:11:48.841612 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:11:48.841612 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m14:12:22.697452 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223141154584", "_hoodie_commit_seqno": "20221223141154584_0_1", "_hoodie_record_key": "10940", "_hoodie_partition_path": "", "_hoodie_file_name": "ce68f924-8f81-4327-8948-934993f2e7aa-0_0-34-151_20221223141154584.parquet", "invoice_id": 10940, "category": "Garden", "state": "MI", "item_id": 39, "update_hudi_ts": "2022-12-23 14:11:54.905000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671804709475, 'CompletedOn': 1671804742547}, 'ResponseMetadata': {'RequestId': '8b241075-ce0e-4c63-9ed8-f4bf81276db1', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:12:23 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4016', 'connection': 'keep-alive', 'x-amzn-requestid': '8b241075-ce0e-4c63-9ed8-f4bf81276db1'}, 'RetryAttempts': 0}}
[0m14:12:22.697452 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:12:22.697452 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:12:22.698452 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m14:12:22.699452 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:12:22.699452 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m14:12:22.699452 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:12:22.699452 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:12:22.699452 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:12:22.700452 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:12:22.700452 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m14:12:24.259491 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223141154584", "_hoodie_commit_seqno": "20221223141154584_0_1", "_hoodie_record_key": "10940", "_hoodie_partition_path": "", "_hoodie_file_name": "ce68f924-8f81-4327-8948-934993f2e7aa-0_0-34-151_20221223141154584.parquet", "invoice_id": 10940, "category": "Garden", "state": "MI", "item_id": 39, "update_hudi_ts": "2022-12-23 14:11:54.905000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671804743332, 'CompletedOn': 1671804744261}, 'ResponseMetadata': {'RequestId': '0c1d3e2a-6ece-4c3f-b80d-322613afb74a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:12:24 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1516', 'connection': 'keep-alive', 'x-amzn-requestid': '0c1d3e2a-6ece-4c3f-b80d-322613afb74a'}, 'RetryAttempts': 0}}
[0m14:12:24.259491 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:12:24.260491 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:12:24.260491 [debug] [Thread-1 (]: SQL status: OK in 1.56 seconds
[0m14:12:24.261491 [debug] [Thread-1 (]: finished collecting timing info
[0m14:12:24.261491 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m14:12:24.261491 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:12:24.261491 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m14:12:24.262491 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:12:24.262491 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a0ac1db7-5c55-47cd-8c88-688974fb22b4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175D1AC3790>]}
[0m14:12:24.263605 [info ] [Thread-1 (]: 1 of 1 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 43.13s]
[0m14:12:24.264145 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m14:12:24.265656 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:12:24.265656 [debug] [MainThread]: On master: ROLLBACK
[0m14:12:24.265656 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:12:24.266658 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m14:12:24.762924 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m14:12:24.762924 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m14:12:24.762924 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-cb82a65f-fd7c-4ba5-991a-13bab9986d91
[0m14:12:25.516031 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:12:25.517031 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:12:25.517031 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:12:25.518031 [debug] [MainThread]: On master: ROLLBACK
[0m14:12:25.518031 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:12:25.518031 [debug] [MainThread]: On master: Close
[0m14:12:25.518031 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m14:12:25.519031 [info ] [MainThread]: 
[0m14:12:25.520031 [info ] [MainThread]: Finished running 1 incremental model in 0 hours 1 minutes and 40.08 seconds (100.08s).
[0m14:12:25.520031 [debug] [MainThread]: Glue adapter: cleanup called
[0m14:12:25.781660 [info ] [MainThread]: 
[0m14:12:25.782661 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:12:25.782661 [info ] [MainThread]: 
[0m14:12:25.783660 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
[0m14:12:25.784660 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175D14F88E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175D20A4100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000175D20A5360>]}
[0m14:12:25.784660 [debug] [MainThread]: Flushing usage events
[0m14:12:26.225302 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 14:19:33.753576 | 2a0e83d4-2f12-426c-a8e7-5e073dcf5bdd ==============================
[0m14:19:33.753576 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:19:33.754576 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:19:33.754576 [debug] [MainThread]: Tracking: tracking
[0m14:19:33.780575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B90ECD540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B90ECD8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B90ECD690>]}
[0m14:19:33.868574 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 1 files added, 1 files changed.
[0m14:19:33.869581 [debug] [MainThread]: Partial parsing: added file: dbtglue://models\metrics\view_state.sql
[0m14:19:33.869581 [debug] [MainThread]: Partial parsing: update schema file: dbtglue://models\metrics\schema.yml
[0m14:19:33.884580 [debug] [MainThread]: 1699: static parser successfully parsed metrics\view_state.sql
[0m14:19:33.928580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2a0e83d4-2f12-426c-a8e7-5e073dcf5bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B911239A0>]}
[0m14:19:33.936575 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2a0e83d4-2f12-426c-a8e7-5e073dcf5bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B9108FFA0>]}
[0m14:19:33.937574 [info ] [MainThread]: Found 2 models, 2 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m14:19:33.937574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a0e83d4-2f12-426c-a8e7-5e073dcf5bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B8DF6C190>]}
[0m14:19:33.938575 [info ] [MainThread]: 
[0m14:19:33.939581 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:19:33.941577 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m14:19:33.941577 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:19:33.941577 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:19:33.942576 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m14:19:33.942576 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m14:20:30.680394 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:20:30.681392 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9f78fbe0-a143-4484-94f6-7b6189292dbf
[0m14:20:38.758495 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:20:39.313794 [debug] [ThreadPool]: On list_hudidb: Close
[0m14:20:39.314795 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:20:39.316794 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m14:20:39.316794 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:20:39.317794 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:20:39.805672 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m14:20:39.806671 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:20:39.806671 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9f78fbe0-a143-4484-94f6-7b6189292dbf
[0m14:20:41.793823 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:20:42.335843 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m14:20:42.336856 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:20:42.337853 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2a0e83d4-2f12-426c-a8e7-5e073dcf5bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B910C24D0>]}
[0m14:20:42.337853 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:20:42.338842 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:20:42.338842 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:20:42.339854 [info ] [MainThread]: 
[0m14:20:42.344843 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m14:20:42.345843 [info ] [Thread-1 (]: 1 of 2 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m14:20:42.346850 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m14:20:42.346850 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m14:20:42.346850 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m14:20:42.352849 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m14:20:42.354843 [debug] [Thread-1 (]: finished collecting timing info
[0m14:20:42.354843 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m14:20:42.384845 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:20:42.385845 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:20:42.861845 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:20:42.861845 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:20:42.862858 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9f78fbe0-a143-4484-94f6-7b6189292dbf
[0m14:20:43.643846 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:20:44.210844 [debug] [Thread-1 (]: Glue adapter: table_name : cleansed_order
[0m14:20:44.211844 [debug] [Thread-1 (]: Glue adapter: table type : table
[0m14:20:44.220848 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:20:44.220848 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:20:44.220848 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m14:20:44.221843 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:20:44.221843 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:20:44.221843 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:20:44.222843 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:20:44.222843 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m14:21:01.483911 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805244848, 'CompletedOn': 1671805261027}, 'ResponseMetadata': {'RequestId': '9721af41-b9d8-4d18-b6bf-98a0073b68f3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:21:01 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': '9721af41-b9d8-4d18-b6bf-98a0073b68f3'}, 'RetryAttempts': 0}}
[0m14:21:01.484911 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:21:01.484911 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:21:01.485917 [debug] [Thread-1 (]: SQL status: OK in 17.26 seconds
[0m14:21:01.489994 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:21:01.492988 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:21:02.075058 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : cleansed_order
                             type : table
                        
[0m14:21:02.076058 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m14:21:02.076058 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:21:02.076058 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:21:02.077060 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:21:02.077060 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m14:21:30.457307 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223142109648", "_hoodie_commit_seqno": "20221223142109648_0_1", "_hoodie_record_key": "10940", "_hoodie_partition_path": "", "_hoodie_file_name": "ce68f924-8f81-4327-8948-934993f2e7aa-0_0-34-153_20221223142109648.parquet", "invoice_id": 10940, "category": "Garden", "state": "MI", "item_id": 39, "update_hudi_ts": "2022-12-23 14:21:09.964000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805262692, 'CompletedOn': 1671805290837}, 'ResponseMetadata': {'RequestId': '60992a4f-eefa-4739-ae17-0321423a655c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:21:30 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4016', 'connection': 'keep-alive', 'x-amzn-requestid': '60992a4f-eefa-4739-ae17-0321423a655c'}, 'RetryAttempts': 0}}
[0m14:21:30.457307 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:21:30.458306 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:21:30.458306 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m14:21:30.459306 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:21:30.460306 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m14:21:30.460306 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:21:30.460306 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:21:30.460306 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:21:30.460306 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:21:30.461306 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m14:21:32.097015 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223142109648", "_hoodie_commit_seqno": "20221223142109648_1_2", "_hoodie_record_key": "15385", "_hoodie_partition_path": "", "_hoodie_file_name": "73408d9a-54b1-4c93-960f-0d1a7fee537f-0_1-40-154_20221223142109648.parquet", "invoice_id": 15385, "category": "Household", "state": "NJ", "item_id": 39, "update_hudi_ts": "2022-12-23 14:21:09.964000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805291120, 'CompletedOn': 1671805292102}, 'ResponseMetadata': {'RequestId': '51603262-c890-46cb-9b0b-dc6026049546', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:21:32 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1519', 'connection': 'keep-alive', 'x-amzn-requestid': '51603262-c890-46cb-9b0b-dc6026049546'}, 'RetryAttempts': 0}}
[0m14:21:32.097015 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:21:32.097015 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:21:32.098015 [debug] [Thread-1 (]: SQL status: OK in 1.64 seconds
[0m14:21:32.099015 [debug] [Thread-1 (]: finished collecting timing info
[0m14:21:32.099015 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m14:21:32.099015 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:21:32.099015 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m14:21:32.099015 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:21:32.100015 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a0e83d4-2f12-426c-a8e7-5e073dcf5bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B91B1BC40>]}
[0m14:21:32.100015 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 49.75s]
[0m14:21:32.101015 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m14:21:32.102015 [debug] [Thread-1 (]: Began running node model.dbtglue.view_state
[0m14:21:32.102015 [info ] [Thread-1 (]: 2 of 2 START sql view model hudidb.view_state .................................. [RUN]
[0m14:21:32.103015 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.view_state"
[0m14:21:32.103015 [debug] [Thread-1 (]: Began compiling node model.dbtglue.view_state
[0m14:21:32.104015 [debug] [Thread-1 (]: Compiling model.dbtglue.view_state
[0m14:21:32.108017 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.view_state"
[0m14:21:32.109015 [debug] [Thread-1 (]: finished collecting timing info
[0m14:21:32.109015 [debug] [Thread-1 (]: Began executing node model.dbtglue.view_state
[0m14:21:32.118016 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:21:32.119017 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:21:32.594293 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:21:32.595293 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:21:32.595293 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9f78fbe0-a143-4484-94f6-7b6189292dbf
[0m14:21:33.366824 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:21:33.907851 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table view_state not found.
[0m14:21:33.915851 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.view_state"
[0m14:21:33.916852 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:21:33.917851 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m14:21:33.917851 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
DROP VIEW IF EXISTS hudidb.view_state
    dbt_next_query
    create view hudidb.view_state
        as
    


SELECT category,
       count(*)
FROM hudidb.cleansed_order
GROUP BY category
ORDER BY category asc

[0m14:21:33.917851 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:21:33.917851 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:21:33.918852 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:21:33.918852 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:21:33.918852 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
DROP VIEW IF EXISTS hudidb.view_state
    dbt_next_query
    create view hudidb.view_state
        as
    


SELECT category,
       count(*)
FROM hudidb.cleansed_order
GROUP BY category
ORDER BY category asc
''')
[0m14:21:36.707235 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\nDROP VIEW IF EXISTS hudidb.view_state\n    dbt_next_query\n    create view hudidb.view_state\n        as\n    \n\n\nSELECT category,\n       count(*)\nFROM hudidb.cleansed_order\nGROUP BY category\nORDER BY category asc\n\'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "\\n    create view hudidb.view_state\\n        as\\n    \\n\\n\\nSELECT category,\\n       count(*)\\nFROM hudidb.cleansed_order\\nGROUP BY category\\nORDER BY category asc\\n", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 11, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805294570, 'CompletedOn': 1671805296697}, 'ResponseMetadata': {'RequestId': '46dd9749-88c8-49d7-bcd7-4afc6e41d8af', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:21:37 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '923', 'connection': 'keep-alive', 'x-amzn-requestid': '46dd9749-88c8-49d7-bcd7-4afc6e41d8af'}, 'RetryAttempts': 0}}
[0m14:21:36.707235 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:21:36.707235 [debug] [Thread-1 (]: Glue adapter: chunks: ['{"type": "results", "sql": "\\n    create view hudidb.view_state\\n        as\\n    \\n\\n\\nSELECT category,\\n       count(*)\\nFROM hudidb.cleansed_order\\nGROUP BY category\\nORDER BY category asc\\n", "schema": null, "results": null}', '{"type": "results", "rowcount": 0, "results": [], "description": []}']
[0m14:21:36.708235 [debug] [Thread-1 (]: Glue adapter: response: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\nDROP VIEW IF EXISTS hudidb.view_state\n    dbt_next_query\n    create view hudidb.view_state\n        as\n    \n\n\nSELECT category,\n       count(*)\nFROM hudidb.cleansed_order\nGROUP BY category\nORDER BY category asc\n\'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "\\n    create view hudidb.view_state\\n        as\\n    \\n\\n\\nSELECT category,\\n       count(*)\\nFROM hudidb.cleansed_order\\nGROUP BY category\\nORDER BY category asc\\n", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 11, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805294570, 'CompletedOn': 1671805296697}, 'ResponseMetadata': {'RequestId': '46dd9749-88c8-49d7-bcd7-4afc6e41d8af', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:21:37 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '923', 'connection': 'keep-alive', 'x-amzn-requestid': '46dd9749-88c8-49d7-bcd7-4afc6e41d8af'}, 'RetryAttempts': 0}}
[0m14:21:36.708235 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:21:36.708235 [debug] [Thread-1 (]: SQL status: OK in 2.79 seconds
[0m14:21:36.717236 [debug] [Thread-1 (]: finished collecting timing info
[0m14:21:36.717236 [debug] [Thread-1 (]: On model.dbtglue.view_state: ROLLBACK
[0m14:21:36.717236 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:21:36.718236 [debug] [Thread-1 (]: On model.dbtglue.view_state: Close
[0m14:21:36.718236 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:21:36.718236 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2a0e83d4-2f12-426c-a8e7-5e073dcf5bdd', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B91CC01F0>]}
[0m14:21:36.719236 [info ] [Thread-1 (]: 2 of 2 OK created sql view model hudidb.view_state ............................. [[32mOK[0m in 4.62s]
[0m14:21:36.720236 [debug] [Thread-1 (]: Finished running node model.dbtglue.view_state
[0m14:21:36.721236 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:21:36.722236 [debug] [MainThread]: On master: ROLLBACK
[0m14:21:36.722236 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:21:36.722236 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m14:21:37.225276 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m14:21:37.225276 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m14:21:37.226281 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-9f78fbe0-a143-4484-94f6-7b6189292dbf
[0m14:21:38.010163 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:21:38.010163 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:21:38.011161 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:21:38.011161 [debug] [MainThread]: On master: ROLLBACK
[0m14:21:38.012161 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:21:38.012161 [debug] [MainThread]: On master: Close
[0m14:21:38.013165 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m14:21:38.014165 [info ] [MainThread]: 
[0m14:21:38.014165 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 2 minutes and 4.07 seconds (124.07s).
[0m14:21:38.015161 [debug] [MainThread]: Glue adapter: cleanup called
[0m14:21:38.292562 [info ] [MainThread]: 
[0m14:21:38.293554 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:21:38.294557 [info ] [MainThread]: 
[0m14:21:38.294557 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:21:38.295557 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B910C0DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B91D5BF40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000025B91D5B100>]}
[0m14:21:38.295557 [debug] [MainThread]: Flushing usage events
[0m14:21:38.486712 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 14:25:06.760632 | c4e77111-e9a9-4383-8c06-f4c6f79c390d ==============================
[0m14:25:06.760632 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:25:06.761632 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:25:06.761632 [debug] [MainThread]: Tracking: tracking
[0m14:25:06.787632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023740D9D6C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023740D9DB70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023740D9D7B0>]}
[0m14:25:06.882631 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:25:06.882631 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\view_state.sql
[0m14:25:06.896631 [debug] [MainThread]: 1699: static parser successfully parsed metrics\view_state.sql
[0m14:25:06.938634 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c4e77111-e9a9-4383-8c06-f4c6f79c390d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023740FFCE50>]}
[0m14:25:06.947632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c4e77111-e9a9-4383-8c06-f4c6f79c390d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023740F61300>]}
[0m14:25:06.947632 [info ] [MainThread]: Found 2 models, 2 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m14:25:06.948639 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c4e77111-e9a9-4383-8c06-f4c6f79c390d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002373DC387F0>]}
[0m14:25:06.950633 [info ] [MainThread]: 
[0m14:25:06.951635 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:25:06.953651 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m14:25:06.953651 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:06.954641 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:25:06.954641 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m14:25:06.954641 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m14:25:46.546935 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:25:46.546935 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-61320834-a96a-4844-b04d-7c11bef37cff
[0m14:25:54.707986 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:25:55.225475 [debug] [ThreadPool]: On list_hudidb: Close
[0m14:25:55.225475 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:25:55.227470 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m14:25:55.227470 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:25:55.228472 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:25:55.703317 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m14:25:55.704317 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:25:55.704317 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-61320834-a96a-4844-b04d-7c11bef37cff
[0m14:25:56.549907 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:25:57.089906 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m14:25:57.090906 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:25:57.091906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c4e77111-e9a9-4383-8c06-f4c6f79c390d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023740F992D0>]}
[0m14:25:57.091906 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:25:57.091906 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:25:57.092906 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:25:57.093906 [info ] [MainThread]: 
[0m14:25:57.098908 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m14:25:57.098908 [info ] [Thread-1 (]: 1 of 2 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m14:25:57.099908 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m14:25:57.099908 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m14:25:57.100908 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m14:25:57.105908 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m14:25:57.105908 [debug] [Thread-1 (]: finished collecting timing info
[0m14:25:57.106908 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m14:25:57.133925 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:25:57.133925 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:25:57.625039 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:25:57.625039 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:25:57.625039 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-61320834-a96a-4844-b04d-7c11bef37cff
[0m14:25:58.427427 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:25:59.003510 [debug] [Thread-1 (]: Glue adapter: table_name : cleansed_order
[0m14:25:59.004509 [debug] [Thread-1 (]: Glue adapter: table type : table
[0m14:25:59.011509 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:25:59.012510 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:25:59.012510 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m14:25:59.012510 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:25:59.012510 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:25:59.013510 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:25:59.013510 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:25:59.013510 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m14:26:18.604521 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805559658, 'CompletedOn': 1671805577967}, 'ResponseMetadata': {'RequestId': '2be280d2-b989-466e-9226-b242abed6508', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:26:19 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': '2be280d2-b989-466e-9226-b242abed6508'}, 'RetryAttempts': 0}}
[0m14:26:18.604521 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:26:18.604521 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:26:18.605521 [debug] [Thread-1 (]: SQL status: OK in 19.59 seconds
[0m14:26:18.610511 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:26:18.613511 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:26:19.183254 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : cleansed_order
                             type : table
                        
[0m14:26:19.184254 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m14:26:19.184254 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:26:19.184254 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:26:19.184254 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:26:19.185254 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m14:26:47.249448 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223142627025", "_hoodie_commit_seqno": "20221223142627025_1_5", "_hoodie_record_key": "15385", "_hoodie_partition_path": "", "_hoodie_file_name": "73408d9a-54b1-4c93-960f-0d1a7fee537f-0_1-40-154_20221223142627025.parquet", "invoice_id": 15385, "category": "Household", "state": "NJ", "item_id": 39, "update_hudi_ts": "2022-12-23 14:26:27.362000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805579849, 'CompletedOn': 1671805607347}, 'ResponseMetadata': {'RequestId': '495cb78f-c101-4b5e-a134-6d91fdf1e3f1', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:26:47 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4019', 'connection': 'keep-alive', 'x-amzn-requestid': '495cb78f-c101-4b5e-a134-6d91fdf1e3f1'}, 'RetryAttempts': 0}}
[0m14:26:47.249448 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:26:47.249448 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:26:47.250435 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m14:26:47.251454 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:26:47.251454 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m14:26:47.251454 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:26:47.251454 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:26:47.252434 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:26:47.252434 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:26:47.252434 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m14:26:48.851382 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223142627025", "_hoodie_commit_seqno": "20221223142627025_1_5", "_hoodie_record_key": "15385", "_hoodie_partition_path": "", "_hoodie_file_name": "73408d9a-54b1-4c93-960f-0d1a7fee537f-0_1-40-154_20221223142627025.parquet", "invoice_id": 15385, "category": "Household", "state": "NJ", "item_id": 39, "update_hudi_ts": "2022-12-23 14:26:27.362000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805607888, 'CompletedOn': 1671805608937}, 'ResponseMetadata': {'RequestId': 'cc77222d-7516-4909-a172-9f03764dd577', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:26:49 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1519', 'connection': 'keep-alive', 'x-amzn-requestid': 'cc77222d-7516-4909-a172-9f03764dd577'}, 'RetryAttempts': 0}}
[0m14:26:48.852394 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:26:48.852394 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:26:48.852394 [debug] [Thread-1 (]: SQL status: OK in 1.6 seconds
[0m14:26:48.853381 [debug] [Thread-1 (]: finished collecting timing info
[0m14:26:48.854380 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m14:26:48.854380 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:26:48.854380 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m14:26:48.854380 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:26:48.855380 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4e77111-e9a9-4383-8c06-f4c6f79c390d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023741A103D0>]}
[0m14:26:48.856380 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 51.76s]
[0m14:26:48.857380 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m14:26:48.857380 [debug] [Thread-1 (]: Began running node model.dbtglue.view_state
[0m14:26:48.858380 [info ] [Thread-1 (]: 2 of 2 START sql view model hudidb.view_state .................................. [RUN]
[0m14:26:48.859380 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.view_state"
[0m14:26:48.859380 [debug] [Thread-1 (]: Began compiling node model.dbtglue.view_state
[0m14:26:48.859380 [debug] [Thread-1 (]: Compiling model.dbtglue.view_state
[0m14:26:48.864380 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.view_state"
[0m14:26:48.864380 [debug] [Thread-1 (]: finished collecting timing info
[0m14:26:48.865380 [debug] [Thread-1 (]: Began executing node model.dbtglue.view_state
[0m14:26:48.873380 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:26:48.874380 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:26:49.352055 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:26:49.352055 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:26:49.353049 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-61320834-a96a-4844-b04d-7c11bef37cff
[0m14:26:50.133310 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:26:50.673338 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : view_state
                             type : view
                        
[0m14:26:50.683340 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.view_state"
[0m14:26:50.684340 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:26:50.685340 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m14:26:50.685340 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
DROP VIEW IF EXISTS hudidb.view_state
    dbt_next_query
    create view hudidb.view_state
        as
    


with cleansed_order as (

    SELECT category,
           count(*)
    FROM hudidb.cleansed_order
    GROUP BY category
    ORDER BY category asc
)

select
    *
from cleansed_order

[0m14:26:50.685340 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:26:50.686341 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:26:50.686341 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:26:50.686341 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:26:50.687340 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
DROP VIEW IF EXISTS hudidb.view_state
    dbt_next_query
    create view hudidb.view_state
        as
    


with cleansed_order as (

    SELECT category,
           count(*)
    FROM hudidb.cleansed_order
    GROUP BY category
    ORDER BY category asc
)

select
    *
from cleansed_order
''')
[0m14:26:55.911697 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\nDROP VIEW IF EXISTS hudidb.view_state\n    dbt_next_query\n    create view hudidb.view_state\n        as\n    \n\n\nwith cleansed_order as (\n\n    SELECT category,\n           count(*)\n    FROM hudidb.cleansed_order\n    GROUP BY category\n    ORDER BY category asc\n)\n\nselect\n    *\nfrom cleansed_order\n\'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "\\n    create view hudidb.view_state\\n        as\\n    \\n\\n\\nwith cleansed_order as (\\n\\n    SELECT category,\\n           count(*)\\n    FROM hudidb.cleansed_order\\n    GROUP BY category\\n    ORDER BY category asc\\n)\\n\\nselect\\n    *\\nfrom cleansed_order\\n", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 11, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805611367, 'CompletedOn': 1671805615534}, 'ResponseMetadata': {'RequestId': '63a8255e-b028-4d67-ad96-4c99137dbff8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:26:56 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1108', 'connection': 'keep-alive', 'x-amzn-requestid': '63a8255e-b028-4d67-ad96-4c99137dbff8'}, 'RetryAttempts': 0}}
[0m14:26:55.912697 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:26:55.912697 [debug] [Thread-1 (]: Glue adapter: chunks: ['{"type": "results", "sql": "\\n    create view hudidb.view_state\\n        as\\n    \\n\\n\\nwith cleansed_order as (\\n\\n    SELECT category,\\n           count(*)\\n    FROM hudidb.cleansed_order\\n    GROUP BY category\\n    ORDER BY category asc\\n)\\n\\nselect\\n    *\\nfrom cleansed_order\\n", "schema": null, "results": null}', '{"type": "results", "rowcount": 0, "results": [], "description": []}']
[0m14:26:55.912697 [debug] [Thread-1 (]: Glue adapter: response: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\nDROP VIEW IF EXISTS hudidb.view_state\n    dbt_next_query\n    create view hudidb.view_state\n        as\n    \n\n\nwith cleansed_order as (\n\n    SELECT category,\n           count(*)\n    FROM hudidb.cleansed_order\n    GROUP BY category\n    ORDER BY category asc\n)\n\nselect\n    *\nfrom cleansed_order\n\'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "\\n    create view hudidb.view_state\\n        as\\n    \\n\\n\\nwith cleansed_order as (\\n\\n    SELECT category,\\n           count(*)\\n    FROM hudidb.cleansed_order\\n    GROUP BY category\\n    ORDER BY category asc\\n)\\n\\nselect\\n    *\\nfrom cleansed_order\\n", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 11, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805611367, 'CompletedOn': 1671805615534}, 'ResponseMetadata': {'RequestId': '63a8255e-b028-4d67-ad96-4c99137dbff8', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:26:56 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1108', 'connection': 'keep-alive', 'x-amzn-requestid': '63a8255e-b028-4d67-ad96-4c99137dbff8'}, 'RetryAttempts': 0}}
[0m14:26:55.912697 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:26:55.913697 [debug] [Thread-1 (]: SQL status: OK in 5.23 seconds
[0m14:26:55.924697 [debug] [Thread-1 (]: finished collecting timing info
[0m14:26:55.925697 [debug] [Thread-1 (]: On model.dbtglue.view_state: ROLLBACK
[0m14:26:55.925697 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:26:55.925697 [debug] [Thread-1 (]: On model.dbtglue.view_state: Close
[0m14:26:55.926698 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:26:55.926986 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4e77111-e9a9-4383-8c06-f4c6f79c390d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000237419BED70>]}
[0m14:26:55.927699 [info ] [Thread-1 (]: 2 of 2 OK created sql view model hudidb.view_state ............................. [[32mOK[0m in 7.07s]
[0m14:26:55.928697 [debug] [Thread-1 (]: Finished running node model.dbtglue.view_state
[0m14:26:55.930164 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:26:55.930164 [debug] [MainThread]: On master: ROLLBACK
[0m14:26:55.930164 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:26:55.930164 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m14:26:56.406617 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m14:26:56.406617 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m14:26:56.407619 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-61320834-a96a-4844-b04d-7c11bef37cff
[0m14:26:57.212267 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:26:57.212267 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:26:57.212267 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:26:57.213268 [debug] [MainThread]: On master: ROLLBACK
[0m14:26:57.213268 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:26:57.213268 [debug] [MainThread]: On master: Close
[0m14:26:57.213268 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m14:26:57.214269 [info ] [MainThread]: 
[0m14:26:57.214269 [info ] [MainThread]: Finished running 1 incremental model, 1 view model in 0 hours 1 minutes and 50.26 seconds (110.26s).
[0m14:26:57.215267 [debug] [MainThread]: Glue adapter: cleanup called
[0m14:26:57.545357 [info ] [MainThread]: 
[0m14:26:57.546357 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:26:57.546357 [info ] [MainThread]: 
[0m14:26:57.547357 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:26:57.547357 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023741B41930>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023741B42C20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023741B40D60>]}
[0m14:26:57.548357 [debug] [MainThread]: Flushing usage events
[0m14:26:57.706355 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 14:30:57.428640 | dde7b8bd-3c62-4f6b-8cbd-850c2113c8a1 ==============================
[0m14:30:57.428640 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:30:57.429646 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:30:57.429646 [debug] [MainThread]: Tracking: tracking
[0m14:30:57.455645 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795B21D540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795B21D8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795B21D690>]}
[0m14:30:57.563632 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:30:57.564632 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\view_state.sql
[0m14:30:57.580630 [debug] [MainThread]: 1699: static parser successfully parsed metrics\view_state.sql
[0m14:30:57.627632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'dde7b8bd-3c62-4f6b-8cbd-850c2113c8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795B47FE80>]}
[0m14:30:57.638632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'dde7b8bd-3c62-4f6b-8cbd-850c2113c8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795B3DD2A0>]}
[0m14:30:57.638632 [info ] [MainThread]: Found 2 models, 2 tests, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m14:30:57.639632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dde7b8bd-3c62-4f6b-8cbd-850c2113c8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795835C190>]}
[0m14:30:57.641632 [info ] [MainThread]: 
[0m14:30:57.643633 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:30:57.644632 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m14:30:57.645631 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:30:57.645631 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:30:57.645631 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m14:30:57.646636 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m14:31:42.257702 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:31:42.258703 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-ead9ef55-23cf-4845-82dd-9b3196bea608
[0m14:31:53.146615 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:31:53.711614 [debug] [ThreadPool]: On list_hudidb: Close
[0m14:31:53.711614 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:31:53.713614 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m14:31:53.713614 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:31:53.713614 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:31:54.182620 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m14:31:54.182620 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:31:54.182620 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-ead9ef55-23cf-4845-82dd-9b3196bea608
[0m14:31:54.996671 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:31:55.552758 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m14:31:55.553758 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:31:55.554757 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'dde7b8bd-3c62-4f6b-8cbd-850c2113c8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795B419E40>]}
[0m14:31:55.554757 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:31:55.554757 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:31:55.555757 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:31:55.555757 [info ] [MainThread]: 
[0m14:31:55.560759 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m14:31:55.561758 [info ] [Thread-1 (]: 1 of 2 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m14:31:55.562759 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m14:31:55.562759 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m14:31:55.562759 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m14:31:55.566759 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m14:31:55.567760 [debug] [Thread-1 (]: finished collecting timing info
[0m14:31:55.567760 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m14:31:55.592761 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:31:55.592761 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:31:56.057715 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:31:56.057715 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:31:56.058715 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-ead9ef55-23cf-4845-82dd-9b3196bea608
[0m14:31:56.991715 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:31:57.563499 [debug] [Thread-1 (]: Glue adapter: table_name : cleansed_order
[0m14:31:57.563499 [debug] [Thread-1 (]: Glue adapter: table type : table
[0m14:31:57.573499 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:31:57.573499 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:31:57.574499 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m14:31:57.574499 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:31:57.574499 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:31:57.574499 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:31:57.575499 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:31:57.575499 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m14:32:21.975172 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805918199, 'CompletedOn': 1671805941507}, 'ResponseMetadata': {'RequestId': 'e020928e-f646-4c9a-b1b8-6170d4a590d2', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:32:22 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': 'e020928e-f646-4c9a-b1b8-6170d4a590d2'}, 'RetryAttempts': 0}}
[0m14:32:21.975172 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:32:21.975172 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:32:21.976170 [debug] [Thread-1 (]: SQL status: OK in 24.4 seconds
[0m14:32:21.981179 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:32:21.984171 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:32:22.591060 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : cleansed_order
                             type : table
                        
[0m14:32:22.592060 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m14:32:22.592060 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:32:22.593060 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:32:22.593060 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:32:22.593060 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m14:32:49.476702 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223143229288", "_hoodie_commit_seqno": "20221223143229288_1_2", "_hoodie_record_key": "15385", "_hoodie_partition_path": "", "_hoodie_file_name": "73408d9a-54b1-4c93-960f-0d1a7fee537f-0_1-40-154_20221223143229288.parquet", "invoice_id": 15385, "category": "Household", "state": "NJ", "item_id": 39, "update_hudi_ts": "2022-12-23 14:32:29.548000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805943272, 'CompletedOn': 1671805968738}, 'ResponseMetadata': {'RequestId': 'cbff805a-eaf7-4c3d-9eb9-a5a4acb8ee00', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:32:49 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4019', 'connection': 'keep-alive', 'x-amzn-requestid': 'cbff805a-eaf7-4c3d-9eb9-a5a4acb8ee00'}, 'RetryAttempts': 0}}
[0m14:32:49.477701 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:32:49.478701 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:32:49.478701 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m14:32:49.480710 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:32:49.480710 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m14:32:49.480710 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:32:49.481702 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:32:49.481702 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:32:49.482702 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:32:49.482702 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m14:32:51.090574 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223143229288", "_hoodie_commit_seqno": "20221223143229288_0_1", "_hoodie_record_key": "10940", "_hoodie_partition_path": "", "_hoodie_file_name": "ce68f924-8f81-4327-8948-934993f2e7aa-0_0-34-153_20221223143229288.parquet", "invoice_id": 10940, "category": "Garden", "state": "MI", "item_id": 39, "update_hudi_ts": "2022-12-23 14:32:29.548000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805970131, 'CompletedOn': 1671805971154}, 'ResponseMetadata': {'RequestId': '4346ea44-3a53-4f86-a856-ec32bded5461', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:32:51 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1516', 'connection': 'keep-alive', 'x-amzn-requestid': '4346ea44-3a53-4f86-a856-ec32bded5461'}, 'RetryAttempts': 0}}
[0m14:32:51.091574 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:32:51.091574 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:32:51.091574 [debug] [Thread-1 (]: SQL status: OK in 1.61 seconds
[0m14:32:51.092578 [debug] [Thread-1 (]: finished collecting timing info
[0m14:32:51.093577 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m14:32:51.093577 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:32:51.093577 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m14:32:51.093577 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:32:51.094575 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dde7b8bd-3c62-4f6b-8cbd-850c2113c8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795BE63DC0>]}
[0m14:32:51.094575 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 55.53s]
[0m14:32:51.095581 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m14:32:51.096590 [debug] [Thread-1 (]: Began running node model.dbtglue.view_state
[0m14:32:51.096590 [info ] [Thread-1 (]: 2 of 2 START sql table model hudidb.view_state ................................. [RUN]
[0m14:32:51.097581 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.view_state"
[0m14:32:51.097581 [debug] [Thread-1 (]: Began compiling node model.dbtglue.view_state
[0m14:32:51.098584 [debug] [Thread-1 (]: Compiling model.dbtglue.view_state
[0m14:32:51.102582 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.view_state"
[0m14:32:51.103587 [debug] [Thread-1 (]: finished collecting timing info
[0m14:32:51.103587 [debug] [Thread-1 (]: Began executing node model.dbtglue.view_state
[0m14:32:51.111573 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:32:51.112573 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:32:51.610138 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:32:51.610677 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:32:51.610677 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-ead9ef55-23cf-4845-82dd-9b3196bea608
[0m14:32:52.416021 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:32:52.970025 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : view_state
                             type : view
                        
[0m14:32:52.985021 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:32:53.555082 [debug] [Thread-1 (]: Glue adapter: table_name : view_state
[0m14:32:53.555082 [debug] [Thread-1 (]: Glue adapter: table type : view
[0m14:32:53.556081 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m14:32:53.557082 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

        drop view if exists hudidb.view_state
[0m14:32:53.557082 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:32:53.557082 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:32:53.558082 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:32:53.558082 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:32:53.558082 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

        drop view if exists hudidb.view_state''')
[0m14:32:57.551569 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\n\n        drop view if exists hudidb.view_state\'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.view_state\\"} */\\n\\n        drop view if exists hudidb.view_state", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 11, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805974197, 'CompletedOn': 1671805976949}, 'ResponseMetadata': {'RequestId': '5a218ea9-392e-4f5f-88e4-cce06bc87de3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:32:57 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '817', 'connection': 'keep-alive', 'x-amzn-requestid': '5a218ea9-392e-4f5f-88e4-cce06bc87de3'}, 'RetryAttempts': 0}}
[0m14:32:57.551569 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:32:57.552566 [debug] [Thread-1 (]: Glue adapter: chunks: ['{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.view_state\\"} */\\n\\n        drop view if exists hudidb.view_state", "schema": null, "results": null}', '{"type": "results", "rowcount": 0, "results": [], "description": []}']
[0m14:32:57.552566 [debug] [Thread-1 (]: Glue adapter: response: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\n\n        drop view if exists hudidb.view_state\'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "sql": "/* {\\"app\\": \\"dbt\\", \\"dbt_version\\": \\"1.3.1\\", \\"profile_name\\": \\"dbtglue\\", \\"target_name\\": \\"dev\\", \\"node_id\\": \\"model.dbtglue.view_state\\"} */\\n\\n        drop view if exists hudidb.view_state", "schema": null, "results": null}\n{"type": "results", "rowcount": 0, "results": [], "description": []}'}, 'ExecutionCount': 11, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671805974197, 'CompletedOn': 1671805976949}, 'ResponseMetadata': {'RequestId': '5a218ea9-392e-4f5f-88e4-cce06bc87de3', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:32:57 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '817', 'connection': 'keep-alive', 'x-amzn-requestid': '5a218ea9-392e-4f5f-88e4-cce06bc87de3'}, 'RetryAttempts': 0}}
[0m14:32:57.553566 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:32:57.553566 [debug] [Thread-1 (]: SQL status: OK in 4.0 seconds
[0m14:32:57.601566 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:32:57.609565 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.view_state"
[0m14:32:57.611568 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:32:57.611568 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m14:32:57.612570 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

  
    
    	create table hudidb.view_state
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
    
	as
	


with cleansed_order as (

    SELECT category,
           count(*)
    FROM hudidb.cleansed_order
    GROUP BY category
    ORDER BY category asc
)

select
    *
from cleansed_order
  
[0m14:32:57.612570 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:32:57.612570 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:32:57.612570 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:32:57.613568 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:32:57.613568 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

  
    
    	create table hudidb.view_state
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
    
	as
	


with cleansed_order as (

    SELECT category,
           count(*)
    FROM hudidb.cleansed_order
    GROUP BY category
    ORDER BY category asc
)

select
    *
from cleansed_order
  ''')
[0m14:32:59.201216 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 12, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\n\n  \n    \n    \tcreate table hudidb.view_state\n    \n    \n    using hudi\n    \n    \n    \n    LOCATION \'s3://soumil-dms-learn/hudi/hudidb/view_state/\'\n    \n\tas\n\t\n\n\nwith cleansed_order as (\n\n    SELECT category,\n           count(*)\n    FROM hudidb.cleansed_order\n    GROUP BY category\n    ORDER BY category asc\n)\n\nselect\n    *\nfrom cleansed_order\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 12, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': "An error occurred while calling o75.sql.\n: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.\n\tat org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', "py4j.protocol.Py4JJavaError: An error occurred while calling o75.sql.\n: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.\n\tat org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671805978257, 'CompletedOn': 1671805978961}, 'ResponseMetadata': {'RequestId': 'a3591e65-2a84-4786-9c87-11731f0f37d5', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:32:59 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '8569', 'connection': 'keep-alive', 'x-amzn-requestid': 'a3591e65-2a84-4786-9c87-11731f0f37d5'}, 'RetryAttempts': 0}}
[0m14:32:59.201216 [debug] [Thread-1 (]: Glue adapter: status = error
[0m14:32:59.202215 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

  
    
    	create table hudidb.view_state
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
    
	as
	


with cleansed_order as (

    SELECT category,
           count(*)
    FROM hudidb.cleansed_order
    GROUP BY category
    ORDER BY category asc
)

select
    *
from cleansed_order
  '''), Py4JJavaError: An error occurred while calling o75.sql.
: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

[0m14:32:59.207215 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

  
    
    	create table hudidb.view_state
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
    
	as
	


with cleansed_order as (

    SELECT category,
           count(*)
    FROM hudidb.cleansed_order
    GROUP BY category
    ORDER BY category asc
)

select
    *
from cleansed_order
  '''), Py4JJavaError: An error occurred while calling o75.sql.
: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

[0m14:32:59.208217 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

  
    
    	create table hudidb.view_state
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
    
	as
	


with cleansed_order as (

    SELECT category,
           count(*)
    FROM hudidb.cleansed_order
    GROUP BY category
    ORDER BY category asc
)

select
    *
from cleansed_order
  
[0m14:32:59.208217 [debug] [Thread-1 (]: On model.dbtglue.view_state: ROLLBACK
[0m14:32:59.208217 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:32:59.209216 [debug] [Thread-1 (]: On model.dbtglue.view_state: Close
[0m14:32:59.209216 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:32:59.209216 [debug] [Thread-1 (]: finished collecting timing info
[0m14:32:59.210215 [debug] [Thread-1 (]: Database Error in model view_state (models\metrics\view_state.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
  
    
      
      	create table hudidb.view_state
      
      
      using hudi
      
      
      
      LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
      
  	as
  	
  
  
  with cleansed_order as (
  
      SELECT category,
             count(*)
      FROM hudidb.cleansed_order
      GROUP BY category
      ORDER BY category asc
  )
  
  select
      *
  from cleansed_order
    '''), Py4JJavaError: An error occurred while calling o75.sql.
  : org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
  	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
  	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
  	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
  	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
  	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
  	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  
  compiled Code at target\run\dbtglue\models\metrics\view_state.sql
[0m14:32:59.211213 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'dde7b8bd-3c62-4f6b-8cbd-850c2113c8a1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795C0C76A0>]}
[0m14:32:59.211213 [error] [Thread-1 (]: 2 of 2 ERROR creating sql table model hudidb.view_state ........................ [[31mERROR[0m in 8.11s]
[0m14:32:59.212213 [debug] [Thread-1 (]: Finished running node model.dbtglue.view_state
[0m14:32:59.214215 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:32:59.214215 [debug] [MainThread]: On master: ROLLBACK
[0m14:32:59.214215 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:32:59.215215 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m14:32:59.699310 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m14:32:59.700311 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m14:32:59.701309 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-ead9ef55-23cf-4845-82dd-9b3196bea608
[0m14:33:00.491929 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:33:00.491929 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:33:00.492929 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:33:00.492929 [debug] [MainThread]: On master: ROLLBACK
[0m14:33:00.492929 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:33:00.492929 [debug] [MainThread]: On master: Close
[0m14:33:00.493929 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m14:33:00.493929 [info ] [MainThread]: 
[0m14:33:00.494929 [info ] [MainThread]: Finished running 1 incremental model, 1 table model in 0 hours 2 minutes and 2.85 seconds (122.85s).
[0m14:33:00.495931 [debug] [MainThread]: Glue adapter: cleanup called
[0m14:33:00.777499 [info ] [MainThread]: 
[0m14:33:00.777499 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:33:00.778499 [info ] [MainThread]: 
[0m14:33:00.779497 [error] [MainThread]: [33mDatabase Error in model view_state (models\metrics\view_state.sql)[0m
[0m14:33:00.780499 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
[0m14:33:00.780499 [error] [MainThread]:   
[0m14:33:00.781499 [error] [MainThread]:     
[0m14:33:00.781499 [error] [MainThread]:       
[0m14:33:00.782499 [error] [MainThread]:       	create table hudidb.view_state
[0m14:33:00.782499 [error] [MainThread]:       
[0m14:33:00.783499 [error] [MainThread]:       
[0m14:33:00.784500 [error] [MainThread]:       using hudi
[0m14:33:00.784500 [error] [MainThread]:       
[0m14:33:00.785501 [error] [MainThread]:       
[0m14:33:00.786503 [error] [MainThread]:       
[0m14:33:00.786503 [error] [MainThread]:       LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
[0m14:33:00.787505 [error] [MainThread]:       
[0m14:33:00.787505 [error] [MainThread]:   	as
[0m14:33:00.788503 [error] [MainThread]:   	
[0m14:33:00.788503 [error] [MainThread]:   
[0m14:33:00.789502 [error] [MainThread]:   
[0m14:33:00.789502 [error] [MainThread]:   with cleansed_order as (
[0m14:33:00.790502 [error] [MainThread]:   
[0m14:33:00.790502 [error] [MainThread]:       SELECT category,
[0m14:33:00.791499 [error] [MainThread]:              count(*)
[0m14:33:00.791499 [error] [MainThread]:       FROM hudidb.cleansed_order
[0m14:33:00.792545 [error] [MainThread]:       GROUP BY category
[0m14:33:00.792545 [error] [MainThread]:       ORDER BY category asc
[0m14:33:00.793504 [error] [MainThread]:   )
[0m14:33:00.794501 [error] [MainThread]:   
[0m14:33:00.794501 [error] [MainThread]:   select
[0m14:33:00.795503 [error] [MainThread]:       *
[0m14:33:00.796500 [error] [MainThread]:   from cleansed_order
[0m14:33:00.796500 [error] [MainThread]:     '''), Py4JJavaError: An error occurred while calling o75.sql.
[0m14:33:00.797501 [error] [MainThread]:   : org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
[0m14:33:00.798504 [error] [MainThread]:   	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
[0m14:33:00.798504 [error] [MainThread]:   	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
[0m14:33:00.799498 [error] [MainThread]:   	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
[0m14:33:00.800499 [error] [MainThread]:   	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
[0m14:33:00.800499 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
[0m14:33:00.801502 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
[0m14:33:00.802502 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[0m14:33:00.802502 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[0m14:33:00.803501 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
[0m14:33:00.804499 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
[0m14:33:00.805500 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
[0m14:33:00.805500 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
[0m14:33:00.806505 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
[0m14:33:00.807501 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
[0m14:33:00.807501 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
[0m14:33:00.808501 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
[0m14:33:00.808501 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
[0m14:33:00.809501 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
[0m14:33:00.809501 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
[0m14:33:00.810501 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
[0m14:33:00.811501 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m14:33:00.811501 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
[0m14:33:00.812500 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
[0m14:33:00.813504 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
[0m14:33:00.814500 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
[0m14:33:00.815501 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m14:33:00.815501 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
[0m14:33:00.816500 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
[0m14:33:00.817501 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m14:33:00.818502 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
[0m14:33:00.818502 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[0m14:33:00.819502 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[0m14:33:00.819502 [error] [MainThread]:   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[0m14:33:00.820503 [error] [MainThread]:   	at java.lang.reflect.Method.invoke(Method.java:498)
[0m14:33:00.820503 [error] [MainThread]:   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[0m14:33:00.821532 [error] [MainThread]:   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[0m14:33:00.822518 [error] [MainThread]:   	at py4j.Gateway.invoke(Gateway.java:282)
[0m14:33:00.822518 [error] [MainThread]:   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[0m14:33:00.823520 [error] [MainThread]:   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[0m14:33:00.823520 [error] [MainThread]:   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[0m14:33:00.824519 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:750)
[0m14:33:00.824519 [error] [MainThread]:   
[0m14:33:00.825519 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\view_state.sql
[0m14:33:00.825519 [info ] [MainThread]: 
[0m14:33:00.826519 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m14:33:00.826519 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795B41A980>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795C0837F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002795C07A950>]}
[0m14:33:00.827517 [debug] [MainThread]: Flushing usage events
[0m14:33:00.989625 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 14:35:34.338165 | 24c13de3-01cd-4e05-8997-9208cf524475 ==============================
[0m14:35:34.338165 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:35:34.340153 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:35:34.340153 [debug] [MainThread]: Tracking: tracking
[0m14:35:34.371105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD8DDD4E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD8DDD870>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD8DDD630>]}
[0m14:35:34.483113 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m14:35:34.484106 [debug] [MainThread]: Partial parsing: update schema file: dbtglue://models\metrics\schema.yml
[0m14:35:34.484106 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\view_state.sql
[0m14:35:34.500105 [debug] [MainThread]: 1699: static parser successfully parsed metrics\view_state.sql
[0m14:35:34.542106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '24c13de3-01cd-4e05-8997-9208cf524475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD9067070>]}
[0m14:35:34.551110 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '24c13de3-01cd-4e05-8997-9208cf524475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD8F11090>]}
[0m14:35:34.551110 [info ] [MainThread]: Found 2 models, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m14:35:34.552108 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24c13de3-01cd-4e05-8997-9208cf524475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD8F13B80>]}
[0m14:35:34.554110 [info ] [MainThread]: 
[0m14:35:34.556106 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:35:34.558119 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m14:35:34.558119 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:35:34.558119 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:35:34.559106 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m14:35:34.559106 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m14:36:31.124600 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:36:31.125600 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-3caf3a56-8995-421a-9c21-09815c26132a
[0m14:36:40.472056 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:36:41.038362 [debug] [ThreadPool]: On list_hudidb: Close
[0m14:36:41.038362 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:36:41.041369 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m14:36:41.041369 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:36:41.042369 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:36:41.737832 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m14:36:41.738835 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:36:41.738835 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-3caf3a56-8995-421a-9c21-09815c26132a
[0m14:36:42.560410 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:36:43.112074 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m14:36:43.112074 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:36:43.113075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '24c13de3-01cd-4e05-8997-9208cf524475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD8FA2560>]}
[0m14:36:43.114076 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:36:43.114076 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:36:43.115079 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:36:43.116075 [info ] [MainThread]: 
[0m14:36:43.122075 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m14:36:43.122075 [info ] [Thread-1 (]: 1 of 2 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m14:36:43.123075 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m14:36:43.124075 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m14:36:43.124075 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m14:36:43.128075 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m14:36:43.130076 [debug] [Thread-1 (]: finished collecting timing info
[0m14:36:43.130076 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m14:36:43.163077 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:36:43.164184 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:36:43.657608 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:36:43.657608 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:36:43.657608 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-3caf3a56-8995-421a-9c21-09815c26132a
[0m14:36:44.819027 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:36:45.387387 [debug] [Thread-1 (]: Glue adapter: table_name : cleansed_order
[0m14:36:45.388387 [debug] [Thread-1 (]: Glue adapter: table type : table
[0m14:36:45.402391 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:36:45.402391 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:36:45.403392 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m14:36:45.403392 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:36:45.404389 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:36:45.404389 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:36:45.404389 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:36:45.404389 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m14:37:06.276117 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806206053, 'CompletedOn': 1671806225784}, 'ResponseMetadata': {'RequestId': '87dc5c5c-4850-412c-8fc0-39cd1867fe03', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:37:06 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': '87dc5c5c-4850-412c-8fc0-39cd1867fe03'}, 'RetryAttempts': 0}}
[0m14:37:06.277117 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:37:06.277117 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:37:06.277117 [debug] [Thread-1 (]: SQL status: OK in 20.87 seconds
[0m14:37:06.285119 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:37:06.289120 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:37:06.865302 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : cleansed_order
                             type : table
                        
[0m14:37:06.866308 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m14:37:06.866308 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:37:06.866308 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:37:06.867300 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:37:06.867300 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m14:37:36.095790 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223143715181", "_hoodie_commit_seqno": "20221223143715181_0_1", "_hoodie_record_key": "10940", "_hoodie_partition_path": "", "_hoodie_file_name": "ce68f924-8f81-4327-8948-934993f2e7aa-0_0-34-153_20221223143715181.parquet", "invoice_id": 10940, "category": "Garden", "state": "MI", "item_id": 39, "update_hudi_ts": "2022-12-23 14:37:15.519000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806227510, 'CompletedOn': 1671806255479}, 'ResponseMetadata': {'RequestId': 'f3ebce7b-1d40-4a5a-96c6-e21e1f66f159', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:37:36 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4016', 'connection': 'keep-alive', 'x-amzn-requestid': 'f3ebce7b-1d40-4a5a-96c6-e21e1f66f159'}, 'RetryAttempts': 0}}
[0m14:37:36.095790 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:37:36.096789 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:37:36.096789 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m14:37:36.097788 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:37:36.097788 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m14:37:36.098789 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:37:36.098789 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:37:36.098789 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:37:36.098789 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:37:36.099790 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m14:37:37.706991 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223143715181", "_hoodie_commit_seqno": "20221223143715181_1_1", "_hoodie_record_key": "15385", "_hoodie_partition_path": "", "_hoodie_file_name": "73408d9a-54b1-4c93-960f-0d1a7fee537f-0_1-40-154_20221223143715181.parquet", "invoice_id": 15385, "category": "Household", "state": "NJ", "item_id": 39, "update_hudi_ts": "2022-12-23 14:37:15.519000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806256740, 'CompletedOn': 1671806257680}, 'ResponseMetadata': {'RequestId': '44229f15-92f0-4cac-b2ff-6e6b629ca580', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:37:38 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1519', 'connection': 'keep-alive', 'x-amzn-requestid': '44229f15-92f0-4cac-b2ff-6e6b629ca580'}, 'RetryAttempts': 0}}
[0m14:37:37.706991 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:37:37.706991 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:37:37.707990 [debug] [Thread-1 (]: SQL status: OK in 1.61 seconds
[0m14:37:37.708990 [debug] [Thread-1 (]: finished collecting timing info
[0m14:37:37.708990 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m14:37:37.708990 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:37:37.708990 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m14:37:37.709991 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:37:37.710526 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24c13de3-01cd-4e05-8997-9208cf524475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD99DB0A0>]}
[0m14:37:37.711061 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 54.59s]
[0m14:37:37.712038 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m14:37:37.712038 [debug] [Thread-1 (]: Began running node model.dbtglue.view_state
[0m14:37:37.713038 [info ] [Thread-1 (]: 2 of 2 START sql table model hudidb.view_state ................................. [RUN]
[0m14:37:37.713038 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.view_state"
[0m14:37:37.714038 [debug] [Thread-1 (]: Began compiling node model.dbtglue.view_state
[0m14:37:37.714038 [debug] [Thread-1 (]: Compiling model.dbtglue.view_state
[0m14:37:37.718117 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.view_state"
[0m14:37:37.719112 [debug] [Thread-1 (]: finished collecting timing info
[0m14:37:37.720039 [debug] [Thread-1 (]: Began executing node model.dbtglue.view_state
[0m14:37:37.729558 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:37:37.730552 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:37:38.212920 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:37:38.213918 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:37:38.213918 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-3caf3a56-8995-421a-9c21-09815c26132a
[0m14:37:39.022473 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:37:39.543532 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table view_state not found.
[0m14:37:39.586532 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:37:39.592531 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.view_state"
[0m14:37:39.593531 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:37:39.593531 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m14:37:39.594531 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

  
    
    	create table hudidb.view_state
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
    
	as
	


with cleansed_order as (
    SELECT state,
            count(*)
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order
  
[0m14:37:39.594531 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:37:39.594531 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:37:39.594531 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:37:39.595531 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:37:39.595531 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

  
    
    	create table hudidb.view_state
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
    
	as
	


with cleansed_order as (
    SELECT state,
            count(*)
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order
  ''')
[0m14:37:41.218399 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\n\n  \n    \n    \tcreate table hudidb.view_state\n    \n    \n    using hudi\n    \n    \n    \n    LOCATION \'s3://soumil-dms-learn/hudi/hudidb/view_state/\'\n    \n\tas\n\t\n\n\nwith cleansed_order as (\n    SELECT state,\n            count(*)\n    FROM hudidb.cleansed_order\n    GROUP BY state\n    ORDER BY state asc\n)\n\nselect\n    *\nfrom cleansed_order\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 11, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': "An error occurred while calling o75.sql.\n: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.\n\tat org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', "py4j.protocol.Py4JJavaError: An error occurred while calling o75.sql.\n: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.\n\tat org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)\n\tat org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671806260258, 'CompletedOn': 1671806261091}, 'ResponseMetadata': {'RequestId': '5ec1bad2-7cfe-40fe-a071-f2c76eeff236', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:37:41 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '8559', 'connection': 'keep-alive', 'x-amzn-requestid': '5ec1bad2-7cfe-40fe-a071-f2c76eeff236'}, 'RetryAttempts': 0}}
[0m14:37:41.219392 [debug] [Thread-1 (]: Glue adapter: status = error
[0m14:37:41.219392 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

  
    
    	create table hudidb.view_state
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
    
	as
	


with cleansed_order as (
    SELECT state,
            count(*)
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order
  '''), Py4JJavaError: An error occurred while calling o75.sql.
: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

[0m14:37:41.223398 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

  
    
    	create table hudidb.view_state
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
    
	as
	


with cleansed_order as (
    SELECT state,
            count(*)
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order
  '''), Py4JJavaError: An error occurred while calling o75.sql.
: org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

[0m14:37:41.223398 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

  
    
    	create table hudidb.view_state
    
    
    using hudi
    
    
    
    LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
    
	as
	


with cleansed_order as (
    SELECT state,
            count(*)
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order
  
[0m14:37:41.223398 [debug] [Thread-1 (]: On model.dbtglue.view_state: ROLLBACK
[0m14:37:41.223398 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:37:41.224391 [debug] [Thread-1 (]: On model.dbtglue.view_state: Close
[0m14:37:41.224391 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:37:41.224391 [debug] [Thread-1 (]: finished collecting timing info
[0m14:37:41.225391 [debug] [Thread-1 (]: Database Error in model view_state (models\metrics\view_state.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
  
    
      
      	create table hudidb.view_state
      
      
      using hudi
      
      
      
      LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
      
  	as
  	
  
  
  with cleansed_order as (
      SELECT state,
              count(*)
      FROM hudidb.cleansed_order
      GROUP BY state
      ORDER BY state asc
  )
  
  select
      *
  from cleansed_order
    '''), Py4JJavaError: An error occurred while calling o75.sql.
  : org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
  	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
  	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
  	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
  	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
  	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
  	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
  	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
  	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
  	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
  	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
  	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
  	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  
  compiled Code at target\run\dbtglue\models\metrics\view_state.sql
[0m14:37:41.225391 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '24c13de3-01cd-4e05-8997-9208cf524475', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD9BD7700>]}
[0m14:37:41.225391 [error] [Thread-1 (]: 2 of 2 ERROR creating sql table model hudidb.view_state ........................ [[31mERROR[0m in 3.51s]
[0m14:37:41.226391 [debug] [Thread-1 (]: Finished running node model.dbtglue.view_state
[0m14:37:41.228392 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:37:41.228392 [debug] [MainThread]: On master: ROLLBACK
[0m14:37:41.228392 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:37:41.228392 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m14:37:42.049488 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m14:37:42.050488 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m14:37:42.050488 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-3caf3a56-8995-421a-9c21-09815c26132a
[0m14:37:44.039119 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:37:44.040118 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:37:44.040118 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:37:44.040118 [debug] [MainThread]: On master: ROLLBACK
[0m14:37:44.040118 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:37:44.041069 [debug] [MainThread]: On master: Close
[0m14:37:44.041069 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m14:37:44.042000 [info ] [MainThread]: 
[0m14:37:44.042000 [info ] [MainThread]: Finished running 1 incremental model, 1 table model in 0 hours 2 minutes and 9.49 seconds (129.49s).
[0m14:37:44.042998 [debug] [MainThread]: Glue adapter: cleanup called
[0m14:37:44.338991 [info ] [MainThread]: 
[0m14:37:44.341062 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:37:44.342049 [info ] [MainThread]: 
[0m14:37:44.342049 [error] [MainThread]: [33mDatabase Error in model view_state (models\metrics\view_state.sql)[0m
[0m14:37:44.344048 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
[0m14:37:44.344048 [error] [MainThread]:   
[0m14:37:44.345048 [error] [MainThread]:     
[0m14:37:44.346050 [error] [MainThread]:       
[0m14:37:44.346050 [error] [MainThread]:       	create table hudidb.view_state
[0m14:37:44.347053 [error] [MainThread]:       
[0m14:37:44.348051 [error] [MainThread]:       
[0m14:37:44.349049 [error] [MainThread]:       using hudi
[0m14:37:44.350054 [error] [MainThread]:       
[0m14:37:44.350054 [error] [MainThread]:       
[0m14:37:44.351048 [error] [MainThread]:       
[0m14:37:44.352048 [error] [MainThread]:       LOCATION 's3://soumil-dms-learn/hudi/hudidb/view_state/'
[0m14:37:44.352048 [error] [MainThread]:       
[0m14:37:44.353048 [error] [MainThread]:   	as
[0m14:37:44.353048 [error] [MainThread]:   	
[0m14:37:44.354052 [error] [MainThread]:   
[0m14:37:44.355051 [error] [MainThread]:   
[0m14:37:44.355051 [error] [MainThread]:   with cleansed_order as (
[0m14:37:44.356048 [error] [MainThread]:       SELECT state,
[0m14:37:44.357056 [error] [MainThread]:               count(*)
[0m14:37:44.357056 [error] [MainThread]:       FROM hudidb.cleansed_order
[0m14:37:44.358049 [error] [MainThread]:       GROUP BY state
[0m14:37:44.358049 [error] [MainThread]:       ORDER BY state asc
[0m14:37:44.359049 [error] [MainThread]:   )
[0m14:37:44.360047 [error] [MainThread]:   
[0m14:37:44.360585 [error] [MainThread]:   select
[0m14:37:44.361120 [error] [MainThread]:       *
[0m14:37:44.362096 [error] [MainThread]:   from cleansed_order
[0m14:37:44.362096 [error] [MainThread]:     '''), Py4JJavaError: An error occurred while calling o75.sql.
[0m14:37:44.363095 [error] [MainThread]:   : org.apache.hudi.exception.HoodieException: 'hoodie.table.name' must be set.
[0m14:37:44.363095 [error] [MainThread]:   	at org.apache.hudi.common.config.HoodieConfig.getStringOrThrow(HoodieConfig.java:205)
[0m14:37:44.364096 [error] [MainThread]:   	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:93)
[0m14:37:44.365095 [error] [MainThread]:   	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
[0m14:37:44.365095 [error] [MainThread]:   	at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:530)
[0m14:37:44.366095 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)
[0m14:37:44.367095 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)
[0m14:37:44.368096 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)
[0m14:37:44.368096 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)
[0m14:37:44.369096 [error] [MainThread]:   	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:120)
[0m14:37:44.369096 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:229)
[0m14:37:44.370617 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3724)
[0m14:37:44.371140 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
[0m14:37:44.371140 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
[0m14:37:44.372129 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
[0m14:37:44.372129 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
[0m14:37:44.373129 [error] [MainThread]:   	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
[0m14:37:44.374129 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
[0m14:37:44.375133 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
[0m14:37:44.375133 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
[0m14:37:44.376133 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
[0m14:37:44.377131 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m14:37:44.378131 [error] [MainThread]:   	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
[0m14:37:44.378131 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3722)
[0m14:37:44.379131 [error] [MainThread]:   	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:229)
[0m14:37:44.379131 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
[0m14:37:44.380135 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m14:37:44.380668 [error] [MainThread]:   	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
[0m14:37:44.380668 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:615)
[0m14:37:44.381652 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
[0m14:37:44.381652 [error] [MainThread]:   	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:610)
[0m14:37:44.382652 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[0m14:37:44.382652 [error] [MainThread]:   	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[0m14:37:44.383652 [error] [MainThread]:   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[0m14:37:44.384655 [error] [MainThread]:   	at java.lang.reflect.Method.invoke(Method.java:498)
[0m14:37:44.384655 [error] [MainThread]:   	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[0m14:37:44.385653 [error] [MainThread]:   	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
[0m14:37:44.385653 [error] [MainThread]:   	at py4j.Gateway.invoke(Gateway.java:282)
[0m14:37:44.386652 [error] [MainThread]:   	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[0m14:37:44.386652 [error] [MainThread]:   	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[0m14:37:44.387652 [error] [MainThread]:   	at py4j.GatewayConnection.run(GatewayConnection.java:238)
[0m14:37:44.388652 [error] [MainThread]:   	at java.lang.Thread.run(Thread.java:750)
[0m14:37:44.388652 [error] [MainThread]:   
[0m14:37:44.389652 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\view_state.sql
[0m14:37:44.389652 [info ] [MainThread]: 
[0m14:37:44.390775 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m14:37:44.391807 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD8FA27D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD9C16710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000026AD9C5C6D0>]}
[0m14:37:44.391807 [debug] [MainThread]: Flushing usage events
[0m14:37:44.559046 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 14:39:31.123547 | 4435cda8-810f-4bfd-8cc0-d836efececd4 ==============================
[0m14:39:31.123547 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:39:31.124546 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:39:31.125546 [debug] [MainThread]: Tracking: tracking
[0m14:39:31.158551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D20BD4E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D20BD990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D20BD5D0>]}
[0m14:39:31.279545 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:39:31.279545 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\view_state.sql
[0m14:39:31.294544 [debug] [MainThread]: 1699: static parser successfully parsed metrics\view_state.sql
[0m14:39:31.329545 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4435cda8-810f-4bfd-8cc0-d836efececd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D234B3A0>]}
[0m14:39:31.337546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4435cda8-810f-4bfd-8cc0-d836efececd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D227FF10>]}
[0m14:39:31.337546 [info ] [MainThread]: Found 2 models, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m14:39:31.338547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4435cda8-810f-4bfd-8cc0-d836efececd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D227D0C0>]}
[0m14:39:31.339546 [info ] [MainThread]: 
[0m14:39:31.340546 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:39:31.342546 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m14:39:31.342546 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:39:31.342546 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:39:31.343546 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m14:39:31.343546 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m14:40:32.686915 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:40:32.686915 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-6959d697-03d0-472f-a25d-bf65478795fe
[0m14:40:40.947506 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:40:41.827534 [debug] [ThreadPool]: On list_hudidb: Close
[0m14:40:41.828534 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:40:41.830534 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m14:40:41.830534 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:40:41.831534 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:40:42.648809 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m14:40:42.648809 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:40:42.649802 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-6959d697-03d0-472f-a25d-bf65478795fe
[0m14:40:43.447925 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:40:43.972388 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m14:40:43.972388 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:40:43.973377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4435cda8-810f-4bfd-8cc0-d836efececd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D22BB820>]}
[0m14:40:43.973377 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:40:43.974374 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:40:43.974374 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:40:43.975373 [info ] [MainThread]: 
[0m14:40:43.980374 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m14:40:43.980374 [info ] [Thread-1 (]: 1 of 2 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m14:40:43.981374 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m14:40:43.981374 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m14:40:43.981374 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m14:40:43.985373 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m14:40:43.986375 [debug] [Thread-1 (]: finished collecting timing info
[0m14:40:43.987374 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m14:40:44.012373 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:40:44.013375 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:40:44.503469 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:40:44.503469 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:40:44.504459 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-6959d697-03d0-472f-a25d-bf65478795fe
[0m14:40:45.260808 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:40:45.829994 [debug] [Thread-1 (]: Glue adapter: table_name : cleansed_order
[0m14:40:45.829994 [debug] [Thread-1 (]: Glue adapter: table type : table
[0m14:40:45.838992 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:40:45.839992 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:40:45.839992 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m14:40:45.839992 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:40:45.839992 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:40:45.840993 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:40:45.840993 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:40:45.840993 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m14:41:03.125741 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806446482, 'CompletedOn': 1671806463357}, 'ResponseMetadata': {'RequestId': '53c3ed8e-8163-4f2d-bfaf-3ff06514c7dd', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:41:03 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': '53c3ed8e-8163-4f2d-bfaf-3ff06514c7dd'}, 'RetryAttempts': 0}}
[0m14:41:03.125741 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:41:03.126742 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:41:03.126742 [debug] [Thread-1 (]: SQL status: OK in 17.29 seconds
[0m14:41:03.131742 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:41:03.134741 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:41:03.705406 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : cleansed_order
                             type : table
                        
[0m14:41:03.706406 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m14:41:03.706406 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:41:03.706406 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:41:03.706406 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:41:03.707405 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m14:41:35.295476 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223144109708", "_hoodie_commit_seqno": "20221223144109708_1_2", "_hoodie_record_key": "15385", "_hoodie_partition_path": "", "_hoodie_file_name": "73408d9a-54b1-4c93-960f-0d1a7fee537f-0_1-40-154_20221223144109708.parquet", "invoice_id": 15385, "category": "Household", "state": "NJ", "item_id": 39, "update_hudi_ts": "2022-12-23 14:41:10.025000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806464344, 'CompletedOn': 1671806494608}, 'ResponseMetadata': {'RequestId': '37c3e948-0747-4a8b-93ba-7a6e16de8c83', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:41:35 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4019', 'connection': 'keep-alive', 'x-amzn-requestid': '37c3e948-0747-4a8b-93ba-7a6e16de8c83'}, 'RetryAttempts': 0}}
[0m14:41:35.296476 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:41:35.296476 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:41:35.297476 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m14:41:35.298479 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:41:35.298479 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m14:41:35.298479 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:41:35.299477 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:41:35.299477 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:41:35.299477 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:41:35.299477 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m14:41:36.915785 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223144109708", "_hoodie_commit_seqno": "20221223144109708_1_2", "_hoodie_record_key": "15385", "_hoodie_partition_path": "", "_hoodie_file_name": "73408d9a-54b1-4c93-960f-0d1a7fee537f-0_1-40-154_20221223144109708.parquet", "invoice_id": 15385, "category": "Household", "state": "NJ", "item_id": 39, "update_hudi_ts": "2022-12-23 14:41:10.025000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806495945, 'CompletedOn': 1671806496840}, 'ResponseMetadata': {'RequestId': 'c7a3017f-d239-432f-b09e-8dcac007badf', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:41:37 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1519', 'connection': 'keep-alive', 'x-amzn-requestid': 'c7a3017f-d239-432f-b09e-8dcac007badf'}, 'RetryAttempts': 0}}
[0m14:41:36.916781 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:41:36.916781 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:41:36.916781 [debug] [Thread-1 (]: SQL status: OK in 1.62 seconds
[0m14:41:36.918780 [debug] [Thread-1 (]: finished collecting timing info
[0m14:41:36.918780 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m14:41:36.918780 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:41:36.918780 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m14:41:36.919779 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:41:36.919779 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4435cda8-810f-4bfd-8cc0-d836efececd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D2CBF010>]}
[0m14:41:36.920784 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 52.94s]
[0m14:41:36.921778 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m14:41:36.922780 [debug] [Thread-1 (]: Began running node model.dbtglue.view_state
[0m14:41:36.922780 [info ] [Thread-1 (]: 2 of 2 START sql incremental model hudidb.view_state ........................... [RUN]
[0m14:41:36.923779 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.view_state"
[0m14:41:36.923779 [debug] [Thread-1 (]: Began compiling node model.dbtglue.view_state
[0m14:41:36.924779 [debug] [Thread-1 (]: Compiling model.dbtglue.view_state
[0m14:41:36.929785 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.view_state"
[0m14:41:36.930778 [debug] [Thread-1 (]: finished collecting timing info
[0m14:41:36.930778 [debug] [Thread-1 (]: Began executing node model.dbtglue.view_state
[0m14:41:36.932783 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:41:36.933782 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:41:37.404822 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:41:37.404822 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:41:37.404822 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-6959d697-03d0-472f-a25d-bf65478795fe
[0m14:41:38.209091 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:41:38.755755 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table view_state not found.
[0m14:41:38.756749 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:41:38.756749 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m14:41:38.756749 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m14:41:38.756749 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:41:38.757748 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:41:38.757748 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:41:38.757748 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:41:38.757748 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m14:41:39.161802 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 11, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806499403, 'CompletedOn': 1671806499571}, 'ResponseMetadata': {'RequestId': 'ad6266bc-e6dc-46f3-9934-9d93b467f27d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:41:39 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '701', 'connection': 'keep-alive', 'x-amzn-requestid': 'ad6266bc-e6dc-46f3-9934-9d93b467f27d'}, 'RetryAttempts': 0}}
[0m14:41:39.161802 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:41:39.162801 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:41:39.162801 [debug] [Thread-1 (]: SQL status: OK in 0.41 seconds
[0m14:41:39.166810 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:41:39.170801 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:41:39.695148 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table view_state not found.
[0m14:41:39.696149 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with cleansed_order as (
    SELECT state,
            count(*)
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
        
spark.sql("""REFRESH TABLE hudidb.view_state""")
SqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")
        
        
[0m14:41:39.696149 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:41:39.696149 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:41:39.697149 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:41:39.697149 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with cleansed_order as (
    SELECT state,
            count(*)
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")

spark.sql("""REFRESH TABLE hudidb.view_state""")
SqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")

[0m14:41:43.681012 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 12, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith cleansed_order as (\n    SELECT state,\n            count(*)\n    FROM hudidb.cleansed_order\n    GROUP BY state\n    ORDER BY state asc\n)\n\nselect\n    *\nfrom cleansed_order""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'state\', \'hoodie.table.name\': \'view_state\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'view_state\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'state\', \'hoodie.table.name\': \'view_state\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'view_state\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")\n\nspark.sql("""REFRESH TABLE hudidb.view_state""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 12, 'Status': 'error', 'ErrorName': 'Py4JJavaError', 'ErrorValue': 'An error occurred while calling o164.save.\n: org.apache.avro.SchemaParseException: Illegal character in: count(1)\n\tat org.apache.avro.Schema.validateName(Schema.java:1151)\n\tat org.apache.avro.Schema.access$200(Schema.java:81)\n\tat org.apache.avro.Schema$Field.<init>(Schema.java:403)\n\tat org.apache.avro.SchemaBuilder$FieldBuilder.completeField(SchemaBuilder.java:2124)\n\tat org.apache.avro.SchemaBuilder$FieldBuilder.completeField(SchemaBuilder.java:2120)\n\tat org.apache.avro.SchemaBuilder$FieldBuilder.access$5200(SchemaBuilder.java:2034)\n\tat org.apache.avro.SchemaBuilder$GenericDefault.noDefault(SchemaBuilder.java:2417)\n\tat org.apache.hudi.spark.org.apache.spark.sql.avro.SchemaConverters$.$anonfun$toAvroType$1(SchemaConverters.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:937)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:937)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1425)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:70)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:69)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:102)\n\tat org.apache.hudi.spark.org.apache.spark.sql.avro.SchemaConverters$.toAvroType(SchemaConverters.scala:191)\n\tat org.apache.hudi.AvroConversionUtils$.convertStructTypeToAvroSchema(AvroConversionUtils.scala:63)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:435)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:162)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n', 'Traceback': ['Traceback (most recent call last):\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1109, in save\n    self._jwrite.save(path)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in deco\n    return f(*a, **kw)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py", line 328, in get_return_value\n    format(target_id, ".", name), value)\n', 'py4j.protocol.Py4JJavaError: An error occurred while calling o164.save.\n: org.apache.avro.SchemaParseException: Illegal character in: count(1)\n\tat org.apache.avro.Schema.validateName(Schema.java:1151)\n\tat org.apache.avro.Schema.access$200(Schema.java:81)\n\tat org.apache.avro.Schema$Field.<init>(Schema.java:403)\n\tat org.apache.avro.SchemaBuilder$FieldBuilder.completeField(SchemaBuilder.java:2124)\n\tat org.apache.avro.SchemaBuilder$FieldBuilder.completeField(SchemaBuilder.java:2120)\n\tat org.apache.avro.SchemaBuilder$FieldBuilder.access$5200(SchemaBuilder.java:2034)\n\tat org.apache.avro.SchemaBuilder$GenericDefault.noDefault(SchemaBuilder.java:2417)\n\tat org.apache.hudi.spark.org.apache.spark.sql.avro.SchemaConverters$.$anonfun$toAvroType$1(SchemaConverters.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:937)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:937)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1425)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:70)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:69)\n\tat org.apache.spark.sql.types.StructType.foreach(StructType.scala:102)\n\tat org.apache.hudi.spark.org.apache.spark.sql.avro.SchemaConverters$.toAvroType(SchemaConverters.scala:191)\n\tat org.apache.hudi.AvroConversionUtils$.convertStructTypeToAvroSchema(AvroConversionUtils.scala:63)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:435)\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:162)\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n\n']}, 'Progress': 1.0, 'StartedOn': 1671806500340, 'CompletedOn': 1671806502959}, 'ResponseMetadata': {'RequestId': '149f3a3e-2178-4cb4-8b92-2dcfe8725d9c', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:41:44 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '12952', 'connection': 'keep-alive', 'x-amzn-requestid': '149f3a3e-2178-4cb4-8b92-2dcfe8725d9c'}, 'RetryAttempts': 0}}
[0m14:41:43.682012 [debug] [Thread-1 (]: Glue adapter: status = error
[0m14:41:43.682012 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with cleansed_order as (
    SELECT state,
            count(*)
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")

spark.sql("""REFRESH TABLE hudidb.view_state""")
SqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")
, Py4JJavaError: An error occurred while calling o164.save.
: org.apache.avro.SchemaParseException: Illegal character in: count(1)
	at org.apache.avro.Schema.validateName(Schema.java:1151)
	at org.apache.avro.Schema.access$200(Schema.java:81)
	at org.apache.avro.Schema$Field.<init>(Schema.java:403)
	at org.apache.avro.SchemaBuilder$FieldBuilder.completeField(SchemaBuilder.java:2124)
	at org.apache.avro.SchemaBuilder$FieldBuilder.completeField(SchemaBuilder.java:2120)
	at org.apache.avro.SchemaBuilder$FieldBuilder.access$5200(SchemaBuilder.java:2034)
	at org.apache.avro.SchemaBuilder$GenericDefault.noDefault(SchemaBuilder.java:2417)
	at org.apache.hudi.spark.org.apache.spark.sql.avro.SchemaConverters$.$anonfun$toAvroType$1(SchemaConverters.scala:194)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:102)
	at org.apache.hudi.spark.org.apache.spark.sql.avro.SchemaConverters$.toAvroType(SchemaConverters.scala:191)
	at org.apache.hudi.AvroConversionUtils$.convertStructTypeToAvroSchema(AvroConversionUtils.scala:63)
	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:435)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:162)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

[0m14:41:43.687014 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with cleansed_order as (
    SELECT state,
            count(*)
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")

spark.sql("""REFRESH TABLE hudidb.view_state""")
SqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")
, Py4JJavaError: An error occurred while calling o164.save.
: org.apache.avro.SchemaParseException: Illegal character in: count(1)
	at org.apache.avro.Schema.validateName(Schema.java:1151)
	at org.apache.avro.Schema.access$200(Schema.java:81)
	at org.apache.avro.Schema$Field.<init>(Schema.java:403)
	at org.apache.avro.SchemaBuilder$FieldBuilder.completeField(SchemaBuilder.java:2124)
	at org.apache.avro.SchemaBuilder$FieldBuilder.completeField(SchemaBuilder.java:2120)
	at org.apache.avro.SchemaBuilder$FieldBuilder.access$5200(SchemaBuilder.java:2034)
	at org.apache.avro.SchemaBuilder$GenericDefault.noDefault(SchemaBuilder.java:2417)
	at org.apache.hudi.spark.org.apache.spark.sql.avro.SchemaConverters$.$anonfun$toAvroType$1(SchemaConverters.scala:194)
	at scala.collection.Iterator.foreach(Iterator.scala:937)
	at scala.collection.Iterator.foreach$(Iterator.scala:937)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:102)
	at org.apache.hudi.spark.org.apache.spark.sql.avro.SchemaConverters$.toAvroType(SchemaConverters.scala:191)
	at org.apache.hudi.AvroConversionUtils$.convertStructTypeToAvroSchema(AvroConversionUtils.scala:63)
	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:435)
	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:162)
	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:750)

[0m14:41:43.688014 [error] [Thread-1 (]: Glue adapter: Database Error
  Glue cursor returned `error` for statement None for code 
  
  from pyspark.sql import SparkSession
  from pyspark.sql.functions import *
  spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
  inputDf = spark.sql("""
  
  
  
  
  with cleansed_order as (
      SELECT state,
              count(*)
      FROM hudidb.cleansed_order
      GROUP BY state
      ORDER BY state asc
  )
  
  select
      *
  from cleansed_order""")
  outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
  if outputDf.count() > 0:
      if None is not None:
  
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
      else:
          combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
          outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
  
  spark.sql("""REFRESH TABLE hudidb.view_state""")
  SqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")
  , Py4JJavaError: An error occurred while calling o164.save.
  : org.apache.avro.SchemaParseException: Illegal character in: count(1)
  	at org.apache.avro.Schema.validateName(Schema.java:1151)
  	at org.apache.avro.Schema.access$200(Schema.java:81)
  	at org.apache.avro.Schema$Field.<init>(Schema.java:403)
  	at org.apache.avro.SchemaBuilder$FieldBuilder.completeField(SchemaBuilder.java:2124)
  	at org.apache.avro.SchemaBuilder$FieldBuilder.completeField(SchemaBuilder.java:2120)
  	at org.apache.avro.SchemaBuilder$FieldBuilder.access$5200(SchemaBuilder.java:2034)
  	at org.apache.avro.SchemaBuilder$GenericDefault.noDefault(SchemaBuilder.java:2417)
  	at org.apache.hudi.spark.org.apache.spark.sql.avro.SchemaConverters$.$anonfun$toAvroType$1(SchemaConverters.scala:194)
  	at scala.collection.Iterator.foreach(Iterator.scala:937)
  	at scala.collection.Iterator.foreach$(Iterator.scala:937)
  	at scala.collection.AbstractIterator.foreach(Iterator.scala:1425)
  	at scala.collection.IterableLike.foreach(IterableLike.scala:70)
  	at scala.collection.IterableLike.foreach$(IterableLike.scala:69)
  	at org.apache.spark.sql.types.StructType.foreach(StructType.scala:102)
  	at org.apache.hudi.spark.org.apache.spark.sql.avro.SchemaConverters$.toAvroType(SchemaConverters.scala:191)
  	at org.apache.hudi.AvroConversionUtils$.convertStructTypeToAvroSchema(AvroConversionUtils.scala:63)
  	at org.apache.hudi.HoodieSparkSqlWriter$.bulkInsertAsRow(HoodieSparkSqlWriter.scala:435)
  	at org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:162)
  	at org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:164)
  	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
  	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:185)
  	at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:223)
  	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:220)
  	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:181)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:134)
  	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:133)
  	at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
  	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  	at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
  	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
  	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
  	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)
  	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
  	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)
  	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)
  	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)
  	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
  	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  	at java.lang.reflect.Method.invoke(Method.java:498)
  	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
  	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
  	at py4j.Gateway.invoke(Gateway.java:282)
  	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
  	at py4j.commands.CallCommand.execute(CallCommand.java:79)
  	at py4j.GatewayConnection.run(GatewayConnection.java:238)
  	at java.lang.Thread.run(Thread.java:750)
  
[0m14:41:43.693014 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.view_state"
[0m14:41:43.694014 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m14:41:43.695018 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
select * from hudidb.view_state limit 1 
[0m14:41:43.695018 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:41:43.695018 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:41:43.695018 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:41:43.695018 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:41:43.696014 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
select * from hudidb.view_state limit 1 ''')
[0m14:41:45.279895 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 13, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\nselect * from hudidb.view_state limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {}, 'ExecutionCount': 13, 'Status': 'error', 'ErrorName': 'AnalysisException', 'ErrorValue': "Table or view not found: hudidb.view_state; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, view_state], [], false\n", 'Traceback': ['Traceback (most recent call last):\n', '  File "<stdin>", line 21, in execute\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 723, in sql\n    return DataFrame(self._jsparkSession.sql(sqlQuery), self._wrapped)\n', '  File "/opt/amazon/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1305, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n', '  File "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in deco\n    raise converted from None\n', "pyspark.sql.utils.AnalysisException: Table or view not found: hudidb.view_state; line 2 pos 14;\n'GlobalLimit 1\n+- 'LocalLimit 1\n   +- 'Project [*]\n      +- 'UnresolvedRelation [hudidb, view_state], [], false\n\n"]}, 'Progress': 1.0, 'StartedOn': 1671806504337, 'CompletedOn': 1671806504562}, 'ResponseMetadata': {'RequestId': 'eea8c8c4-963c-4b2f-9762-0e7000716b54', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:41:45 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1406', 'connection': 'keep-alive', 'x-amzn-requestid': 'eea8c8c4-963c-4b2f-9762-0e7000716b54'}, 'RetryAttempts': 0}}
[0m14:41:45.280889 [debug] [Thread-1 (]: Glue adapter: status = error
[0m14:41:45.280889 [error] [Thread-1 (]: Glue adapter: Glue returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
select * from hudidb.view_state limit 1 '''), AnalysisException: Table or view not found: hudidb.view_state; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, view_state], [], false

[0m14:41:45.281889 [debug] [Thread-1 (]: Glue adapter: Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
select * from hudidb.view_state limit 1 '''), AnalysisException: Table or view not found: hudidb.view_state; line 2 pos 14;
'GlobalLimit 1
+- 'LocalLimit 1
   +- 'Project [*]
      +- 'UnresolvedRelation [hudidb, view_state], [], false

[0m14:41:45.281889 [debug] [Thread-1 (]: Glue adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
select * from hudidb.view_state limit 1 
[0m14:41:45.281889 [debug] [Thread-1 (]: On model.dbtglue.view_state: ROLLBACK
[0m14:41:45.282888 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:41:45.282888 [debug] [Thread-1 (]: On model.dbtglue.view_state: Close
[0m14:41:45.282888 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:41:45.282888 [debug] [Thread-1 (]: finished collecting timing info
[0m14:41:45.283888 [debug] [Thread-1 (]: Database Error in model view_state (models\metrics\view_state.sql)
  Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
  select * from hudidb.view_state limit 1 '''), AnalysisException: Table or view not found: hudidb.view_state; line 2 pos 14;
  'GlobalLimit 1
  +- 'LocalLimit 1
     +- 'Project [*]
        +- 'UnresolvedRelation [hudidb, view_state], [], false
  
  compiled Code at target\run\dbtglue\models\metrics\view_state.sql
[0m14:41:45.283888 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '4435cda8-810f-4bfd-8cc0-d836efececd4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D2EC3FD0>]}
[0m14:41:45.284906 [error] [Thread-1 (]: 2 of 2 ERROR creating sql incremental model hudidb.view_state .................. [[31mERROR[0m in 8.36s]
[0m14:41:45.285888 [debug] [Thread-1 (]: Finished running node model.dbtglue.view_state
[0m14:41:45.286889 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:41:45.286889 [debug] [MainThread]: On master: ROLLBACK
[0m14:41:45.286889 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:41:45.287888 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m14:41:45.812982 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m14:41:45.813982 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m14:41:45.813982 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-6959d697-03d0-472f-a25d-bf65478795fe
[0m14:41:47.150364 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:41:47.151362 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:41:47.151362 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:41:47.151362 [debug] [MainThread]: On master: ROLLBACK
[0m14:41:47.151362 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:41:47.152366 [debug] [MainThread]: On master: Close
[0m14:41:47.152366 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m14:41:47.153364 [info ] [MainThread]: 
[0m14:41:47.153364 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 2 minutes and 15.81 seconds (135.81s).
[0m14:41:47.154367 [debug] [MainThread]: Glue adapter: cleanup called
[0m14:41:47.408362 [info ] [MainThread]: 
[0m14:41:47.409362 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m14:41:47.410362 [info ] [MainThread]: 
[0m14:41:47.410362 [error] [MainThread]: [33mDatabase Error in model view_state (models\metrics\view_state.sql)[0m
[0m14:41:47.411361 [error] [MainThread]:   Glue cursor returned `error` for statement None for code SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
[0m14:41:47.411361 [error] [MainThread]:   select * from hudidb.view_state limit 1 '''), AnalysisException: Table or view not found: hudidb.view_state; line 2 pos 14;
[0m14:41:47.412362 [error] [MainThread]:   'GlobalLimit 1
[0m14:41:47.412362 [error] [MainThread]:   +- 'LocalLimit 1
[0m14:41:47.413361 [error] [MainThread]:      +- 'Project [*]
[0m14:41:47.413361 [error] [MainThread]:         +- 'UnresolvedRelation [hudidb, view_state], [], false
[0m14:41:47.414362 [error] [MainThread]:   
[0m14:41:47.414362 [error] [MainThread]:   compiled Code at target\run\dbtglue\models\metrics\view_state.sql
[0m14:41:47.415369 [info ] [MainThread]: 
[0m14:41:47.415369 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=1 SKIP=0 TOTAL=2
[0m14:41:47.416363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D22BBB50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D2EC2A70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002C5D2F54B20>]}
[0m14:41:47.416363 [debug] [MainThread]: Flushing usage events
[0m14:41:47.601964 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 14:44:06.692595 | 959177cf-8e5d-457e-b5aa-b3d280180127 ==============================
[0m14:44:06.692595 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:44:06.692595 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:44:06.693568 [debug] [MainThread]: Tracking: tracking
[0m14:44:06.718732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002422BC7D540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002422BC7D9F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002422BC7D630>]}
[0m14:44:06.823724 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m14:44:06.824713 [debug] [MainThread]: Partial parsing: update schema file: dbtglue://models\source_tables.yml
[0m14:44:06.824713 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\view_state.sql
[0m14:44:06.839719 [debug] [MainThread]: 1699: static parser successfully parsed metrics\view_state.sql
[0m14:44:06.876713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '959177cf-8e5d-457e-b5aa-b3d280180127', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002422BF0F520>]}
[0m14:44:06.883713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '959177cf-8e5d-457e-b5aa-b3d280180127', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002422BE3FFD0>]}
[0m14:44:06.883713 [info ] [MainThread]: Found 2 models, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m14:44:06.884713 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '959177cf-8e5d-457e-b5aa-b3d280180127', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002422BE3D0F0>]}
[0m14:44:06.885713 [info ] [MainThread]: 
[0m14:44:06.886710 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:44:06.888709 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m14:44:06.888709 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:06.888709 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:44:06.889709 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m14:44:06.889709 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called


============================== 2022-12-23 14:44:25.448326 | ec3b2d78-47a6-4d22-9679-43a8caf2381b ==============================
[0m14:44:25.448326 [info ] [MainThread]: Running with dbt=1.3.1
[0m14:44:25.449326 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m14:44:25.449326 [debug] [MainThread]: Tracking: tracking
[0m14:44:25.474331 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66E5DD540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66E5DD8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66E5DD690>]}
[0m14:44:25.576837 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m14:44:25.577837 [debug] [MainThread]: Partial parsing: update schema file: dbtglue://models\metrics\schema.yml
[0m14:44:25.594840 [debug] [MainThread]: 1699: static parser successfully parsed metrics\view_state.sql
[0m14:44:25.630837 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ec3b2d78-47a6-4d22-9679-43a8caf2381b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66E866E00>]}
[0m14:44:25.639849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ec3b2d78-47a6-4d22-9679-43a8caf2381b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66E713670>]}
[0m14:44:25.639849 [info ] [MainThread]: Found 2 models, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m14:44:25.640841 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec3b2d78-47a6-4d22-9679-43a8caf2381b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66E713A30>]}
[0m14:44:25.641842 [info ] [MainThread]: 
[0m14:44:25.642839 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:44:25.644840 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m14:44:25.644840 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:44:25.644840 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:44:25.645838 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m14:44:25.645838 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m14:45:05.303016 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:45:05.303016 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-54711564-317a-41f9-8a03-ef77b611be45
[0m14:45:13.811891 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:45:14.379222 [debug] [ThreadPool]: On list_hudidb: Close
[0m14:45:14.380225 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:45:14.383226 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m14:45:14.383226 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m14:45:14.383226 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m14:45:14.883482 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m14:45:14.884482 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m14:45:14.884482 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-54711564-317a-41f9-8a03-ef77b611be45
[0m14:45:15.661002 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m14:45:16.250006 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m14:45:16.251000 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m14:45:16.251000 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ec3b2d78-47a6-4d22-9679-43a8caf2381b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66E762A10>]}
[0m14:45:16.252003 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:45:16.252003 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:45:16.253008 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m14:45:16.253008 [info ] [MainThread]: 
[0m14:45:16.258000 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m14:45:16.258999 [info ] [Thread-1 (]: 1 of 2 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m14:45:16.258999 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m14:45:16.260000 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m14:45:16.260000 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m14:45:16.265000 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m14:45:16.266000 [debug] [Thread-1 (]: finished collecting timing info
[0m14:45:16.267000 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m14:45:16.294005 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m14:45:16.295005 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:45:16.802009 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:45:16.802009 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:45:16.802999 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-54711564-317a-41f9-8a03-ef77b611be45
[0m14:45:17.596552 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:45:18.189505 [debug] [Thread-1 (]: Glue adapter: table_name : cleansed_order
[0m14:45:18.189505 [debug] [Thread-1 (]: Glue adapter: table type : table
[0m14:45:18.202505 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:45:18.203505 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:45:18.203505 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m14:45:18.203505 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:45:18.204505 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:45:18.204505 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:45:18.204505 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:45:18.205505 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m14:45:40.108423 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806718837, 'CompletedOn': 1671806740389}, 'ResponseMetadata': {'RequestId': 'c1a7bde0-3ddd-4697-bee1-bd621ef50122', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:45:40 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': 'c1a7bde0-3ddd-4697-bee1-bd621ef50122'}, 'RetryAttempts': 0}}
[0m14:45:40.109423 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:45:40.109423 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:45:40.109423 [debug] [Thread-1 (]: SQL status: OK in 21.91 seconds
[0m14:45:40.115423 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:45:40.118424 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:45:40.694408 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : cleansed_order
                             type : table
                        
[0m14:45:40.695409 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m14:45:40.696408 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:45:40.696408 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:45:40.696408 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:45:40.697409 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m14:46:12.301028 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223144546564", "_hoodie_commit_seqno": "20221223144546564_0_1", "_hoodie_record_key": "10940", "_hoodie_partition_path": "", "_hoodie_file_name": "ce68f924-8f81-4327-8948-934993f2e7aa-0_0-34-153_20221223144546564.parquet", "invoice_id": 10940, "category": "Garden", "state": "MI", "item_id": 39, "update_hudi_ts": "2022-12-23 14:45:46.875000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806741366, 'CompletedOn': 1671806771851}, 'ResponseMetadata': {'RequestId': 'd06322e2-ca21-42a9-9b47-89fe0777558a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:46:12 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4016', 'connection': 'keep-alive', 'x-amzn-requestid': 'd06322e2-ca21-42a9-9b47-89fe0777558a'}, 'RetryAttempts': 0}}
[0m14:46:12.302025 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:46:12.302025 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:46:12.303024 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m14:46:12.304024 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m14:46:12.304024 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m14:46:12.304024 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:46:12.305024 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:46:12.305024 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:46:12.305024 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:46:12.305024 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m14:46:13.999097 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223144546564", "_hoodie_commit_seqno": "20221223144546564_0_1", "_hoodie_record_key": "10940", "_hoodie_partition_path": "", "_hoodie_file_name": "ce68f924-8f81-4327-8948-934993f2e7aa-0_0-34-153_20221223144546564.parquet", "invoice_id": 10940, "category": "Garden", "state": "MI", "item_id": 39, "update_hudi_ts": "2022-12-23 14:45:46.875000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806773023, 'CompletedOn': 1671806774014}, 'ResponseMetadata': {'RequestId': '9b03be04-72a3-43e7-8cca-a950c9897a66', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:46:14 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1516', 'connection': 'keep-alive', 'x-amzn-requestid': '9b03be04-72a3-43e7-8cca-a950c9897a66'}, 'RetryAttempts': 0}}
[0m14:46:13.999097 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:46:14.000097 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:46:14.000097 [debug] [Thread-1 (]: SQL status: OK in 1.7 seconds
[0m14:46:14.001097 [debug] [Thread-1 (]: finished collecting timing info
[0m14:46:14.001097 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m14:46:14.001097 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:46:14.002097 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m14:46:14.002097 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:46:14.002097 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec3b2d78-47a6-4d22-9679-43a8caf2381b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66F1DF070>]}
[0m14:46:14.003097 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 57.74s]
[0m14:46:14.004097 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m14:46:14.004097 [debug] [Thread-1 (]: Began running node model.dbtglue.view_state
[0m14:46:14.005102 [info ] [Thread-1 (]: 2 of 2 START sql incremental model hudidb.view_state ........................... [RUN]
[0m14:46:14.005102 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.view_state"
[0m14:46:14.006102 [debug] [Thread-1 (]: Began compiling node model.dbtglue.view_state
[0m14:46:14.006102 [debug] [Thread-1 (]: Compiling model.dbtglue.view_state
[0m14:46:14.011098 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.view_state"
[0m14:46:14.012097 [debug] [Thread-1 (]: finished collecting timing info
[0m14:46:14.012097 [debug] [Thread-1 (]: Began executing node model.dbtglue.view_state
[0m14:46:14.014098 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m14:46:14.015098 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m14:46:14.540738 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m14:46:14.541738 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m14:46:14.541738 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-54711564-317a-41f9-8a03-ef77b611be45
[0m14:46:15.336826 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:46:15.864820 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table view_state not found.
[0m14:46:15.865750 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m14:46:15.866807 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m14:46:15.866807 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m14:46:15.866807 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:46:15.867749 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:46:15.867749 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:46:15.867749 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:46:15.867749 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m14:46:17.459414 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 11, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806776510, 'CompletedOn': 1671806776760}, 'ResponseMetadata': {'RequestId': '560efc9a-b5ca-4506-8333-885c723f793d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:46:17 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '701', 'connection': 'keep-alive', 'x-amzn-requestid': '560efc9a-b5ca-4506-8333-885c723f793d'}, 'RetryAttempts': 0}}
[0m14:46:17.459414 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:46:17.460414 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:46:17.460414 [debug] [Thread-1 (]: SQL status: OK in 1.59 seconds
[0m14:46:17.463414 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:46:17.467414 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:46:17.985414 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table view_state not found.
[0m14:46:17.986415 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with cleansed_order as (
    SELECT state,
            count(*) as total_invoice
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
        
spark.sql("""REFRESH TABLE hudidb.view_state""")
SqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")
        
        
[0m14:46:17.986415 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:46:17.986415 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:46:17.987415 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:46:17.987415 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with cleansed_order as (
    SELECT state,
            count(*) as total_invoice
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")

spark.sql("""REFRESH TABLE hudidb.view_state""")
SqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")

[0m14:46:32.120690 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 12, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith cleansed_order as (\n    SELECT state,\n            count(*) as total_invoice\n    FROM hudidb.cleansed_order\n    GROUP BY state\n    ORDER BY state asc\n)\n\nselect\n    *\nfrom cleansed_order""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'state\', \'hoodie.table.name\': \'view_state\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'view_state\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'state\', \'hoodie.table.name\': \'view_state\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'view_state\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")\n\nspark.sql("""REFRESH TABLE hudidb.view_state""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223144620041", "_hoodie_commit_seqno": "20221223144620041_0_1", "_hoodie_record_key": "ME", "_hoodie_partition_path": "", "_hoodie_file_name": "f222b7b7-e984-45ff-a6c4-5edd99da1c6b-0_0-217-0_20221223144620041.parquet", "state": "ME", "total_invoice": 1, "update_hudi_ts": "2022-12-23 14:46:21.437000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "total_invoice", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 12, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806778638, 'CompletedOn': 1671806791689}, 'ResponseMetadata': {'RequestId': '975b2daa-2214-4f0d-9ddd-c32c03c91d97', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:46:32 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3560', 'connection': 'keep-alive', 'x-amzn-requestid': '975b2daa-2214-4f0d-9ddd-c32c03c91d97'}, 'RetryAttempts': 0}}
[0m14:46:32.120690 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:46:32.120690 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:46:32.121691 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.view_state"
[0m14:46:32.122689 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m14:46:32.122689 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
select * from hudidb.view_state limit 1 
[0m14:46:32.122689 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m14:46:32.122689 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m14:46:32.123690 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m14:46:32.123690 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m14:46:32.123690 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
select * from hudidb.view_state limit 1 ''')
[0m14:46:33.725436 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 13, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\nselect * from hudidb.view_state limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223144620041", "_hoodie_commit_seqno": "20221223144620041_2_4", "_hoodie_record_key": "OR", "_hoodie_partition_path": "", "_hoodie_file_name": "06ce1ece-3699-47b2-bdff-bb4ebb4a8d47-0_2-219-0_20221223144620041.parquet", "state": "OR", "total_invoice": 1, "update_hudi_ts": "2022-12-23 14:46:21.437000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "total_invoice", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 13, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671806792783, 'CompletedOn': 1671806793634}, 'ResponseMetadata': {'RequestId': '207bfc0f-b121-4966-9b2e-20ef73aae5da', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 14:46:34 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1364', 'connection': 'keep-alive', 'x-amzn-requestid': '207bfc0f-b121-4966-9b2e-20ef73aae5da'}, 'RetryAttempts': 0}}
[0m14:46:33.725436 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m14:46:33.726436 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m14:46:33.726436 [debug] [Thread-1 (]: SQL status: OK in 1.6 seconds
[0m14:46:33.727437 [debug] [Thread-1 (]: finished collecting timing info
[0m14:46:33.727437 [debug] [Thread-1 (]: On model.dbtglue.view_state: ROLLBACK
[0m14:46:33.727437 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m14:46:33.728435 [debug] [Thread-1 (]: On model.dbtglue.view_state: Close
[0m14:46:33.728435 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m14:46:33.728435 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ec3b2d78-47a6-4d22-9679-43a8caf2381b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66F325B40>]}
[0m14:46:33.729435 [info ] [Thread-1 (]: 2 of 2 OK created sql incremental model hudidb.view_state ...................... [[32mOK[0m in 19.72s]
[0m14:46:33.729435 [debug] [Thread-1 (]: Finished running node model.dbtglue.view_state
[0m14:46:33.731446 [debug] [MainThread]: Acquiring new glue connection "master"
[0m14:46:33.731446 [debug] [MainThread]: On master: ROLLBACK
[0m14:46:33.731446 [debug] [MainThread]: Opening a new connection, currently in state init
[0m14:46:33.731446 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m14:46:34.192436 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m14:46:34.192436 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m14:46:34.192436 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-54711564-317a-41f9-8a03-ef77b611be45
[0m14:46:35.015252 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:46:35.015252 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m14:46:35.015252 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m14:46:35.016250 [debug] [MainThread]: On master: ROLLBACK
[0m14:46:35.016250 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m14:46:35.016250 [debug] [MainThread]: On master: Close
[0m14:46:35.017250 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m14:46:35.017250 [info ] [MainThread]: 
[0m14:46:35.018249 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 2 minutes and 9.37 seconds (129.37s).
[0m14:46:35.019250 [debug] [MainThread]: Glue adapter: cleanup called
[0m14:46:35.269795 [info ] [MainThread]: 
[0m14:46:35.270794 [info ] [MainThread]: [32mCompleted successfully[0m
[0m14:46:35.271794 [info ] [MainThread]: 
[0m14:46:35.272794 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m14:46:35.272794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66E762AA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66F3E6B30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B66F47EE90>]}
[0m14:46:35.272794 [debug] [MainThread]: Flushing usage events
[0m14:46:35.440797 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 15:02:32.289976 | 8ddd7312-bc47-4ae8-a951-bde22d30ba7a ==============================
[0m15:02:32.289976 [info ] [MainThread]: Running with dbt=1.3.1
[0m15:02:32.290976 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m15:02:32.290976 [debug] [MainThread]: Tracking: tracking
[0m15:02:32.318976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012DFD0A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012AEFBDF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012DFD0490>]}
[0m15:02:32.423977 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m15:02:32.423977 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m15:02:32.430977 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8ddd7312-bc47-4ae8-a951-bde22d30ba7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012E1D80D0>]}
[0m15:02:32.439975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8ddd7312-bc47-4ae8-a951-bde22d30ba7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012E1036A0>]}
[0m15:02:32.439975 [info ] [MainThread]: Found 2 models, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m15:02:32.440975 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ddd7312-bc47-4ae8-a951-bde22d30ba7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012AEFBDF0>]}
[0m15:02:32.442975 [info ] [MainThread]: 
[0m15:02:32.443975 [debug] [MainThread]: Acquiring new glue connection "master"
[0m15:02:32.444976 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m15:02:32.445975 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:02:32.445975 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m15:02:32.445975 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m15:02:32.446976 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m15:05:17.791443 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m15:05:17.791443 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-4768ae38-a2bd-46ce-bc81-dc9d894e9cb8
[0m15:05:25.810706 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m15:05:26.386546 [debug] [ThreadPool]: On list_hudidb: Close
[0m15:05:26.387546 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m15:05:26.389552 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m15:05:26.389552 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:05:26.389552 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m15:05:26.854083 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m15:05:26.855083 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m15:05:26.855083 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-4768ae38-a2bd-46ce-bc81-dc9d894e9cb8
[0m15:05:27.711086 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m15:05:28.283651 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m15:05:28.283651 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m15:05:28.284652 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8ddd7312-bc47-4ae8-a951-bde22d30ba7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012E6630A0>]}
[0m15:05:28.285652 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m15:05:28.285652 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m15:05:28.286652 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:05:28.287651 [info ] [MainThread]: 
[0m15:05:28.293651 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m15:05:28.294651 [info ] [Thread-1 (]: 1 of 2 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m15:05:28.295652 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m15:05:28.296654 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m15:05:28.296654 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m15:05:28.302654 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m15:05:28.303653 [debug] [Thread-1 (]: finished collecting timing info
[0m15:05:28.304653 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m15:05:28.332728 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:05:28.332728 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m15:05:28.805660 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m15:05:28.806654 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m15:05:28.806654 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-4768ae38-a2bd-46ce-bc81-dc9d894e9cb8
[0m15:05:29.647985 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:05:30.193937 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m15:05:30.202939 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m15:05:30.202939 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m15:05:30.203940 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m15:05:30.203940 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:05:30.203940 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:05:30.203940 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:05:30.204939 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:05:30.204939 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m15:06:06.877141 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671807930868, 'CompletedOn': 1671807966609}, 'ResponseMetadata': {'RequestId': 'dc910b8e-134d-4adb-bd5e-1d36637a988f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:06:07 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': 'dc910b8e-134d-4adb-bd5e-1d36637a988f'}, 'RetryAttempts': 0}}
[0m15:06:06.878140 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:06:06.878140 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:06:06.878140 [debug] [Thread-1 (]: SQL status: OK in 36.67 seconds
[0m15:06:06.882140 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:06:06.885140 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:06:07.429235 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table cleansed_order not found.
[0m15:06:07.429235 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m15:06:07.429235 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:06:07.430234 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:06:07.430234 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:06:07.430234 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m15:06:36.590673 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223150613319", "_hoodie_commit_seqno": "20221223150613319_1_2", "_hoodie_record_key": "6142", "_hoodie_partition_path": "", "_hoodie_file_name": "238a5b4d-6c1d-4f58-b0cd-5b7bd83ea86c-0_1-8-0_20221223150613319.parquet", "invoice_id": 6142, "category": "Household", "state": "HI", "item_id": 36, "update_hudi_ts": "2022-12-23 15:06:15.308000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671807968100, 'CompletedOn': 1671807996223}, 'ResponseMetadata': {'RequestId': 'b4ad26c3-1671-4b6e-93c7-5f39417ab4cf', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:06:37 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3864', 'connection': 'keep-alive', 'x-amzn-requestid': 'b4ad26c3-1671-4b6e-93c7-5f39417ab4cf'}, 'RetryAttempts': 0}}
[0m15:06:36.591675 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:06:36.591675 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:06:36.592674 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m15:06:36.593674 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m15:06:36.593674 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m15:06:36.593674 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:06:36.594674 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:06:36.594674 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:06:36.594674 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:06:36.594674 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m15:06:38.193180 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223150613319", "_hoodie_commit_seqno": "20221223150613319_0_3", "_hoodie_record_key": "18321", "_hoodie_partition_path": "", "_hoodie_file_name": "8a55e2db-3f0c-429e-893e-480de48d6958-0_0-7-0_20221223150613319.parquet", "invoice_id": 18321, "category": "Office", "state": "AL", "item_id": 53, "update_hudi_ts": "2022-12-23 15:06:15.308000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671807997253, 'CompletedOn': 1671807998325}, 'ResponseMetadata': {'RequestId': '2682ce6a-bd6c-467a-85b3-d2b3d7e62925', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:06:38 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1513', 'connection': 'keep-alive', 'x-amzn-requestid': '2682ce6a-bd6c-467a-85b3-d2b3d7e62925'}, 'RetryAttempts': 0}}
[0m15:06:38.194180 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:06:38.194180 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:06:38.195180 [debug] [Thread-1 (]: SQL status: OK in 1.6 seconds
[0m15:06:38.195180 [debug] [Thread-1 (]: finished collecting timing info
[0m15:06:38.196180 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m15:06:38.196180 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m15:06:38.196180 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m15:06:38.196180 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m15:06:38.197180 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ddd7312-bc47-4ae8-a951-bde22d30ba7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012EBA8DC0>]}
[0m15:06:38.198181 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 69.90s]
[0m15:06:38.198181 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m15:06:38.199180 [debug] [Thread-1 (]: Began running node model.dbtglue.view_state
[0m15:06:38.199180 [info ] [Thread-1 (]: 2 of 2 START sql incremental model hudidb.view_state ........................... [RUN]
[0m15:06:38.200181 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.view_state"
[0m15:06:38.200181 [debug] [Thread-1 (]: Began compiling node model.dbtglue.view_state
[0m15:06:38.200181 [debug] [Thread-1 (]: Compiling model.dbtglue.view_state
[0m15:06:38.204180 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.view_state"
[0m15:06:38.205180 [debug] [Thread-1 (]: finished collecting timing info
[0m15:06:38.205180 [debug] [Thread-1 (]: Began executing node model.dbtglue.view_state
[0m15:06:38.206180 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:06:38.207180 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m15:06:38.751212 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m15:06:38.751212 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m15:06:38.751212 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-4768ae38-a2bd-46ce-bc81-dc9d894e9cb8
[0m15:06:39.592086 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:06:40.134200 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table view_state not found.
[0m15:06:40.135200 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m15:06:40.135200 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m15:06:40.136200 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m15:06:40.136200 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:06:40.136200 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:06:40.136200 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:06:40.137200 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:06:40.137200 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m15:06:41.731411 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 11, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671808000792, 'CompletedOn': 1671808001010}, 'ResponseMetadata': {'RequestId': 'cce71fd0-5e11-4f1d-b585-91680a05aea1', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:06:42 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '701', 'connection': 'keep-alive', 'x-amzn-requestid': 'cce71fd0-5e11-4f1d-b585-91680a05aea1'}, 'RetryAttempts': 0}}
[0m15:06:41.732415 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:06:41.732415 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:06:41.732415 [debug] [Thread-1 (]: SQL status: OK in 1.6 seconds
[0m15:06:41.738413 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:06:41.742414 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:06:42.262411 [debug] [Thread-1 (]: Glue adapter: An error occurred (EntityNotFoundException) when calling the GetTable operation: Table view_state not found.
[0m15:06:42.263413 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with cleansed_order as (
    SELECT state,
            count(*) as total_invoice
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
        
spark.sql("""REFRESH TABLE hudidb.view_state""")
SqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")
        
        
[0m15:06:42.263413 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:06:42.263413 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:06:42.263413 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:06:42.264413 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with cleansed_order as (
    SELECT state,
            count(*) as total_invoice
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.bulkinsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'bulk_insert'}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Overwrite').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")

spark.sql("""REFRESH TABLE hudidb.view_state""")
SqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")

[0m15:06:55.823697 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 12, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith cleansed_order as (\n    SELECT state,\n            count(*) as total_invoice\n    FROM hudidb.cleansed_order\n    GROUP BY state\n    ORDER BY state asc\n)\n\nselect\n    *\nfrom cleansed_order""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'state\', \'hoodie.table.name\': \'view_state\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'view_state\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'state\', \'hoodie.table.name\': \'view_state\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'view_state\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.bulkinsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'bulk_insert\'}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Overwrite\').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")\n\nspark.sql("""REFRESH TABLE hudidb.view_state""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223150644567", "_hoodie_commit_seqno": "20221223150644567_0_5", "_hoodie_record_key": "AL", "_hoodie_partition_path": "", "_hoodie_file_name": "455af2df-fa87-4339-8260-7e0ae7cf8bf4-0_0-66-0_20221223150644567.parquet", "state": "AL", "total_invoice": 1, "update_hudi_ts": "2022-12-23 15:06:45.912000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "total_invoice", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 12, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671808002900, 'CompletedOn': 1671808015853}, 'ResponseMetadata': {'RequestId': 'db1f72a9-d16a-4071-98fd-1ff1c8b2fca7', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:06:56 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3559', 'connection': 'keep-alive', 'x-amzn-requestid': 'db1f72a9-d16a-4071-98fd-1ff1c8b2fca7'}, 'RetryAttempts': 0}}
[0m15:06:55.823697 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:06:55.824697 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:06:55.824697 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.view_state"
[0m15:06:55.825696 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m15:06:55.826702 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
select * from hudidb.view_state limit 1 
[0m15:06:55.826702 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:06:55.826702 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:06:55.826702 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:06:55.826702 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:06:55.827693 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
select * from hudidb.view_state limit 1 ''')
[0m15:06:57.396733 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 13, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\nselect * from hudidb.view_state limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223150644567", "_hoodie_commit_seqno": "20221223150644567_1_1", "_hoodie_record_key": "DE", "_hoodie_partition_path": "", "_hoodie_file_name": "94c9eb09-e29e-4c43-be06-da9b0f8fa8e4-0_1-67-0_20221223150644567.parquet", "state": "DE", "total_invoice": 1, "update_hudi_ts": "2022-12-23 15:06:45.912000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "total_invoice", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 13, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671808016472, 'CompletedOn': 1671808017361}, 'ResponseMetadata': {'RequestId': '779f0178-887d-4682-aac8-c4a5b31c911b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:06:57 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1363', 'connection': 'keep-alive', 'x-amzn-requestid': '779f0178-887d-4682-aac8-c4a5b31c911b'}, 'RetryAttempts': 0}}
[0m15:06:57.396733 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:06:57.396733 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:06:57.397733 [debug] [Thread-1 (]: SQL status: OK in 1.57 seconds
[0m15:06:57.397733 [debug] [Thread-1 (]: finished collecting timing info
[0m15:06:57.398734 [debug] [Thread-1 (]: On model.dbtglue.view_state: ROLLBACK
[0m15:06:57.398734 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m15:06:57.398734 [debug] [Thread-1 (]: On model.dbtglue.view_state: Close
[0m15:06:57.399733 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m15:06:57.399733 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8ddd7312-bc47-4ae8-a951-bde22d30ba7a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012ED6DEA0>]}
[0m15:06:57.400733 [info ] [Thread-1 (]: 2 of 2 OK created sql incremental model hudidb.view_state ...................... [[32mOK[0m in 19.20s]
[0m15:06:57.401732 [debug] [Thread-1 (]: Finished running node model.dbtglue.view_state
[0m15:06:57.402732 [debug] [MainThread]: Acquiring new glue connection "master"
[0m15:06:57.402732 [debug] [MainThread]: On master: ROLLBACK
[0m15:06:57.402732 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:06:57.403733 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m15:06:57.872347 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m15:06:57.872347 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m15:06:57.872347 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-4768ae38-a2bd-46ce-bc81-dc9d894e9cb8
[0m15:06:58.649907 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m15:06:58.650914 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m15:06:58.650914 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m15:06:58.650914 [debug] [MainThread]: On master: ROLLBACK
[0m15:06:58.650914 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m15:06:58.651910 [debug] [MainThread]: On master: Close
[0m15:06:58.651910 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m15:06:58.651910 [info ] [MainThread]: 
[0m15:06:58.652916 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 4 minutes and 26.21 seconds (266.21s).
[0m15:06:58.652916 [debug] [MainThread]: Glue adapter: cleanup called
[0m15:06:58.937906 [info ] [MainThread]: 
[0m15:06:58.938906 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:06:58.938906 [info ] [MainThread]: 
[0m15:06:58.939906 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:06:58.939906 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012EBA8910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012EF02E00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002012FF5F2B0>]}
[0m15:06:58.940906 [debug] [MainThread]: Flushing usage events
[0m15:06:59.113913 [debug] [MainThread]: Glue adapter: cleanup called


============================== 2022-12-23 15:09:26.207813 | 5ce70c1f-a699-4c0e-b195-7b6beb9a5b12 ==============================
[0m15:09:26.207813 [info ] [MainThread]: Running with dbt=1.3.1
[0m15:09:26.207813 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\s.shah\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m15:09:26.208767 [debug] [MainThread]: Tracking: tracking
[0m15:09:26.235759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FB96D540>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FB96D8D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FB96D690>]}
[0m15:09:26.340756 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m15:09:26.340756 [debug] [MainThread]: Partial parsing: updated file: dbtglue://models\metrics\view_state.sql
[0m15:09:26.359757 [debug] [MainThread]: 1699: static parser successfully parsed metrics\view_state.sql
[0m15:09:26.400786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5ce70c1f-a699-4c0e-b195-7b6beb9a5b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FBBFB490>]}
[0m15:09:26.408786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5ce70c1f-a699-4c0e-b195-7b6beb9a5b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FBB2FFD0>]}
[0m15:09:26.408786 [info ] [MainThread]: Found 2 models, 1 test, 0 snapshots, 0 analyses, 363 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m15:09:26.409786 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ce70c1f-a699-4c0e-b195-7b6beb9a5b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FBB2D150>]}
[0m15:09:26.410787 [info ] [MainThread]: 
[0m15:09:26.411786 [debug] [MainThread]: Acquiring new glue connection "master"
[0m15:09:26.413786 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb"
[0m15:09:26.413786 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:09:26.413786 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m15:09:26.413786 [debug] [ThreadPool]: Glue adapter: No session present, starting one
[0m15:09:26.414786 [debug] [ThreadPool]: Glue adapter: GlueConnection _start_session called
[0m15:10:10.612273 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m15:10:10.613039 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-08a385c3-f55c-4f99-b0f1-9cb15cd38ef0
[0m15:10:18.711821 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m15:10:19.247504 [debug] [ThreadPool]: On list_hudidb: Close
[0m15:10:19.247504 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m15:10:19.249516 [debug] [ThreadPool]: Acquiring new glue connection "list_hudidb_hudidb"
[0m15:10:19.249516 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m15:10:19.250506 [debug] [ThreadPool]: Glue adapter: GlueConnection connect called
[0m15:10:19.728694 [debug] [ThreadPool]: Glue adapter: Existing session with status : READY
[0m15:10:19.728694 [debug] [ThreadPool]: Glue adapter: GlueConnection _init_session called
[0m15:10:19.728694 [debug] [ThreadPool]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-08a385c3-f55c-4f99-b0f1-9cb15cd38ef0
[0m15:10:20.509916 [debug] [ThreadPool]: Glue adapter: GlueConnection cursor called
[0m15:10:21.042869 [debug] [ThreadPool]: On list_hudidb_hudidb: Close
[0m15:10:21.042869 [debug] [ThreadPool]: Glue adapter: NotImplemented: close
[0m15:10:21.043869 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ce70c1f-a699-4c0e-b195-7b6beb9a5b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FBB8EB60>]}
[0m15:10:21.043869 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m15:10:21.044911 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m15:10:21.044911 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m15:10:21.045938 [info ] [MainThread]: 
[0m15:10:21.050879 [debug] [Thread-1 (]: Began running node model.dbtglue.cleansed_order
[0m15:10:21.051869 [info ] [Thread-1 (]: 1 of 2 START sql incremental model hudidb.cleansed_order ....................... [RUN]
[0m15:10:21.052869 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.cleansed_order"
[0m15:10:21.052869 [debug] [Thread-1 (]: Began compiling node model.dbtglue.cleansed_order
[0m15:10:21.052869 [debug] [Thread-1 (]: Compiling model.dbtglue.cleansed_order
[0m15:10:21.056870 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.cleansed_order"
[0m15:10:21.057869 [debug] [Thread-1 (]: finished collecting timing info
[0m15:10:21.057869 [debug] [Thread-1 (]: Began executing node model.dbtglue.cleansed_order
[0m15:10:21.083870 [debug] [Thread-1 (]: Opening a new connection, currently in state init
[0m15:10:21.084870 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m15:10:21.584384 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m15:10:21.585384 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m15:10:21.585384 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-08a385c3-f55c-4f99-b0f1-9cb15cd38ef0
[0m15:10:22.711963 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:10:23.287176 [debug] [Thread-1 (]: Glue adapter: table_name : cleansed_order
[0m15:10:23.287176 [debug] [Thread-1 (]: Glue adapter: table type : table
[0m15:10:23.298162 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m15:10:23.298162 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m15:10:23.299162 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m15:10:23.299162 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:10:23.299162 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:10:23.299162 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:10:23.300161 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:10:23.300161 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m15:10:38.299752 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 6, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 6, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671808224246, 'CompletedOn': 1671808238715}, 'ResponseMetadata': {'RequestId': '81eb204f-afa1-43e9-8928-dca5a086d45f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:10:38 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '703', 'connection': 'keep-alive', 'x-amzn-requestid': '81eb204f-afa1-43e9-8928-dca5a086d45f'}, 'RetryAttempts': 0}}
[0m15:10:38.299752 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:10:38.299752 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:10:38.300747 [debug] [Thread-1 (]: SQL status: OK in 15.0 seconds
[0m15:10:38.305748 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:10:38.308742 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:10:38.921045 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : cleansed_order
                             type : table
                        
[0m15:10:38.922045 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
        
spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")
        
        
[0m15:10:38.922045 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:10:38.923046 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:10:38.923046 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:10:38.923046 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with source_data as (
    SELECT
        invoiceid ,
        category ,
        destinationstate,
        itemid
    FROM hudidb.order
)

select
    invoiceid as invoice_id,
    source_data.category as category,
    source_data.destinationstate as state,
    source_data.itemid as item_id
from source_data""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'invoice_id', 'hoodie.table.name': 'cleansed_order', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'cleansed_order', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")

spark.sql("""REFRESH TABLE hudidb.cleansed_order""")
SqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")

[0m15:11:08.091109 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 7, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith source_data as (\n    SELECT\n        invoiceid ,\n        category ,\n        destinationstate,\n        itemid\n    FROM hudidb.order\n)\n\nselect\n    invoiceid as invoice_id,\n    source_data.category as category,\n    source_data.destinationstate as state,\n    source_data.itemid as item_id\nfrom source_data""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'invoice_id\', \'hoodie.table.name\': \'cleansed_order\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'cleansed_order\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/cleansed_order/")\n\nspark.sql("""REFRESH TABLE hudidb.cleansed_order""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.cleansed_order LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223151047654", "_hoodie_commit_seqno": "20221223151047654_1_1", "_hoodie_record_key": "6142", "_hoodie_partition_path": "", "_hoodie_file_name": "238a5b4d-6c1d-4f58-b0cd-5b7bd83ea86c-0_1-40-154_20221223151047654.parquet", "invoice_id": 6142, "category": "Household", "state": "HI", "item_id": 36, "update_hudi_ts": "2022-12-23 15:10:47.986000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 7, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671808239624, 'CompletedOn': 1671808268047}, 'ResponseMetadata': {'RequestId': '3f29efac-d8ad-4832-bc24-62220fb57b1b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:11:08 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '4017', 'connection': 'keep-alive', 'x-amzn-requestid': '3f29efac-d8ad-4832-bc24-62220fb57b1b'}, 'RetryAttempts': 0}}
[0m15:11:08.092109 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:11:08.092109 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:11:08.093112 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.cleansed_order"
[0m15:11:08.094111 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.cleansed_order"
[0m15:11:08.094111 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 
[0m15:11:08.094111 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:11:08.094111 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:11:08.095110 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:11:08.095110 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:11:08.095110 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */
select * from hudidb.cleansed_order limit 1 ''')
[0m15:11:09.712480 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 8, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.cleansed_order"} */\nselect * from hudidb.cleansed_order limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223151047654", "_hoodie_commit_seqno": "20221223151047654_0_1", "_hoodie_record_key": "18321", "_hoodie_partition_path": "", "_hoodie_file_name": "8a55e2db-3f0c-429e-893e-480de48d6958-0_0-34-153_20221223151047654.parquet", "invoice_id": 18321, "category": "Office", "state": "AL", "item_id": 53, "update_hudi_ts": "2022-12-23 15:10:47.986000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "invoice_id", "type": "LongType"}, {"name": "category", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "item_id", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 8, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671808268776, 'CompletedOn': 1671808269738}, 'ResponseMetadata': {'RequestId': '323810b0-9ea4-4503-a215-92063bbfbf0a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:11:10 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1516', 'connection': 'keep-alive', 'x-amzn-requestid': '323810b0-9ea4-4503-a215-92063bbfbf0a'}, 'RetryAttempts': 0}}
[0m15:11:09.712480 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:11:09.712480 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:11:09.713480 [debug] [Thread-1 (]: SQL status: OK in 1.62 seconds
[0m15:11:09.714480 [debug] [Thread-1 (]: finished collecting timing info
[0m15:11:09.714480 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: ROLLBACK
[0m15:11:09.714480 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m15:11:09.714480 [debug] [Thread-1 (]: On model.dbtglue.cleansed_order: Close
[0m15:11:09.715480 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m15:11:09.715480 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ce70c1f-a699-4c0e-b195-7b6beb9a5b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FC567250>]}
[0m15:11:09.715480 [info ] [Thread-1 (]: 1 of 2 OK created sql incremental model hudidb.cleansed_order .................. [[32mOK[0m in 48.66s]
[0m15:11:09.716480 [debug] [Thread-1 (]: Finished running node model.dbtglue.cleansed_order
[0m15:11:09.717480 [debug] [Thread-1 (]: Began running node model.dbtglue.view_state
[0m15:11:09.717480 [info ] [Thread-1 (]: 2 of 2 START sql incremental model hudidb.view_state ........................... [RUN]
[0m15:11:09.718480 [debug] [Thread-1 (]: Acquiring new glue connection "model.dbtglue.view_state"
[0m15:11:09.718480 [debug] [Thread-1 (]: Began compiling node model.dbtglue.view_state
[0m15:11:09.718480 [debug] [Thread-1 (]: Compiling model.dbtglue.view_state
[0m15:11:09.724486 [debug] [Thread-1 (]: Writing injected SQL for node "model.dbtglue.view_state"
[0m15:11:09.725485 [debug] [Thread-1 (]: finished collecting timing info
[0m15:11:09.726485 [debug] [Thread-1 (]: Began executing node model.dbtglue.view_state
[0m15:11:09.728542 [debug] [Thread-1 (]: Opening a new connection, currently in state closed
[0m15:11:09.729560 [debug] [Thread-1 (]: Glue adapter: GlueConnection connect called
[0m15:11:10.202677 [debug] [Thread-1 (]: Glue adapter: Existing session with status : READY
[0m15:11:10.203677 [debug] [Thread-1 (]: Glue adapter: GlueConnection _init_session called
[0m15:11:10.203677 [debug] [Thread-1 (]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-08a385c3-f55c-4f99-b0f1-9cb15cd38ef0
[0m15:11:10.978932 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:11:11.544629 [debug] [Thread-1 (]: Glue adapter: table_name : view_state
[0m15:11:11.545628 [debug] [Thread-1 (]: Glue adapter: table type : table
[0m15:11:11.546628 [debug] [Thread-1 (]: Glue adapter: NotImplemented: add_begin_query
[0m15:11:11.546628 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m15:11:11.546628 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  
[0m15:11:11.546628 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:11:11.547628 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:11:11.547628 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:11:11.547628 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:11:11.547628 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */

    set spark.sql.autoBroadcastJoinThreshold=-1
  ''')
[0m15:11:13.164158 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 11, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\n\n    set spark.sql.autoBroadcastJoinThreshold=-1\n  \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"key": "spark.sql.autoBroadcastJoinThreshold", "value": "-1"}}], "description": [{"name": "key", "type": "StringType"}, {"name": "value", "type": "StringType"}]}'}, 'ExecutionCount': 11, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671808272221, 'CompletedOn': 1671808272438}, 'ResponseMetadata': {'RequestId': '7a60faf1-1128-4d8f-a773-5ce4707e8638', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:11:13 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '701', 'connection': 'keep-alive', 'x-amzn-requestid': '7a60faf1-1128-4d8f-a773-5ce4707e8638'}, 'RetryAttempts': 0}}
[0m15:11:13.164158 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:11:13.164158 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:11:13.165158 [debug] [Thread-1 (]: SQL status: OK in 1.62 seconds
[0m15:11:13.170164 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:11:13.174156 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:11:13.747163 [debug] [Thread-1 (]: Glue adapter: schema : hudidb
                             identifier : view_state
                             type : table
                        
[0m15:11:13.748155 [debug] [Thread-1 (]: Glue adapter: hudi code :
        
custom_glue_code_for_dbt_adapter
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with cleansed_order as (
    SELECT state,
            count(*) as total_invoice
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:
        
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
        
spark.sql("""REFRESH TABLE hudidb.view_state""")
SqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")
        
        
[0m15:11:13.748155 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:11:13.749155 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:11:13.749155 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:11:13.749155 [debug] [Thread-1 (]: Glue adapter: client : 

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()
inputDf = spark.sql("""




with cleansed_order as (
    SELECT state,
            count(*) as total_invoice
    FROM hudidb.cleansed_order
    GROUP BY state
    ORDER BY state asc
)

select
    *
from cleansed_order""")
outputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())
if outputDf.count() > 0:
    if None is not None:

        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',   'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")
    else:
        combinedConf = {'className' : 'org.apache.hudi', 'hoodie.datasource.hive_sync.use_jdbc':'false', 'hoodie.datasource.write.precombine.field': 'update_hudi_ts', 'hoodie.consistency.check.enabled': 'true', 'hoodie.datasource.write.recordkey.field': 'state', 'hoodie.table.name': 'view_state', 'hoodie.datasource.hive_sync.database': 'hudidb', 'hoodie.datasource.hive_sync.table': 'view_state', 'hoodie.datasource.hive_sync.enable': 'true',  'hoodie.datasource.hive_sync.partition_extractor_class': 'org.apache.hudi.hive.NonPartitionedExtractor', 'hoodie.datasource.write.keygenerator.class': 'org.apache.hudi.keygen.NonpartitionedKeyGenerator','hoodie.index.type': 'GLOBAL_BLOOM', 'hoodie.bloom.index.update.partition.path': 'true',  'hoodie.upsert.shuffle.parallelism': 20, 'hoodie.datasource.write.operation': 'upsert', 'hoodie.cleaner.policy': 'KEEP_LATEST_COMMITS', 'hoodie.cleaner.commits.retained': 10}
        outputDf.write.format('org.apache.hudi').options(**combinedConf).mode('Append').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")

spark.sql("""REFRESH TABLE hudidb.view_state""")
SqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")

[0m15:11:27.395568 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 12, 'Code': '\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nspark = SparkSession.builder .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") .getOrCreate()\ninputDf = spark.sql("""\n\n\n\n\nwith cleansed_order as (\n    SELECT state,\n            count(*) as total_invoice\n    FROM hudidb.cleansed_order\n    GROUP BY state\n    ORDER BY state asc\n)\n\nselect\n    *\nfrom cleansed_order""")\noutputDf = inputDf.drop("dbt_unique_key").withColumn("update_hudi_ts",current_timestamp())\nif outputDf.count() > 0:\n    if None is not None:\n\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'state\', \'hoodie.table.name\': \'view_state\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'view_state\', \'hoodie.datasource.hive_sync.enable\': \'true\',   \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")\n    else:\n        combinedConf = {\'className\' : \'org.apache.hudi\', \'hoodie.datasource.hive_sync.use_jdbc\':\'false\', \'hoodie.datasource.write.precombine.field\': \'update_hudi_ts\', \'hoodie.consistency.check.enabled\': \'true\', \'hoodie.datasource.write.recordkey.field\': \'state\', \'hoodie.table.name\': \'view_state\', \'hoodie.datasource.hive_sync.database\': \'hudidb\', \'hoodie.datasource.hive_sync.table\': \'view_state\', \'hoodie.datasource.hive_sync.enable\': \'true\',  \'hoodie.datasource.hive_sync.partition_extractor_class\': \'org.apache.hudi.hive.NonPartitionedExtractor\', \'hoodie.datasource.write.keygenerator.class\': \'org.apache.hudi.keygen.NonpartitionedKeyGenerator\',\'hoodie.index.type\': \'GLOBAL_BLOOM\', \'hoodie.bloom.index.update.partition.path\': \'true\',  \'hoodie.upsert.shuffle.parallelism\': 20, \'hoodie.datasource.write.operation\': \'upsert\', \'hoodie.cleaner.policy\': \'KEEP_LATEST_COMMITS\', \'hoodie.cleaner.commits.retained\': 10}\n        outputDf.write.format(\'org.apache.hudi\').options(**combinedConf).mode(\'Append\').save("s3://soumil-dms-learn/hudi/hudidb/view_state/")\n\nspark.sql("""REFRESH TABLE hudidb.view_state""")\nSqlWrapper2.execute("""SELECT * FROM hudidb.view_state LIMIT 1""")\n', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223151115860", "_hoodie_commit_seqno": "20221223151115860_0_2", "_hoodie_record_key": "HI", "_hoodie_partition_path": "", "_hoodie_file_name": "26789ec8-9084-4626-a0aa-720b6e99d67d-0_0-114-370_20221223151115860.parquet", "state": "HI", "total_invoice": 5, "update_hudi_ts": "2022-12-23 15:11:15.869000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "total_invoice", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 12, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671808274408, 'CompletedOn': 1671808287678}, 'ResponseMetadata': {'RequestId': '65270386-8b8c-47a8-a4ac-2ad7ad26849e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:11:27 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '3712', 'connection': 'keep-alive', 'x-amzn-requestid': '65270386-8b8c-47a8-a4ac-2ad7ad26849e'}, 'RetryAttempts': 0}}
[0m15:11:27.396569 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:11:27.396569 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:11:27.396569 [debug] [Thread-1 (]: Writing runtime sql for node "model.dbtglue.view_state"
[0m15:11:27.397568 [debug] [Thread-1 (]: Using glue connection "model.dbtglue.view_state"
[0m15:11:27.397568 [debug] [Thread-1 (]: On model.dbtglue.view_state: /* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
select * from hudidb.view_state limit 1 
[0m15:11:27.397568 [debug] [Thread-1 (]: Glue adapter: GlueConnection cursor called
[0m15:11:27.398568 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute called
[0m15:11:27.398568 [debug] [Thread-1 (]: Glue adapter: GlueCursor remove_comments_header called
[0m15:11:27.398568 [debug] [Thread-1 (]: Glue adapter: GlueCursor add_end_space_if_single_quote called
[0m15:11:27.398568 [debug] [Thread-1 (]: Glue adapter: client : SqlWrapper2.execute('''/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */
select * from hudidb.view_state limit 1 ''')
[0m15:11:28.998915 [debug] [Thread-1 (]: Glue adapter: {'Statement': {'Id': 13, 'Code': 'SqlWrapper2.execute(\'\'\'/* {"app": "dbt", "dbt_version": "1.3.1", "profile_name": "dbtglue", "target_name": "dev", "node_id": "model.dbtglue.view_state"} */\nselect * from hudidb.view_state limit 1 \'\'\')', 'State': 'AVAILABLE', 'Output': {'Data': {'TextPlain': '{"type": "results", "rowcount": 1, "results": [{"type": "record", "data": {"_hoodie_commit_time": "20221223151115860", "_hoodie_commit_seqno": "20221223151115860_0_2", "_hoodie_record_key": "HI", "_hoodie_partition_path": "", "_hoodie_file_name": "26789ec8-9084-4626-a0aa-720b6e99d67d-0_0-114-370_20221223151115860.parquet", "state": "HI", "total_invoice": 5, "update_hudi_ts": "2022-12-23 15:11:15.869000"}}], "description": [{"name": "_hoodie_commit_time", "type": "StringType"}, {"name": "_hoodie_commit_seqno", "type": "StringType"}, {"name": "_hoodie_record_key", "type": "StringType"}, {"name": "_hoodie_partition_path", "type": "StringType"}, {"name": "_hoodie_file_name", "type": "StringType"}, {"name": "state", "type": "StringType"}, {"name": "total_invoice", "type": "LongType"}, {"name": "update_hudi_ts", "type": "TimestampType"}]}'}, 'ExecutionCount': 13, 'Status': 'ok'}, 'Progress': 1.0, 'StartedOn': 1671808288066, 'CompletedOn': 1671808288864}, 'ResponseMetadata': {'RequestId': '0ab21180-80e0-4278-8bf3-d5544a1fa561', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Fri, 23 Dec 2022 15:11:29 GMT', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1366', 'connection': 'keep-alive', 'x-amzn-requestid': '0ab21180-80e0-4278-8bf3-d5544a1fa561'}, 'RetryAttempts': 0}}
[0m15:11:28.999918 [debug] [Thread-1 (]: Glue adapter: status = ok
[0m15:11:28.999918 [debug] [Thread-1 (]: Glue adapter: GlueCursor execute successfully
[0m15:11:28.999918 [debug] [Thread-1 (]: SQL status: OK in 1.6 seconds
[0m15:11:29.000905 [debug] [Thread-1 (]: finished collecting timing info
[0m15:11:29.001906 [debug] [Thread-1 (]: On model.dbtglue.view_state: ROLLBACK
[0m15:11:29.001906 [debug] [Thread-1 (]: Glue adapter: NotImplemented: rollback
[0m15:11:29.001906 [debug] [Thread-1 (]: On model.dbtglue.view_state: Close
[0m15:11:29.001906 [debug] [Thread-1 (]: Glue adapter: NotImplemented: close
[0m15:11:29.002905 [debug] [Thread-1 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5ce70c1f-a699-4c0e-b195-7b6beb9a5b12', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FC6B1AE0>]}
[0m15:11:29.002905 [info ] [Thread-1 (]: 2 of 2 OK created sql incremental model hudidb.view_state ...................... [[32mOK[0m in 19.29s]
[0m15:11:29.003913 [debug] [Thread-1 (]: Finished running node model.dbtglue.view_state
[0m15:11:29.005906 [debug] [MainThread]: Acquiring new glue connection "master"
[0m15:11:29.005906 [debug] [MainThread]: On master: ROLLBACK
[0m15:11:29.005906 [debug] [MainThread]: Opening a new connection, currently in state init
[0m15:11:29.005906 [debug] [MainThread]: Glue adapter: GlueConnection connect called
[0m15:11:29.527168 [debug] [MainThread]: Glue adapter: Existing session with status : READY
[0m15:11:29.527168 [debug] [MainThread]: Glue adapter: GlueConnection _init_session called
[0m15:11:29.528162 [debug] [MainThread]: Glue adapter: GlueConnection session_id : GlueInteractiveSessionRole-dbt-glue-08a385c3-f55c-4f99-b0f1-9cb15cd38ef0
[0m15:11:30.318294 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m15:11:30.318294 [debug] [MainThread]: Glue adapter: NotImplemented: add_begin_query
[0m15:11:30.318294 [debug] [MainThread]: Glue adapter: NotImplemented: commit
[0m15:11:30.319293 [debug] [MainThread]: On master: ROLLBACK
[0m15:11:30.319293 [debug] [MainThread]: Glue adapter: NotImplemented: rollback
[0m15:11:30.319293 [debug] [MainThread]: On master: Close
[0m15:11:30.319293 [debug] [MainThread]: Glue adapter: NotImplemented: close
[0m15:11:30.320287 [info ] [MainThread]: 
[0m15:11:30.320287 [info ] [MainThread]: Finished running 2 incremental models in 0 hours 2 minutes and 3.91 seconds (123.91s).
[0m15:11:30.321288 [debug] [MainThread]: Glue adapter: cleanup called
[0m15:11:30.557180 [info ] [MainThread]: 
[0m15:11:30.557180 [info ] [MainThread]: [32mCompleted successfully[0m
[0m15:11:30.558180 [info ] [MainThread]: 
[0m15:11:30.559181 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2
[0m15:11:30.559181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FC76A5F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FC76A560>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000208FC769180>]}
[0m15:11:30.560182 [debug] [MainThread]: Flushing usage events
[0m15:11:30.737953 [debug] [MainThread]: Glue adapter: cleanup called
